{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "description: Converts from a `sciflow` format notebook to a `sagemaker` pipeline.\n",
    "output-file: to_sagemaker.html\n",
    "title: Sciflow Notebook to Sagemaker Pipeline\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# | default_exp converters.to_sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "import logging\n",
    "import os\n",
    "import shutil\n",
    "from importlib import reload\n",
    "from pathlib import Path, PosixPath\n",
    "from typing import Iterable\n",
    "\n",
    "from execnb.nbio import read_nb\n",
    "from fastcore.script import Param, bool_arg, call_parse\n",
    "from nbdev.config import get_config\n",
    "from nbdev.doclinks import nbglob\n",
    "from scilint.utils import configure_logging\n",
    "\n",
    "from sciflow.packaging import determine_dependencies\n",
    "from sciflow.params import ParamMeta, extract_param_meta, params_as_dict\n",
    "from sciflow.parse_module import (\n",
    "    FuncDetails,\n",
    "    extract_module_only,\n",
    "    extract_return_var_names,\n",
    "    extract_steps,\n",
    ")\n",
    "from sciflow.utils import (\n",
    "    find_default_export,\n",
    "    get_flow_path,\n",
    "    indent_multiline,\n",
    "    lib_path,\n",
    "    prepare_env,\n",
    "    titleize,\n",
    ")\n",
    "\n",
    "reload(logging)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Test setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nbs_dir = Path(\".\").resolve().parent\n",
    "test_dir = Path(nbs_dir, \"test\")\n",
    "nb_path = Path(test_dir, \"test_multistep.ipynb\")\n",
    "nb = read_nb(nb_path)\n",
    "module_name = find_default_export(nb[\"cells\"]).replace(\".\", \"/\")\n",
    "test_module = os.path.join(get_config().path(\"lib_path\"), f\"{module_name}.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prepare_env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "flow_path = get_flow_path(nb_path, flow_provider=\"sagemaker\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Path('/home/sagemaker-user/git/sciflow/nbs/test/flows/sagemaker/test_multistep.py')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flow_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "steps = extract_steps(test_module)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helpers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step Detection utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def is_train_step(step):\n",
    "    return any(step.name.startswith(prefix) for prefix in (\"fit\", \"train\"))\n",
    "\n",
    "\n",
    "def is_processing_step(step):\n",
    "    return not is_train_step(step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_step = FuncDetails(\n",
    "    \"fit_something\",\n",
    "    docstring=None,\n",
    "    args=None,\n",
    "    has_return=False,\n",
    "    return_stmt=None,\n",
    "    code=\"\",\n",
    ")\n",
    "proc_step = FuncDetails(\n",
    "    \"blabla\", docstring=None, args=None, has_return=False, return_stmt=None, code=\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "assert not is_processing_step(training_step)\n",
    "assert is_train_step(training_step)\n",
    "assert is_processing_step(proc_step)\n",
    "assert not is_train_step(proc_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lib_name = get_config().get(\"lib_name\")\n",
    "module_name = find_default_export(nb[\"cells\"])\n",
    "fq_module_name = f\"{lib_name}.{module_name}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input formatting utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def format_job_arguments(param_meta):\n",
    "    job_arg_values = [f\"self.{p}.to_string()\" for p in param_meta.keys()]\n",
    "    stitched_args = list(zip([f\"--{p}\" for p in param_meta.keys()], job_arg_values))\n",
    "    flattened = [item for sublist in stitched_args for item in sublist]\n",
    "    return flattened"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- # click on the button below to open reference:\n",
    "\n",
    "<button data-commandlinker-command=\"help:open\" data-commandlinker-args='{\"url\": \"https://jupyter.org/documentation\", \"text\": \"Jupyter Reference\"}'>Open Jupyter Documentation</button> -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class MockParamMeta:\n",
    "    instance_type: type\n",
    "\n",
    "\n",
    "assert [\"--str_param\", \"self.str_param.to_string()\"] == format_job_arguments(\n",
    "    {\"str_param\": MockParamMeta(str)}\n",
    ")\n",
    "assert [\n",
    "    \"--int_param\",\n",
    "    \"self.int_param.to_string()\",\n",
    "    \"--float_param\",\n",
    "    \"self.float_param.to_string()\",\n",
    "] == format_job_arguments(\n",
    "    {\"int_param\": MockParamMeta(int), \"float_param\": MockParamMeta(float)}\n",
    ")\n",
    "assert [\"--int_param\", \"self.int_param.to_string()\"] == format_job_arguments(\n",
    "    {\"int_param\": MockParamMeta(int)}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def format_hyperparams(param_meta):\n",
    "    job_arg_values = [f\"self.{p}.to_string()\" for p in param_meta.keys()]\n",
    "    return dict(zip(param_meta.keys(), job_arg_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "assert {\n",
    "    \"int_param\": \"self.int_param.to_string()\",\n",
    "    \"str_param\": \"self.str_param.to_string()\",\n",
    "} == format_hyperparams(\n",
    "    {\"int_param\": MockParamMeta(int), \"str_param\": MockParamMeta(str)}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def format_arg(arg, param_meta):\n",
    "    if arg in param_meta and not param_meta[arg].has_metaflow_param:\n",
    "        result = arg\n",
    "    else:\n",
    "        result = \"self.\" + arg\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def format_args(params):\n",
    "    result = []\n",
    "    for key, val in params.items():\n",
    "        # TODO simplify\n",
    "        if val.instance_type == int:\n",
    "            result.append(f\"int({key})\")\n",
    "        elif val.instance_type == float:\n",
    "            result.append(f\"float({key})\")\n",
    "        else:\n",
    "            result.append(key)\n",
    "    return \", \".join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "assert \"int(int_param), str_param\" == format_args(\n",
    "    {\"int_param\": MockParamMeta(int), \"str_param\": MockParamMeta(str)}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter Handling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `write_params`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def write_params(flow_file, param_meta, ind):\n",
    "    for param in param_meta.keys():\n",
    "        if param_meta[param].instance_type == int:\n",
    "            flow_file.write(\n",
    "                f\"{ind}{param} = ParameterInteger(name='{param}', default_value={param})\\n\"\n",
    "            )\n",
    "        elif param_meta[param].instance_type == float:\n",
    "            flow_file.write(\n",
    "                f\"{ind}{param} = ParameterFloat(name='{param}', default_value={param})\\n\"\n",
    "            )\n",
    "        elif param_meta[param].instance_type == str:\n",
    "            flow_file.write(\n",
    "                f\"{ind}{param} = ParameterString(name='{param}', default_value={param})\\n\"\n",
    "            )\n",
    "        elif param_meta[param].instance_type == bool:\n",
    "            flow_file.write(\n",
    "                f\"{ind}{param} = ParameterBoolean(name='{param}', default_value={param})\\n\"\n",
    "            )\n",
    "        elif param_meta[param].instance_type == PosixPath:\n",
    "            flow_file.write(\n",
    "                f\"{ind}{param} = ParameterString(name='{param}', default_value=str({param}))\\n\"\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"Unsupported parameter type for sagemaker pipeline: {param_meta[param].instance_type}\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ` write_sm_params`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def write_sm_params(flow_file, sm_params, ind):\n",
    "    flow_file.write(f\"\\n{ind}# Sagemaker Specific Parameters\\n\")\n",
    "    for key, val in sm_params.items():\n",
    "        if type(val) == int:\n",
    "            flow_file.write(\n",
    "                f\"{ind}{key} = ParameterInteger(name='{key}', default_value={val})\\n\"\n",
    "            )\n",
    "        elif type(val) == float:\n",
    "            flow_file.write(\n",
    "                f\"{ind}{key} = ParameterFloat(name='{key}', default_value={val})\\n\"\n",
    "            )\n",
    "        elif type(val) == str:\n",
    "            flow_file.write(\n",
    "                f\"{ind}{key} = ParameterString(name='{key}', default_value=\\\"{val}\\\")\\n\"\n",
    "            )\n",
    "        elif type(val) == bool:\n",
    "            flow_file.write(\n",
    "                f\"{ind}{key} = ParameterBoolean(name='{key}', default_value=\\\"{val}\\\")\\n\"\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"Unsupported parameter type for sagemaker pipeline: {type(val)}\"\n",
    "            )\n",
    "    flow_file.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `set_sm_params`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def set_sm_params(params, has_processing_step, has_train_step):\n",
    "    # Precedence (higher given precedence)\n",
    "    # 1. Environment var\n",
    "    # 2. Papermill\n",
    "    sm_params = {}\n",
    "    if has_processing_step:\n",
    "        if \"proc_image_uri\" not in params:\n",
    "            if \"SM_PROC_IMAGE_URI\" not in os.environ:\n",
    "                raise ValueError(\n",
    "                    (\n",
    "                        \"Sagemaker Processing Step(s) are included but no processing image URI has \"\n",
    "                        \"been set in either notebook parameters (papermill) or \"\n",
    "                        \"via SM_PROC_IMAGE_URI environment variable. \\nSet one of these to resolve this issue.\"\n",
    "                    )\n",
    "                )\n",
    "            else:\n",
    "                sm_params[\"proc_image_uri\"] = os.environ[\"SM_PROC_IMAGE_URI\"]\n",
    "        if \"proc_instance_type\" not in params:\n",
    "            sm_params[\"proc_instance_type\"] = (\n",
    "                os.environ[\"SM_PROC_INSTANCE_TYPE\"]\n",
    "                if \"SM_PROC_INSTANCE_TYPE\" in os.environ\n",
    "                else \"ml.m5.large\"\n",
    "            )\n",
    "    if has_train_step:\n",
    "        if \"train_image_uri\" not in params:\n",
    "            if \"SM_TRAIN_IMAGE_URI\" not in os.environ:\n",
    "                raise ValueError(\n",
    "                    (\n",
    "                        \"Sagemaker Trainging Step(s) are included but no training image URI has \"\n",
    "                        \"been set in either notebook parameters (papermill) or \"\n",
    "                        \"via SM_TRAIN_IMAGE_URI environment variable. \\nSet one of these to resolve this issue.\"\n",
    "                    )\n",
    "                )\n",
    "            else:\n",
    "                sm_params[\"train_image_uri\"] = os.environ[\"SM_TRAIN_IMAGE_URI\"]\n",
    "        if \"train_instance_type\" not in params:\n",
    "            sm_params[\"train_instance_type\"] = (\n",
    "                os.environ[\"SM_TRAIN_INSTANCE_TYPE\"]\n",
    "                if \"SM_PROC_INSTANCE_TYPE\" in os.environ\n",
    "                else \"ml.m5.large\"\n",
    "            )\n",
    "    return sm_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single Pipeline Conversion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `nb_to_sagemaker_pipeline`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def nb_to_sagemaker_pipeline(\n",
    "    nb_path: Path,\n",
    "    flow_path: Path,\n",
    "    silent: bool = True,\n",
    "):\n",
    "    nb = read_nb(nb_path)\n",
    "    lib_name = get_config().get(\"lib_name\")\n",
    "    module_name = find_default_export(nb[\"cells\"])\n",
    "    if not module_name:\n",
    "        logger.debug(f\"Ignoring conversion for nb with no default export: {nb_path}\")\n",
    "        return\n",
    "    module_name = module_name\n",
    "    path_sep_module_name = module_name.replace(\".\", \"/\")\n",
    "    nb_name = os.path.basename(nb_path)\n",
    "    exported_module = os.path.join(\n",
    "        get_config().path(\"lib_path\"), f\"{path_sep_module_name}.py\"\n",
    "    )\n",
    "    steps = extract_steps(exported_module)\n",
    "    if len(steps) == 0:\n",
    "        logger.debug(f\"Ignoring conversion for nb with no named steps: {nb_path}\")\n",
    "        return\n",
    "    params = params_as_dict(nb_path)\n",
    "    if len(params) == 0:\n",
    "        logger.info(f\"No params cell found for: {os.path.basename(nb_path)}\")\n",
    "    pipeline_class_name = f\"{titleize(extract_module_only(module_name))}Pipeline\"\n",
    "\n",
    "    fq_module_name = f\"{lib_name}.{module_name}\"\n",
    "    param_meta = extract_param_meta(fq_module_name, params)\n",
    "\n",
    "    try:\n",
    "        steps_param_meta, steps_vars = write_pipeline_to_files(\n",
    "            flow_path,\n",
    "            pipeline_class_name,\n",
    "            lib_name,\n",
    "            module_name,\n",
    "            steps,\n",
    "            params,\n",
    "            param_meta,\n",
    "            fq_module_name,\n",
    "        )\n",
    "    except ValueError as ve:\n",
    "        print(f\"Sagemaker conversion failed for {nb_name}, Reason: {ve}\")\n",
    "        return\n",
    "    if not silent:\n",
    "        print(\n",
    "            f\"Converted {nb_name} to {pipeline_class_name} in: {os.path.basename(flow_path)}\"\n",
    "        )\n",
    "    generate_sagemaker_modules(\n",
    "        flow_path,\n",
    "        pipeline_class_name,\n",
    "        lib_name,\n",
    "        module_name,\n",
    "        steps,\n",
    "        params,\n",
    "        param_meta,\n",
    "        steps_param_meta,\n",
    "        steps_vars,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## `write_pipeline_to_files`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def write_pipeline_to_files(\n",
    "    flow_path: Path,\n",
    "    pipeline_class_name: str,\n",
    "    lib_name: str,\n",
    "    module_name: str,\n",
    "    steps: Iterable[FuncDetails],\n",
    "    params: dict,\n",
    "    param_meta: Iterable[ParamMeta],\n",
    "    fq_module_name: str,\n",
    "):\n",
    "    if not os.path.exists(flow_path.parent):\n",
    "        os.mkdir(flow_path.parent)\n",
    "\n",
    "    has_train_step = any([is_train_step(s) for s in steps])\n",
    "    has_processing_step = any([is_processing_step(s) for s in steps])\n",
    "\n",
    "    sm_params = set_sm_params(params, has_processing_step, has_train_step)\n",
    "\n",
    "    config = get_config()\n",
    "\n",
    "    with open(flow_path, \"w\") as flow_file:\n",
    "        flow_file.write(\"#!/usr/bin/env python\\n\")\n",
    "        flow_file.write(\"# coding=utf-8\\n\")\n",
    "        flow_file.write(\"# SCIFLOW GENERATED FILE - EDIT COMPANION NOTEBOOK\\n\")\n",
    "\n",
    "        flow_file.write(\"import os\\n\")\n",
    "        flow_file.write(\"import sys\\n\")\n",
    "        flow_file.write(\"from datetime import datetime\\n\")\n",
    "        flow_file.write(\"from pathlib import Path\\n\")\n",
    "        flow_file.write(\"\\n\")\n",
    "        flow_file.write(\"import boto3\\n\")\n",
    "        flow_file.write(\"import sagemaker\\n\")\n",
    "        flow_file.write(\"from sagemaker.session import Session\\n\")\n",
    "        flow_file.write(\"from sagemaker.workflow.pipeline import Pipeline\\n\")\n",
    "\n",
    "        if has_train_step and has_processing_step:\n",
    "            flow_file.write(\n",
    "                \"from sagemaker.workflow.steps import ProcessingStep, TrainingStep\\n\"\n",
    "            )\n",
    "        elif has_train_step:\n",
    "            flow_file.write(\"from sagemaker.workflow.steps import TrainingStep\\n\")\n",
    "        elif has_processing_step:\n",
    "            flow_file.write(\"from sagemaker.workflow.steps import ProcessingStep\\n\")\n",
    "        if has_processing_step:\n",
    "            flow_file.write(\n",
    "                \"from sagemaker.processing import ScriptProcessor, ProcessingInput, ProcessingOutput\\n\"\n",
    "            )\n",
    "            flow_file.write(\"from sagemaker.workflow.pipeline import Pipeline\\n\")\n",
    "        if has_train_step:\n",
    "            flow_file.write(\"from sagemaker.inputs import TrainingInput\\n\")\n",
    "            flow_file.write(\"from sagemaker.estimator import Estimator\\n\")\n",
    "        if \"promote\" in params.keys():\n",
    "            flow_file.write(\"from sagemaker.model_metrics import MetricsSource, ModelMetrics\\n\")\n",
    "            flow_file.write(\"from sagemaker.workflow.step_collections import RegisterModel\\n\")\n",
    "            flow_file.write(\"from sagemaker.workflow.conditions import ConditionEquals\\n\")\n",
    "            flow_file.write(\"from sagemaker.workflow.condition_step import ConditionStep\\n\")\n",
    "            flow_file.write(\"from sagemaker.workflow.functions import Join\\n\")\n",
    "            \n",
    "        has_sm_param = any((p.has_sagemaker_param for p in param_meta.values()))\n",
    "        if has_sm_param or len(sm_params) > 0:\n",
    "            instance_types = [p.instance_type for p in param_meta.values()] + [\n",
    "                type(v) for v in sm_params.values()\n",
    "            ]\n",
    "            sm_params_import = \"from sagemaker.workflow.parameters import \"\n",
    "            if int in instance_types:\n",
    "                sm_params_import += \"ParameterInteger\"\n",
    "                if float in instance_types or str in instance_types:\n",
    "                    sm_params_import += \", \"\n",
    "            if float in instance_types:\n",
    "                sm_params_import += \"ParameterFloat\"\n",
    "                if str in instance_types:\n",
    "                    sm_params_import += \", \"\n",
    "            if str in instance_types:\n",
    "                sm_params_import += \"ParameterString\"\n",
    "                if bool in instance_types:\n",
    "                    sm_params_import += \", \"\n",
    "            if bool in instance_types:\n",
    "                sm_params_import += \"ParameterBoolean\"\n",
    "\n",
    "            flow_file.write(sm_params_import + \"\\n\")\n",
    "\n",
    "        flow_file.write(\"from sciflow.s3_utils import upload_directory\\n\")\n",
    "        flow_file.write(\"\\n\")\n",
    "        flow_file.write(\n",
    "            f\"from {fq_module_name} import {', '.join([s.name for s in steps])}\\n\"\n",
    "        )\n",
    "        if len(params) > 0:\n",
    "            flow_file.write(\n",
    "                f\"from {fq_module_name} import {', '.join(params.keys())}\\n\"\n",
    "            )\n",
    "\n",
    "        flow_file.write(f\"\\n\\nclass {pipeline_class_name}():\\n\")\n",
    "        ind = \"    \"\n",
    "        write_params(flow_file, param_meta, ind)\n",
    "\n",
    "        if len(sm_params) > 0:\n",
    "            write_sm_params(flow_file, sm_params, ind)\n",
    "\n",
    "        params.update(sm_params)\n",
    "\n",
    "        flow_file.write(f\"{ind}args = []\\n\")\n",
    "        flow_file.write(\n",
    "            f\"{ind}param_types = {dict(zip(param_meta.keys(), [v.instance_type.__name__ for v in param_meta.values()]))}\\n\"\n",
    "        )\n",
    "\n",
    "        flow_file.write(\"\\n\")\n",
    "        step_names = [s.name for s in steps]\n",
    "        # Add manual promotion step at end of pipeline\n",
    "        if \"promote\" in params.keys():\n",
    "            step_names = step_names + [\"cond_promote\"]\n",
    "        flow_file.write(f\"{ind}steps = {step_names}\\n\")\n",
    "        flow_file.write(\"\\n\")\n",
    "        steps_param_meta, steps_vars = write_steps(\n",
    "            flow_path,\n",
    "            lib_name,\n",
    "            module_name,\n",
    "            fq_module_name,\n",
    "            flow_file,\n",
    "            steps,\n",
    "            param_meta,\n",
    "            ind,\n",
    "            params.get(\"proc_image_uri\", None),\n",
    "            params.get(\"proc_instance_type\", None),\n",
    "            params.get(\"train_image_uri\", None),\n",
    "            params.get(\"train_instance_type\", None),\n",
    "        )\n",
    "        flow_file.write(\"\\n\")\n",
    "\n",
    "        flow_file.write(f\"{ind}def set_run_params(self):\\n\")\n",
    "        flow_file.write(f\"{ind}{ind}for arg_key, arg_val in self.args.items():\\n\")\n",
    "        flow_file.write(f\"{ind}{ind}{ind}for param in self.param_types.keys():\\n\")\n",
    "        flow_file.write(f\"{ind}{ind}{ind}{ind}if arg_key.strip('-') == param:\\n\")\n",
    "        flow_file.write(\n",
    "            f\"{ind}{ind}{ind}{ind}{ind}if self.param_types[param] == 'int':\\n\"\n",
    "        )\n",
    "        flow_file.write(\n",
    "            f\"{ind}{ind}{ind}{ind}{ind}{ind}setattr(self, param, ParameterInteger(name=param, default_value=int(arg_val)))\\n\"\n",
    "        )\n",
    "        flow_file.write(\n",
    "            f\"{ind}{ind}{ind}{ind}{ind}elif self.param_types[param] == 'float':\\n\"\n",
    "        )\n",
    "        flow_file.write(\n",
    "            f\"{ind}{ind}{ind}{ind}{ind}{ind}setattr(self, param, ParameterFloat(name=param, default_value=float(arg_val)))\\n\"\n",
    "        )\n",
    "        flow_file.write(\n",
    "            f\"{ind}{ind}{ind}{ind}{ind}elif self.param_types[param] == 'str':\\n\"\n",
    "        )\n",
    "        flow_file.write(\n",
    "            f\"{ind}{ind}{ind}{ind}{ind}{ind}setattr(self, param, ParameterString(name=param, default_value=arg_val))\\n\"\n",
    "        )\n",
    "        flow_file.write(\"\\n\")\n",
    "\n",
    "        flow_file.write(f\"{ind}def get_pipeline(self) -> Pipeline:\\n\")\n",
    "        flow_file.write(\n",
    "            f\"{ind}{ind}pipeline_steps = [getattr(self, step)() for step in self.steps]\\n\"\n",
    "        )\n",
    "        flow_file.write(f\"{ind}{ind}if len(self.args) > 0:\\n\")\n",
    "        flow_file.write(f\"{ind}{ind}{ind}self.set_run_params()\\n\")\n",
    "        flow_file.write(\"\\n\")\n",
    "\n",
    "        flow_file.write(f\"{ind}{ind}pipeline = Pipeline(\\n\")\n",
    "        flow_file.write(f\"{ind}{ind}{ind}name=self.flow_base_key.replace('_', '-'),\\n\")\n",
    "        flow_file.write(f\"{ind}{ind}{ind}parameters=[\\n\")\n",
    "        for param_name in params.keys():\n",
    "            flow_file.write(f\"{ind}{ind}{ind}{ind}self.{param_name},\\n\")\n",
    "        flow_file.write(f\"{ind}{ind}{ind}],\\n\")\n",
    "        flow_file.write(f\"{ind}{ind}{ind}steps = pipeline_steps,\\n\")\n",
    "        flow_file.write(f\"{ind}{ind}{ind}sagemaker_session = self.sagemaker_session,\\n\")\n",
    "        flow_file.write(f\"{ind}{ind})\\n\")\n",
    "        flow_file.write(f\"{ind}{ind}return pipeline\\n\")\n",
    "        flow_file.write(\"\\n\")\n",
    "\n",
    "        flow_file.write(f\"{ind}def __init__(self):\\n\")\n",
    "        flow_file.write(f\"{ind}{ind}self.bucket = os.environ['SCIFLOW_BUCKET']\\n\")\n",
    "        flow_file.write(f\"{ind}{ind}self.role = sagemaker.get_execution_role()\\n\")\n",
    "        flow_file.write(f\"{ind}{ind}self.region = 'eu-west-1'\\n\")\n",
    "        flow_file.write(\n",
    "            f\"{ind}{ind}self.sagemaker_session = Session(default_bucket=self.bucket)\\n\\n\"\n",
    "        )\n",
    "        flow_file.write(\n",
    "            f'{ind}{ind}self.flow_base_key = \"{extract_module_only(module_name)}\"\\n'\n",
    "        )\n",
    "        flow_file.write(\n",
    "            f\"{ind}{ind}run_timestamp = datetime.today().__str__().replace(':', '-').replace('.', '-').replace(' ', '-')[:-3]\\n\"\n",
    "        )\n",
    "        flow_file.write(f'{ind}{ind}self.flow_run_id = f\"pipeline-{{run_timestamp}}\"\\n')\n",
    "        flow_file.write(\n",
    "            f'{ind}{ind}self.s3_prefix = f\"{{self.flow_base_key}}/{{self.flow_run_id}}\"\\n'\n",
    "        )\n",
    "        flow_file.write(\n",
    "            f'{ind}{ind}self.flow_s3_uri = f\"s3://{{self.bucket}}/{{self.s3_prefix}}\"\\n'\n",
    "        )\n",
    "        flow_file.write(f'{ind}{ind}self.s3_client = boto3.client(\"s3\")\\n')\n",
    "\n",
    "        proc_steps = [s for s in steps if is_processing_step(s)]\n",
    "        lib_reqs_path = Path(lib_path(), \"requirements.txt\")\n",
    "        if not lib_reqs_path.exists():\n",
    "            logger.info(f\"No existing library requirements file found at: {lib_reqs_path} - generating a new one.\")\n",
    "            determine_dependencies(generated_pip_file_name=\"requirements.txt\")\n",
    "        flow_reqs_path = Path(\n",
    "            lib_path(), config.flows_path, \"sagemaker\", \"requirements.txt\"\n",
    "        )\n",
    "        if not flow_reqs_path.exists():\n",
    "            logger.info(f\"No existing flow requirements file found at: {flow_reqs_path} - copying the library requirements file from: {lib_reqs_path}\")\n",
    "            shutil.copyfile(lib_reqs_path, flow_reqs_path)\n",
    "        flow_file.write(\n",
    "            f'{ind}{ind}self.s3_client.upload_file(\"{flow_reqs_path}\", self.bucket, f\"{{self.s3_prefix}}/requirements.txt\")\\n'\n",
    "        )\n",
    "        for proc_step in proc_steps:\n",
    "            step_module = (\n",
    "                f\"_sciflow_{extract_module_only(module_name)}_{proc_step.name}.py\"\n",
    "            )\n",
    "            flow_file.write(\n",
    "                f'{ind}{ind}self.s3_client.upload_file(\"{Path(lib_path(), config.flows_path, \"sagemaker\", step_module)}\", self.bucket, f\"{{self.s3_prefix}}/code/{step_module}\")\\n'\n",
    "            )\n",
    "        modules_dir = Path(lib_path(), config.lib_name)\n",
    "        flow_file.write(\n",
    "            f'{ind}{ind}self.lib_code_key = f\"{{self.s3_prefix}}/code/{config.lib_name}\"\\n'\n",
    "        )\n",
    "        flow_file.write(\n",
    "            f'{ind}{ind}upload_directory(self.s3_client, \"{modules_dir}\", self.bucket, self.lib_code_key)\\n'\n",
    "        )\n",
    "\n",
    "        flow_file.write(\"\\n\")\n",
    "        flow_file.write(f\"{ind}def show(self):\\n\")\n",
    "        flow_file.write(f\"{ind}{ind}pipeline = self.get_pipeline()\\n\")\n",
    "        flow_file.write(f\"{ind}{ind}pipeline.upsert(role_arn=self.role)\\n\")\n",
    "        flow_file.write(f\"{ind}{ind}description = pipeline.describe()\\n\")\n",
    "        flow_file.write(f'{ind}{ind}print(\"Sciflow generated pipeline is valid\")\\n')\n",
    "        flow_file.write(\n",
    "            f\"{ind}{ind}print(f\\\"Pipeline name: {{description['PipelineName']}}\\\")\\n\"\n",
    "        )\n",
    "        flow_file.write(\n",
    "            f\"{ind}{ind}print(f\\\"Pipeline ARN: {{description['PipelineArn']}}\\\")\\n\"\n",
    "        )\n",
    "        flow_file.write(\"\\n\")\n",
    "\n",
    "        flow_file.write(f\"{ind}def run(self):\\n\")\n",
    "        flow_file.write(f\"{ind}{ind}pipeline = self.get_pipeline()\\n\")\n",
    "        flow_file.write(f\"{ind}{ind}pipeline.upsert(role_arn=self.role)\\n\")\n",
    "        flow_file.write(f\"{ind}{ind}execution = pipeline.start()\\n\")\n",
    "        flow_file.write(\n",
    "            f'{ind}{ind}print(f\"Starting Sciflow generated pipeline: {{self.flow_run_id}}\")\\n'\n",
    "        )\n",
    "        flow_file.write(f\"{ind}{ind}print(execution.describe())\\n\")\n",
    "        flow_file.write(f\"{ind}{ind}execution.wait()\\n\")\n",
    "        flow_file.write(\"\\n\")\n",
    "\n",
    "        flow_file.write('if __name__ == \"__main__\":\\n')\n",
    "        flow_file.write(f\"{ind}if len(sys.argv) == 1:\\n\")\n",
    "        flow_file.write(f\"{ind}{ind}{pipeline_class_name}().show()\\n\")\n",
    "        flow_file.write(f\"{ind}else:\\n\")\n",
    "        flow_file.write(f\"{ind}{ind}if sys.argv[1] == 'show':\\n\")\n",
    "        flow_file.write(f\"{ind}{ind}{ind}{pipeline_class_name}().show()\\n\")\n",
    "        flow_file.write(f\"{ind}{ind}if sys.argv[1] == 'run':\\n\")\n",
    "\n",
    "        flow_file.write(f\"{ind}{ind}{ind}flow = {pipeline_class_name}()\\n\")\n",
    "        flow_file.write(f\"{ind}{ind}{ind}if len(sys.argv[2:]) > 0:\\n\")\n",
    "        flow_file.write(f\"{ind}{ind}{ind}{ind}args = sys.argv[2:]\\n\")\n",
    "        flow_file.write(\n",
    "            f\"{ind}{ind}{ind}{ind}flow.args = dict(list(zip(args, args[1:]))[::2])\\n\"\n",
    "        )\n",
    "        flow_file.write(f\"{ind}{ind}{ind}flow.run()\\n\")\n",
    "\n",
    "        return steps_param_meta, steps_vars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `write_script_processor`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def write_script_processor(flow_file, ind, proc_image_uri, proc_instance_type):\n",
    "    flow_file.write(f\"{ind}{ind}script_processor = ScriptProcessor(\\n\")\n",
    "    flow_file.write(f\"{ind}{ind}{ind}command=['python3'],\\n\")\n",
    "    flow_file.write(f'{ind}{ind}{ind}image_uri=\"{proc_image_uri}\",\\n')\n",
    "    flow_file.write(f\"{ind}{ind}{ind}role=self.role,\\n\")\n",
    "    flow_file.write(f\"{ind}{ind}{ind}instance_count=1,\\n\")\n",
    "    flow_file.write(f'{ind}{ind}{ind}instance_type=\"{proc_instance_type}\",\\n')\n",
    "    flow_file.write(f\"{ind}{ind}{ind}sagemaker_session=self.sagemaker_session,\\n\")\n",
    "    flow_file.write(f'{ind}{ind}{ind}env={{\"AWS_DEFAULT_REGION\": self.region,\\n')\n",
    "    flow_file.write(f'{ind}{ind}{ind}{ind}{ind}\"SCIFLOW_BUCKET\": self.bucket}}\\n')\n",
    "    flow_file.write(f\"{ind}{ind})\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `extract_step_vars`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def extract_step_vars(step, param_names, processing_flow_scope, train_flow_scope):\n",
    "    logger.debug(\n",
    "        f\"Extracting step variables: {step.name} params: {param_names} proc scope: {processing_flow_scope} train scope: {train_flow_scope}\"\n",
    "    )\n",
    "    if len(step.args) == 0:\n",
    "        result = {}\n",
    "    else:\n",
    "        args = [x.strip() for x in step.args.split(\",\")]\n",
    "        step_input = [a for a in args if a in param_names]\n",
    "        step_proc_vars = [a for a in args if a in processing_flow_scope]\n",
    "        step_train_vars = [a for a in args if a in train_flow_scope]\n",
    "        unscoped_vars = set(args).difference(\n",
    "            set(step_input + step_proc_vars + step_train_vars)\n",
    "        )\n",
    "        if len(unscoped_vars) > 0:\n",
    "            raise ValueError(\n",
    "                f'Step: {step.name} depends on variable(s) not in flow scope: \"{unscoped_vars}\"'\n",
    "            )\n",
    "        result = {\n",
    "            \"step_input\": step_input,\n",
    "            \"step_proc_vars\": step_proc_vars,\n",
    "            \"step_train_vars\": step_train_vars,\n",
    "        }\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "no_arg_step = FuncDetails(\n",
    "    \"no_arg_step\", docstring=None, args=\"\", has_return=False, return_stmt=None, code=\"\"\n",
    ")\n",
    "single_arg_step = FuncDetails(\n",
    "    \"fit_single_arg_step\",\n",
    "    docstring=None,\n",
    "    args=\"one_param\",\n",
    "    has_return=False,\n",
    "    return_stmt=None,\n",
    "    code=\"\",\n",
    ")\n",
    "multi_arg_step = FuncDetails(\n",
    "    \"multi_arg_step\",\n",
    "    docstring=None,\n",
    "    args=\"one_param,proc_param\",\n",
    "    has_return=False,\n",
    "    return_stmt=None,\n",
    "    code=\"\",\n",
    ")\n",
    "post_train_step = FuncDetails(\n",
    "    \"post_train_step\",\n",
    "    docstring=None,\n",
    "    args=\"one_param,proc_param,train_param\",\n",
    "    has_return=False,\n",
    "    return_stmt=None,\n",
    "    code=\"\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "assert extract_step_vars(no_arg_step, [\"one_param\"], [\"two_param\"], []) == {}\n",
    "assert extract_step_vars(no_arg_step, [], [], []) == {}\n",
    "raised = False\n",
    "try:\n",
    "    extract_step_vars(single_arg_step, [], [], []) == {}\n",
    "except:\n",
    "    raised = True\n",
    "assert raised\n",
    "assert extract_step_vars(single_arg_step, [\"one_param\"], [\"proc_param\"], []) == {\n",
    "    \"step_input\": [\"one_param\"],\n",
    "    \"step_proc_vars\": [],\n",
    "    \"step_train_vars\": [],\n",
    "}\n",
    "assert extract_step_vars(multi_arg_step, [\"one_param\"], [\"proc_param\"], []) == {\n",
    "    \"step_input\": [\"one_param\"],\n",
    "    \"step_proc_vars\": [\"proc_param\"],\n",
    "    \"step_train_vars\": [],\n",
    "}\n",
    "assert extract_step_vars(\n",
    "    post_train_step, [\"one_param\"], [\"proc_param\"], [\"train_param\"]\n",
    ") == {\n",
    "    \"step_input\": [\"one_param\"],\n",
    "    \"step_proc_vars\": [\"proc_param\"],\n",
    "    \"step_train_vars\": [\"train_param\"],\n",
    "}\n",
    "raised = False\n",
    "try:\n",
    "    assert extract_step_vars(multi_arg_step, [\"one_param\"], [], [])\n",
    "except:\n",
    "    raised = True\n",
    "assert raised"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `write_steps`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def write_steps(\n",
    "    flow_path,\n",
    "    lib_name,\n",
    "    module_name,\n",
    "    fq_module_name,\n",
    "    flow_file,\n",
    "    steps,\n",
    "    param_meta,\n",
    "    ind,\n",
    "    proc_image_uri,\n",
    "    proc_instance_type,\n",
    "    train_image_uri,\n",
    "    train_instance_type,\n",
    "):\n",
    "    steps_param_meta = {}\n",
    "    steps_vars = {}\n",
    "    param_names = list(param_meta.keys())\n",
    "    outputs = {}\n",
    "    proc_flow_scope = []\n",
    "    train_flow_scope = []\n",
    "\n",
    "    module_local_name = extract_module_only(module_name)\n",
    "\n",
    "    for i, step in enumerate(steps):\n",
    "        return_vars = extract_return_var_names(step)\n",
    "        logger.debug(f\"Extracted return vars: {return_vars} from step: {step.name}\")\n",
    "\n",
    "        step_vars = extract_step_vars(\n",
    "            step, param_names, proc_flow_scope, train_flow_scope\n",
    "        )\n",
    "        steps_vars[step.name] = step_vars\n",
    "\n",
    "        flow_file.write(f\"{ind}def {step.name}(self):\\n\")\n",
    "        if step.docstring:\n",
    "            flow_file.write(f\"{indent_multiline(step.docstring, 2)}\\n\")\n",
    "\n",
    "        if is_processing_step(step):\n",
    "            step_param_meta = _write_processing_step(\n",
    "                return_vars,\n",
    "                step_vars,\n",
    "                flow_file,\n",
    "                flow_path,\n",
    "                ind,\n",
    "                step,\n",
    "                param_meta,\n",
    "                outputs,\n",
    "                lib_name,\n",
    "                proc_image_uri,\n",
    "                proc_instance_type,\n",
    "                module_local_name,\n",
    "            )\n",
    "            steps_param_meta.update(step_param_meta)\n",
    "            proc_flow_scope.extend(return_vars)\n",
    "        elif is_train_step(step):\n",
    "            step_param_meta = _write_train_step(\n",
    "                return_vars,\n",
    "                step_vars,\n",
    "                flow_file,\n",
    "                flow_path,\n",
    "                ind,\n",
    "                step,\n",
    "                param_meta,\n",
    "                outputs,\n",
    "                lib_name,\n",
    "                train_image_uri,\n",
    "                train_instance_type,\n",
    "                module_local_name,\n",
    "            )\n",
    "            steps_param_meta.update(step_param_meta)\n",
    "            train_flow_scope.extend(return_vars)\n",
    "\n",
    "        flow_file.write(f\"{ind}{ind}self.{step.name}_step = {step.name}_step\\n\")\n",
    "        flow_file.write(f\"{ind}{ind}return {step.name}_step\\n\")\n",
    "        flow_file.write(\"\\n\")\n",
    "        \n",
    "    if \"promote\" in param_names:\n",
    "        _write_manual_promote_step(flow_file, ind)\n",
    "\n",
    "    return steps_param_meta, steps_vars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `_write_processing_step`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def _write_processing_step(\n",
    "    return_vars,\n",
    "    step_vars,\n",
    "    flow_file,\n",
    "    flow_path,\n",
    "    ind,\n",
    "    step,\n",
    "    param_meta,\n",
    "    outputs,\n",
    "    lib_name,\n",
    "    proc_image_uri,\n",
    "    proc_instance_type,\n",
    "    module_local_name,\n",
    "):\n",
    "    steps_param_meta = {}\n",
    "\n",
    "    write_script_processor(flow_file, ind, proc_image_uri, proc_instance_type)\n",
    "    flow_file.write(\"\\n\")\n",
    "    flow_file.write(f\"{ind}{ind}{step.name}_step = ProcessingStep(\\n\")\n",
    "    flow_file.write(f'{ind}{ind}{ind}name = \"{step.name}\",\\n')\n",
    "    flow_file.write(f\"{ind}{ind}{ind}processor = script_processor,\\n\")\n",
    "    flow_file.write(\n",
    "        f'{ind}{ind}{ind}code = f\"{{self.flow_s3_uri}}/code/_sciflow_{module_local_name}_{step.name}.py\",\\n'\n",
    "    )\n",
    "    job_arg_pairs = [\n",
    "        (\"--bucket_name\", \"self.bucket\"),\n",
    "        (\"--flow_base_key\", \"self.flow_base_key\"),\n",
    "        (\"--flow_run_id\", \"self.flow_run_id\"),\n",
    "        (\"--lib_name\", f'\"{lib_name}\"'),\n",
    "        (\"--remote_key\", \"self.lib_code_key\"),\n",
    "    ]\n",
    "    flow_file.write(f\"{ind}{ind}{ind}inputs = [\\n\")\n",
    "    flow_file.write(\n",
    "        f'{ind}{ind}{ind}{ind}ProcessingInput(source=f\"{{self.flow_s3_uri}}/requirements.txt\", destination=\"/opt/ml/processing/requirements\"),\\n'\n",
    "    )\n",
    "\n",
    "    if len(step_vars) > 0:\n",
    "        step_param_meta = {k: param_meta[k] for k in step_vars[\"step_input\"]}\n",
    "        steps_param_meta[step.name] = step_param_meta\n",
    "        if len(step_param_meta) > 0:\n",
    "            # Job Args\n",
    "            job_args = format_job_arguments(step_param_meta)\n",
    "            job_arg_pairs.extend(zip(job_args[::2], job_args[1::2]))\n",
    "\n",
    "        # ProcInputs\n",
    "        if (\n",
    "            len(step_vars[\"step_proc_vars\"]) > 0\n",
    "            or len(step_vars[\"step_train_vars\"]) > 0\n",
    "        ):\n",
    "            # For now all output artefacts from training go into model directory. Not yet using the recommended /opt/ml/output/data\n",
    "            # https://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms-training-algo-output.html\n",
    "            proc_inputs = (\n",
    "                step_vars[\"step_proc_vars\"]\n",
    "                if len(step_vars[\"step_train_vars\"]) == 0\n",
    "                else step_vars[\"step_proc_vars\"] + [\"model\"]\n",
    "            )\n",
    "            flow_file.write(\n",
    "                \"\\n\".join(\n",
    "                    [\n",
    "                        f'{ind}{ind}{ind}{ind}ProcessingInput(source=self.{outputs[cv]}, destination=\"/opt/ml/processing/input/{cv}\"),'\n",
    "                        for cv in proc_inputs\n",
    "                    ]\n",
    "                )\n",
    "            )\n",
    "    flow_file.write(f\"{ind}{ind}{ind}],\\n\")\n",
    "\n",
    "    if len(return_vars) > 0:\n",
    "        # ProcOutputs\n",
    "        proc_outs = {\n",
    "            (\n",
    "                v,\n",
    "                f'{step.name}_step.properties.ProcessingOutputConfig.Outputs[\"{v}\"].S3Output.S3Uri',\n",
    "            )\n",
    "            for v in return_vars\n",
    "        }\n",
    "        outputs.update(proc_outs)\n",
    "\n",
    "        if len(proc_outs) > 0:\n",
    "            flow_file.write(f\"{ind}{ind}{ind}outputs = [\\n\")\n",
    "            flow_file.write(\n",
    "                \"\\n\".join(\n",
    "                    [\n",
    "                        f'{ind}{ind}{ind}{ind}ProcessingOutput(destination=f\"{{self.flow_s3_uri}}/{v}\", output_name=\"{v}\", source=\"/opt/ml/processing/output/{v}\"),'\n",
    "                        for v in [x[0] for x in proc_outs]\n",
    "                    ]\n",
    "                )\n",
    "                + \"\\n\"\n",
    "            )\n",
    "            flow_file.write(f\"{ind}{ind}{ind}],\\n\")\n",
    "\n",
    "    flow_file.write(f\"{ind}{ind}{ind}job_arguments=[\\n\")\n",
    "    for job_arg_pair in job_arg_pairs:\n",
    "        flow_file.write(\n",
    "            f'{ind}{ind}{ind}{ind}\"{job_arg_pair[0]}\", {job_arg_pair[1]},\\n'\n",
    "        )\n",
    "    flow_file.write(f\"{ind}{ind}{ind}],\\n\")\n",
    "\n",
    "    flow_file.write(f\"{ind}{ind})\\n\")\n",
    "    return steps_param_meta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `_write_train_step`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def _write_train_step(\n",
    "    return_vars,\n",
    "    step_vars,\n",
    "    flow_file,\n",
    "    flow_path,\n",
    "    ind,\n",
    "    step,\n",
    "    param_meta,\n",
    "    outputs,\n",
    "    lib_name,\n",
    "    train_image_uri,\n",
    "    train_instance_type,\n",
    "    module_local_name,\n",
    "):\n",
    "    steps_param_meta = {}\n",
    "\n",
    "    if len(step_vars) > 0:\n",
    "        train_outs = {\n",
    "            (v, f\"{step.name}_step.properties.ModelArtifacts.S3ModelArtifacts\")\n",
    "            for v in return_vars\n",
    "        }\n",
    "        outputs.update(train_outs)\n",
    "\n",
    "        flow_file.write(f\"{ind}{ind}metrics_regex = None\\n\")\n",
    "        flow_file.write(f\"{ind}{ind}if 'metric_names' in self.__dict__:\\n\")\n",
    "        flow_file.write(\n",
    "            f'{ind}{ind}{ind}metrics = self.metric_names.split(\",\")\\n',\n",
    "        )\n",
    "        flow_file.write(\n",
    "            f'{ind}{ind}{ind}metrics_regex = [{{\"Name\": m, \"Regex\": f\"{{m}}=(.*?);\"}} for m in metrics]\\n'\n",
    "        )\n",
    "        flow_file.write(f\"\\n\")\n",
    "        flow_file.write(f\"{ind}{ind}estimator = Estimator(\\n\")\n",
    "        flow_file.write(f'{ind}{ind}{ind}image_uri = \"{train_image_uri}\",\\n')\n",
    "        flow_file.write(\n",
    "            f'{ind}{ind}{ind}entry_point = \"_sciflow_{module_local_name}_{step.name}.py\",\\n'\n",
    "        )\n",
    "        flow_file.write(f'{ind}{ind}{ind}source_dir = \"{flow_path.parent}\",\\n')\n",
    "        # Repeated code - refactor\n",
    "        step_param_meta = {k: param_meta[k] for k in step_vars[\"step_input\"]}\n",
    "        steps_param_meta[step.name] = step_param_meta\n",
    "        hyper_params = {\n",
    "            \"bucket_name\": \"self.bucket\",\n",
    "            \"flow_base_key\": \"self.flow_base_key\",\n",
    "            \"flow_run_id\": \"self.flow_run_id\",\n",
    "            \"lib_name\": f'\"{lib_name}\"',\n",
    "            \"remote_key\": \"self.lib_code_key\",\n",
    "        }\n",
    "        if len(step_vars[\"step_input\"]) > 0:\n",
    "            hyper_params.update(format_hyperparams(step_param_meta))\n",
    "        flow_file.write(f\"{ind}{ind}{ind}hyperparameters = {{\\n\")\n",
    "        for key, val in hyper_params.items():\n",
    "            flow_file.write(f'{ind}{ind}{ind}{ind}\"{key}\": {val},\\n')\n",
    "        flow_file.write(f\"{ind}{ind}{ind}}},\\n\")\n",
    "        flow_file.write(f'{ind}{ind}{ind}instance_type = \"{train_instance_type}\",\\n')\n",
    "        flow_file.write(f\"{ind}{ind}{ind}instance_count = 1,\\n\")\n",
    "        flow_file.write(f\"{ind}{ind}{ind}output_path = self.flow_s3_uri,\\n\")\n",
    "        flow_file.write(f'{ind}{ind}{ind}base_job_name = \"{step.name}\",\\n')\n",
    "        flow_file.write(\n",
    "            f'{ind}{ind}{ind}code_location = f\"{{self.flow_s3_uri}}/code\",\\n'\n",
    "        )\n",
    "        flow_file.write(f\"{ind}{ind}{ind}sagemaker_session = self.sagemaker_session,\\n\")\n",
    "        flow_file.write(f\"{ind}{ind}{ind}role = self.role,\\n\")\n",
    "        flow_file.write(f\"{ind}{ind}{ind}metric_definitions=metrics_regex,\\n\")\n",
    "        flow_file.write(f\"{ind}{ind}{ind}enable_sagemaker_metrics=True,\\n\")\n",
    "        flow_file.write(\n",
    "            f\"{ind}{ind}{ind}environment={{'AWS_DEFAULT_REGION': self.region,\\n\"\n",
    "        )\n",
    "        flow_file.write(f\"{ind}{ind}{ind}{ind}{ind}'SCIFLOW_BUCKET': self.bucket}}\\n\")\n",
    "        flow_file.write(f\"{ind}{ind})\\n\")\n",
    "        flow_file.write(\"\\n\")\n",
    "        flow_file.write(f\"{ind}{ind}{step.name}_step = TrainingStep(\\n\")\n",
    "        flow_file.write(f'{ind}{ind}{ind}name=\"{step.name}\",\\n')\n",
    "        flow_file.write(f\"{ind}{ind}{ind}estimator=estimator,\\n\")\n",
    "        if \"step_proc_vars\" in step_vars and len(step_vars[\"step_proc_vars\"]) > 0:\n",
    "            flow_file.write(f\"{ind}{ind}{ind}inputs={{\\n\")\n",
    "            for training_input in step_vars[\"step_proc_vars\"]:\n",
    "                flow_file.write(\n",
    "                    f'{ind}{ind}{ind}{ind}\"{training_input}\": TrainingInput(\\n'\n",
    "                )\n",
    "                # TODO store content type mapping\n",
    "                # parquet should be: \"application/octet-stream\"\n",
    "                flow_file.write(\n",
    "                    f'{ind}{ind}{ind}{ind}{ind}s3_data=self.{outputs[training_input]}, content_type=\"text/csv\"\\n'\n",
    "                )\n",
    "                flow_file.write(f\"{ind}{ind}{ind}{ind}),\\n\")\n",
    "            flow_file.write(f\"{ind}{ind}{ind}}}\\n\")\n",
    "        flow_file.write(f\"{ind}{ind})\\n\")\n",
    "        flow_file.write(f\"{ind}{ind}self.estimator = estimator\\n\")\n",
    "    return steps_param_meta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Manual Promotion Step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `_write_manual_promote_step`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def _write_manual_promote_step(flow_file, ind):\n",
    "    # TODO preopr logic for detecting eval step\n",
    "    has_eval_step = True\n",
    "    if has_eval_step:\n",
    "        flow_file.write(f\"{ind}def cond_promote(self):\\n\")\n",
    "        flow_file.write(f\"{ind}{ind}model_metrics = ModelMetrics(\\n\")\n",
    "        flow_file.write(f\"{ind}{ind}{ind}model_statistics=MetricsSource(\\n\")\n",
    "        flow_file.write(f\"{ind}{ind}{ind}{ind}s3_uri=Join(\\n\")\n",
    "        flow_file.write(f\"{ind}{ind}{ind}{ind}{ind}on=\\\"/\\\",\\n\")\n",
    "        flow_file.write(f\"{ind}{ind}{ind}{ind}{ind}values=[\\n\")\n",
    "        flow_file.write(f\"{ind}{ind}{ind}{ind}{ind}{ind}self.evaluate_step.arguments[\\\"ProcessingOutputConfig\\\"][\\\"Outputs\\\"][0][\\\"S3Output\\\"][\\n\")\n",
    "        flow_file.write(f\"{ind}{ind}{ind}{ind}{ind}{ind}{ind}\\\"S3Uri\\\"\\n\")\n",
    "        flow_file.write(f\"{ind}{ind}{ind}{ind}{ind}{ind}],\\n\")\n",
    "        flow_file.write(f\"{ind}{ind}{ind}{ind}{ind}{ind}\\\"evaluation_report\\\",\\n\")\n",
    "        flow_file.write(f\"{ind}{ind}{ind}{ind}{ind}],\\n\")\n",
    "        flow_file.write(f\"{ind}{ind}{ind}{ind}),\\n\")\n",
    "        flow_file.write(f\"{ind}{ind}{ind}{ind}content_type=\\\"application/json\\\",\\n\")\n",
    "        flow_file.write(f\"{ind}{ind}{ind})\\n\")\n",
    "        flow_file.write(f\"{ind}{ind})\\n\")\n",
    "        \n",
    "    flow_file.write(f\"{ind}{ind}step_register_model = RegisterModel(\\n\")\n",
    "    flow_file.write(f\"{ind}{ind}{ind}{ind}name=\\\"Promote\\\",\\n\")\n",
    "    flow_file.write(f\"{ind}{ind}{ind}{ind}estimator=self.estimator,\\n\")\n",
    "    # TODO don't hardcode train step\n",
    "    flow_file.write(f\"{ind}{ind}{ind}{ind}model_data=self.fit_step.properties.ModelArtifacts.S3ModelArtifacts,\\n\")\n",
    "    flow_file.write(f\"{ind}{ind}{ind}{ind}content_types=[\\\"text/csv\\\"],\\n\")\n",
    "    flow_file.write(f\"{ind}{ind}{ind}{ind}response_types=[\\\"text/csv\\\"],\\n\")\n",
    "    # Params/config for inference and transform instances\n",
    "    flow_file.write(f\"{ind}{ind}{ind}{ind}inference_instances=[\\\"ml.t2.medium\\\", \\\"ml.m5.xlarge\\\", \\\"ml.m5.large\\\"],\\n\")\n",
    "    flow_file.write(f\"{ind}{ind}{ind}{ind}transform_instances=[\\\"ml.m5.xlarge\\\"],\\n\")\n",
    "    flow_file.write(f\"{ind}{ind}{ind}{ind}model_package_group_name=self.flow_base_key,\\n\")\n",
    "    flow_file.write(f\"{ind}{ind}{ind}{ind}model_metrics=model_metrics,\\n\")\n",
    "    flow_file.write(f\"{ind}{ind})\\n\")\n",
    "        \n",
    "    flow_file.write(f\"{ind}{ind}cond_eq = ConditionEquals(\\n\")\n",
    "    flow_file.write(f\"{ind}{ind}{ind}left=self.promote,\\n\")\n",
    "    flow_file.write(f\"{ind}{ind}{ind}right=True,\\n\")\n",
    "    flow_file.write(f\"{ind}{ind})\\n\")\n",
    "    flow_file.write(f\"{ind}{ind}step_cond = ConditionStep(\\n\")\n",
    "    flow_file.write(f\"{ind}{ind}{ind}name=\\\"Manual-Promote-Condition\\\",\\n\")\n",
    "    flow_file.write(f\"{ind}{ind}{ind}conditions=[cond_eq],\\n\")\n",
    "    flow_file.write(f\"{ind}{ind}{ind}if_steps=[step_register_model],\\n\")\n",
    "    flow_file.write(f\"{ind}{ind}{ind}else_steps=[],\\n\")\n",
    "    flow_file.write(f\"{ind}{ind})\\n\")\n",
    "    flow_file.write(f\"{ind}{ind}self.cond_promote_step = step_cond\\n\")\n",
    "    flow_file.write(f\"{ind}{ind}return step_cond\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `generate_sagemaker_modules`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def generate_sagemaker_modules(\n",
    "    flow_path,\n",
    "    pipeline_class_name,\n",
    "    lib_name,\n",
    "    module_name,\n",
    "    steps,\n",
    "    params,\n",
    "    param_meta,\n",
    "    steps_param_meta,\n",
    "    steps_vars,\n",
    "):\n",
    "    ind = \"    \"\n",
    "    module_path = Path(lib_path(), lib_name, f\"{module_name.replace('.', '/')}.py\")\n",
    "\n",
    "    with open(module_path) as module_file:\n",
    "        module_lines = module_file.readlines()\n",
    "    lib_refs = []\n",
    "    lines = []\n",
    "    for line in module_lines:\n",
    "        if line.startswith(\"from ..\"):\n",
    "            lib_refs.append(line)\n",
    "        else:\n",
    "            lines.append(line)\n",
    "    lib_refs = [l.replace(\"..\", f\"{lib_name}.\") for l in lib_refs]\n",
    "\n",
    "    for step in steps:\n",
    "        sm_module_path = Path(\n",
    "            flow_path.parent, \"_sciflow_\" + flow_path.stem + f\"_{step.name}.py\"\n",
    "        )\n",
    "\n",
    "        main_args = [\n",
    "            \"lib_name\",\n",
    "            \"remote_key\",\n",
    "            \"bucket_name\",\n",
    "            \"flow_base_key\",\n",
    "            \"flow_run_id\",\n",
    "        ]\n",
    "\n",
    "        with open(sm_module_path, \"w\") as sm_module_file:\n",
    "            sm_module_file.write(\n",
    "                \"# SCIFLOW GENERATED SAGEMAKER MODULE - EDIT COMPANION NOTEBOOK\\n\"\n",
    "            )\n",
    "            sm_module_file.write(\"import boto3\\n\")\n",
    "            sm_module_file.write(\"import os\\n\")\n",
    "            sm_module_file.write(\"import sys\\n\")\n",
    "            sm_module_file.write(\"import pandas as pd\\n\")\n",
    "            sm_module_file.write(\"import pickle\\n\")\n",
    "            sm_module_file.write(\"from pathlib import Path\\n\")\n",
    "            sm_module_file.write(\"import argparse\\n\")\n",
    "            sm_module_file.write(\"import subprocess\\n\")\n",
    "\n",
    "            if is_processing_step(step):\n",
    "                sm_module_file.write(\n",
    "                    f'has_additional_dependencies = Path(\"/opt/ml/processing/requirements/requirements.txt\").exists()\\n'\n",
    "                )\n",
    "                sm_module_file.write(f\"if has_additional_dependencies:\\n\")\n",
    "                sm_module_file.write(\n",
    "                    f\"{ind}print('Installing additional dependencies from requirements.txt')\\n\"\n",
    "                )\n",
    "                sm_module_file.write(\n",
    "                    f'{ind}subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-r\", \"/opt/ml/processing/requirements/requirements.txt\"])\\n'\n",
    "                )\n",
    "                sm_module_file.write(\n",
    "                    f'{ind}print(\"Installed additional dependencies\")\\n'\n",
    "                )\n",
    "            sm_module_file.write(\"\\n\")\n",
    "            sm_module_file.write(\"\".join(lines))\n",
    "            sm_module_file.write(\"\\n\")\n",
    "            write_preamble(step, sm_module_file, ind)\n",
    "            sm_module_file.write(\"\\n\")\n",
    "            sm_module_file.write(\"\\n\")\n",
    "            if step.name in steps_param_meta and len(steps_param_meta[step.name]) > 0:\n",
    "                step_args = list(steps_param_meta[step.name].keys())\n",
    "                sm_module_file.write(\n",
    "                    f\"def main({', '.join(main_args)}, {', '.join(step_args)}):\\n\"\n",
    "                )\n",
    "            else:\n",
    "                sm_module_file.write(f\"def main({', '.join(main_args)}):\\n\")\n",
    "            if (\n",
    "                is_processing_step(step)\n",
    "                and \"step_train_vars\" in steps_vars[step.name]\n",
    "                and len(steps_vars[step.name][\"step_train_vars\"]) > 0\n",
    "            ):\n",
    "                sm_module_file.write(f\"\\n\")\n",
    "                sm_module_file.write(f\"{ind}import tarfile\\n\")\n",
    "                sm_module_file.write(\n",
    "                    f'{ind}model_path = f\"/opt/ml/processing/input/model/model.tar.gz\"\\n'\n",
    "                )\n",
    "                sm_module_file.write(f\"{ind}with tarfile.open(model_path) as tar:\\n\")\n",
    "                sm_module_file.write(f'{ind}{ind}tar.extractall(path=\".\")\\n')\n",
    "                sm_module_file.write(f\"\\n\")\n",
    "\n",
    "            sm_module_file.write(f\"{ind}add_lib_to_pythonpath(lib_name, remote_key)\\n\")\n",
    "            sm_module_file.write(\"\".join([f\"{ind}{lr}\" for lr in lib_refs]))\n",
    "\n",
    "            if is_processing_step(step):\n",
    "                if (\n",
    "                    \"step_train_vars\" in steps_vars[step.name]\n",
    "                    and len(steps_vars[step.name][\"step_train_vars\"]) > 0\n",
    "                ):\n",
    "                    for step_train_var in steps_vars[step.name][\"step_train_vars\"]:\n",
    "                        sm_module_file.write(\n",
    "                            f'{ind}{step_train_var} = load_result(\".\", \"{step_train_var}\")\\n'\n",
    "                        )\n",
    "                if (\n",
    "                    \"step_proc_vars\" in steps_vars[step.name]\n",
    "                    and len(steps_vars[step.name][\"step_proc_vars\"]) > 0\n",
    "                ):\n",
    "                    for step_proc_var in steps_vars[step.name][\"step_proc_vars\"]:\n",
    "                        sm_module_file.write(\n",
    "                            f'{ind}{step_proc_var} = load_result(\"/opt/ml/processing/input/{step_proc_var}\", \"{step_proc_var}\")\\n'\n",
    "                        )\n",
    "            elif is_train_step(step):\n",
    "                if (\n",
    "                    \"step_proc_vars\" in steps_vars[step.name]\n",
    "                    and len(steps_vars[step.name][\"step_proc_vars\"]) > 0\n",
    "                ):\n",
    "                    for step_proc_var in steps_vars[step.name][\"step_proc_vars\"]:\n",
    "                        sm_module_file.write(\n",
    "                            f'{ind}{step_proc_var} = load_result(\"/opt/ml/input/data/{step_proc_var}\", \"{step_proc_var}\")\\n'\n",
    "                        )\n",
    "\n",
    "            step_func_args = []\n",
    "            if len(steps_vars[step.name]) > 0:\n",
    "                step_vars = (\n",
    "                    steps_vars[step.name][\"step_proc_vars\"]\n",
    "                    + steps_vars[step.name][\"step_train_vars\"]\n",
    "                )\n",
    "                # load result for each step var\n",
    "                step_func_args.extend(step_vars)\n",
    "            if step.name in steps_param_meta and len(steps_param_meta[step.name]) > 0:\n",
    "                step_params = (\n",
    "                    format_args(steps_param_meta[step.name]).replace(\" \", \"\").split(\",\")\n",
    "                )\n",
    "                main_args.extend(step_args)\n",
    "                step_func_args.extend(step_params)\n",
    "            if len(step_func_args) > 0:\n",
    "                step_func_args = \",\".join(\n",
    "                    [\n",
    "                        f\"{a.replace('int', '').replace('float', '').strip('()')}={a}\"\n",
    "                        for a in step_func_args\n",
    "                    ]\n",
    "                )\n",
    "                step_func_call_text = f\"results = {step.name}({step_func_args})\"\n",
    "            else:\n",
    "                step_func_call_text = f\"results = {step.name}()\"\n",
    "\n",
    "            sm_module_file.write(f\"{ind}{step_func_call_text}\\n\")\n",
    "\n",
    "            if is_processing_step(step):\n",
    "                sm_module_file.write(\n",
    "                    f'{ind}save_results(\"/opt/ml/processing/output\", results)\\n'\n",
    "                )\n",
    "            elif is_train_step(step):\n",
    "                sm_module_file.write(f'{ind}save_results(\"/opt/ml/model\", results)\\n')\n",
    "\n",
    "            sm_module_file.write(f\"\\n\")\n",
    "            sm_module_file.write(f\"def parse_args():\\n\")\n",
    "            sm_module_file.write(\n",
    "                f\"{ind}parser = argparse.ArgumentParser(description=__doc__)\\n\"\n",
    "            )\n",
    "            sm_module_file.write(\n",
    "                f\"{ind}parser.formatter_class = argparse.RawDescriptionHelpFormatter\\n\"\n",
    "            )\n",
    "\n",
    "            for main_arg in main_args:\n",
    "                sm_module_file.write(\n",
    "                    f'{ind}parser.add_argument(f\"--{main_arg}\", required=True)\\n'\n",
    "                )\n",
    "            sm_module_file.write(f\"{ind}return parser.parse_args()\")\n",
    "            sm_module_file.write(f\"\\n\")\n",
    "            sm_module_file.write(\"\\n\")\n",
    "            sm_module_file.write(f'if __name__ == \"__main__\":\\n')\n",
    "            sm_module_file.write(f\"{ind}args = parse_args()\\n\")\n",
    "            sm_module_file.write(f\"{ind}main(**vars(args))\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `write_preamble`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def write_preamble(step, sm_module_file, ind):\n",
    "    sm_module_file.write(\"\\n\")\n",
    "    sm_module_file.write(\n",
    "        f\"def download_directory(bucket_name: str, remote_key: str, local_dir: str):\\n\"\n",
    "    )\n",
    "    sm_module_file.write(f\"{ind}s3_client = boto3.client('s3')\\n\")\n",
    "    sm_module_file.write(f\"{ind}s3_res = boto3.resource('s3')\\n\")\n",
    "    sm_module_file.write(f\"{ind}if not Path(local_dir).exists():\\n\")\n",
    "    sm_module_file.write(f\"{ind}{ind}Path(local_dir).mkdir(parents=True)\\n\")\n",
    "    sm_module_file.write(\n",
    "        f\"{ind}all_files = [obj.key for obj in s3_res.Bucket(bucket_name).objects.filter(Prefix=remote_key)]\\n\"\n",
    "    )\n",
    "    sm_module_file.write(f\"{ind}for file in all_files:\\n\")\n",
    "    sm_module_file.write(\n",
    "        f\"{ind}{ind}file_name = file.replace(remote_key, '').lstrip('/')\\n\"\n",
    "    )\n",
    "    sm_module_file.write(f\"{ind}{ind}local_path = Path(local_dir, file_name)\\n\")\n",
    "    sm_module_file.write(f\"{ind}{ind}if not local_path.parent.exists():\\n\")\n",
    "    sm_module_file.write(f\"{ind}{ind}{ind}local_path.parent.mkdir(parents=True)\\n\")\n",
    "    sm_module_file.write(\n",
    "        f\"{ind}{ind}s3_client.download_file(bucket_name, file, f'{{local_path}}')\\n\"\n",
    "    )\n",
    "    sm_module_file.write(\"\\n\")\n",
    "\n",
    "    sm_module_file.write(\n",
    "        f\"def add_lib_to_pythonpath(lib_name: str, remote_key: str):\\n\"\n",
    "    )\n",
    "    sm_module_file.write(f\"{ind}lib_dir = f'/tmp/{{lib_name}}'\\n\")\n",
    "    sm_module_file.write(f\"{ind}package_dir = f'{{lib_dir}}/{{lib_name}}'\\n\")\n",
    "    sm_module_file.write(\n",
    "        f\"{ind}download_directory(os.environ['SCIFLOW_BUCKET'], remote_key, package_dir)\\n\"\n",
    "    )\n",
    "    sm_module_file.write(f\"{ind}sys.path.append(lib_dir)\\n\")\n",
    "    sm_module_file.write(\"\\n\")\n",
    "\n",
    "    sm_module_file.write(f\"def save_results(save_dir, results):\\n\")\n",
    "    sm_module_file.write(f\"{ind}if results is not None and len(results) > 0:\\n\")\n",
    "    sm_module_file.write(f\"{ind}{ind}for key, value in results.items():\\n\")\n",
    "    sm_module_file.write(\n",
    "        f\"{ind}{ind}{ind}if isinstance(value, pd.Series) or isinstance(value, pd.DataFrame):\\n\"\n",
    "    )\n",
    "    if is_processing_step(step):\n",
    "        sm_module_file.write(\n",
    "            f'{ind}{ind}{ind}{ind}value.to_parquet(f\"{{save_dir}}/{{key}}/{{key}}\")\\n'\n",
    "        )\n",
    "    elif is_train_step(step):\n",
    "        sm_module_file.write(\n",
    "            f'{ind}{ind}{ind}{ind}value.to_parquet(f\"{{save_dir}}/{{key}}\")\\n'\n",
    "        )\n",
    "    sm_module_file.write(f\"{ind}{ind}{ind}else:\\n\")\n",
    "    if is_processing_step(step):\n",
    "        sm_module_file.write(\n",
    "            f'{ind}{ind}{ind}{ind}with open(f\"{{save_dir}}/{{key}}/{{key}}\", \"wb\") as pickle_file:\\n'\n",
    "        )\n",
    "    elif is_train_step(step):\n",
    "        sm_module_file.write(\n",
    "            f'{ind}{ind}{ind}{ind}with open(f\"{{save_dir}}/{{key}}\", \"wb\") as pickle_file:\\n'\n",
    "        )\n",
    "    sm_module_file.write(f\"{ind}{ind}{ind}{ind}{ind}pickle.dump(value, pickle_file)\\n\")\n",
    "    # TODO log artifacts & metrics\n",
    "    sm_module_file.write(\"\\n\")\n",
    "\n",
    "    sm_module_file.write(f\"def load_result(load_dir, result_key):\\n\")\n",
    "    sm_module_file.write(f\"{ind}try:\\n\")\n",
    "    sm_module_file.write(\n",
    "        f'{ind}{ind}result = pd.read_parquet(f\"{{load_dir}}/{{result_key}}\")\\n'\n",
    "    )\n",
    "    sm_module_file.write(f\"{ind}except:\\n\")\n",
    "    sm_module_file.write(\n",
    "        f'{ind}{ind}result = pickle.load( open(f\"{{load_dir}}/{{result_key}}\", \"rb\") )\\n'\n",
    "    )\n",
    "    sm_module_file.write(f\"{ind}return result\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Inference Pipeline Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "\n",
    "* Put in different source file\n",
    "* Separate pipeline\n",
    "* Preprocessing step\n",
    "* Batch transform step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Single Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "config = get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted test_multistep.ipynb to TestMultistepPipeline in: test_multistep.py\n"
     ]
    }
   ],
   "source": [
    "nb_to_sagemaker_pipeline(nb_path, flow_path, silent=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ignore notebooks without Sciflow steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nb_to_sagemaker_pipeline(\n",
    "    Path(nbs_dir, \"packaging.ipynb\"),\n",
    "    get_flow_path(Path(nbs_dir, \"packaging.ipynb\")),\n",
    "    config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple Flow Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## `generate_flows`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def generate_flows(nb_glob: str = None, clear_dir: bool = True):\n",
    "    if clear_dir:\n",
    "        flows_dir = Path(get_config().path(\"flows_path\"), \"sagemaker\")\n",
    "        [f.unlink() for f in flows_dir.iterdir() if not f.is_dir() if f.name != \"requirements.txt\"]\n",
    "       \n",
    "    nb_paths = nbglob(nb_glob)\n",
    "    for nb_path in nb_paths:\n",
    "        nb_to_sagemaker_pipeline(\n",
    "            nb_path, get_flow_path(nb_path, flow_provider=\"sagemaker\"), silent=False\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLI Commands"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `sciflow_sagemaker`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "@call_parse\n",
    "def sciflow_sagemaker(\n",
    "    nb_glob: str = None, clear_dir: bool = True, log_level: str = \"warn\"\n",
    "):\n",
    "    configure_logging(log_level)\n",
    "    generate_flows(nb_glob=nb_glob, clear_dir=clear_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rootLogger = logging.getLogger()\n",
    "rootLogger.handlers = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted test_export_step.ipynb to TestExportStepPipeline in: test_export_step.py\n",
      "Converted test_multistep.ipynb to TestMultistepPipeline in: test_multistep.py\n",
      "Converted test_export.ipynb to TestExportPipeline in: test_export.py\n",
      "Converted test_module.ipynb to TestModulePipeline in: test_module.py\n",
      "2023-11-23 16:25:35,474 [MainThread  ] [INFO ]  No params cell found for: test_multistep_no_params.ipynb\n",
      "Sagemaker conversion failed for test_multistep_no_params.ipynb, Reason: Step: preprocess depends on variable(s) not in flow scope: \"{'min_date', 'model_level', 'traffic_percent'}\"\n",
      "Sagemaker conversion failed for test_data_handling.ipynb, Reason: Unsupported parameter type for sagemaker pipeline: <class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "sciflow_sagemaker(log_level=\"info\")"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.m5.large",
  "kernelspec": {
   "display_name": "Python 3 (SageMaker Distribution v0 CPU)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:eu-west-1:819792524951:image/sagemaker-distribution-cpu-v0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
