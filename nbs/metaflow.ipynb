{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp metaflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "import asyncio\n",
    "import multiprocessing\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "from itertools import product\n",
    "from pathlib import Path, PosixPath\n",
    "from typing import Any, Dict, Iterable\n",
    "\n",
    "import pandas as pd\n",
    "from fastcore.script import Param, call_parse\n",
    "from nbdev.export import find_default_export, get_config, nbglob, read_nb\n",
    "from sciflow.data_handler import extract_param_meta\n",
    "from sciflow.params import params_as_dict\n",
    "from sciflow.parse_module import FuncDetails, extract_steps\n",
    "from sciflow.utils import prepare_env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sciflow Notebook to MetaFlow Flow\n",
    "\n",
    "> Converts from a `sciflow` format notebook to a `metaflow` flow. \n",
    "\n",
    "Supported features:\n",
    "\n",
    "* Linear/sequential DAGs\n",
    "* Simple `Parameters`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_path = Path(os.path.join(\"test\", \"test_export.ipynb\"))\n",
    "nb = read_nb(nb_path)\n",
    "module_name = find_default_export(nb[\"cells\"]).replace(\".\", \"/\")\n",
    "test_module = os.path.join(get_config().path(\"lib_path\"), f\"{module_name}.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "def titleize(name):\n",
    "    return name.title().replace(\"_\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert titleize(\"snake_case\") == \"SnakeCase\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "def rename_steps_for_metaflow(steps):\n",
    "    for i, step in enumerate(steps):\n",
    "        if i == 0:\n",
    "            step.name = \"start\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = extract_steps(test_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_steps = extract_steps(os.path.join(get_config().path(\"lib_path\"), f\"_nbdev.py\"))\n",
    "assert len(no_steps) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert [\"first\", \"preprocess\", \"train\", \"last\"] == [step.name for step in steps]\n",
    "rename_steps_for_metaflow(steps)\n",
    "assert [\"start\", \"preprocess\", \"train\", \"last\"] == [step.name for step in steps]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "def indent_multiline(multiline_text, indent=1):\n",
    "    lines = multiline_text.strip().split(\"\\n\")\n",
    "    spaces = \"\".join([\"    \" for _ in range(indent)])\n",
    "    for i in range(len(lines)):\n",
    "        prefix = spaces if i > 0 else spaces + '\"\"\"'\n",
    "        lines[i] = prefix + lines[i]\n",
    "    return \"\\n\".join(lines) + '\"\"\"'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "Some text\n",
    ":param param: text\n",
    "\"\"\"\n",
    "assert '    \"\"\"Some text\\n    :param param: text\"\"\"' == indent_multiline(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare_env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_path = Path(os.path.join(\"test\", \"test_clustering.ipynb\"))\n",
    "nb = read_nb(nb_path)\n",
    "module_name = find_default_export(nb[\"cells\"]).replace(\".\", \"/\")\n",
    "test_module = os.path.join(get_config().path(\"lib_path\"), f\"{module_name}.py\")\n",
    "steps = extract_steps(test_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "def nb_to_metaflow(nb_path: Path, flow_path: Path, silent=True, track_experiment=True):\n",
    "    nb = read_nb(nb_path)\n",
    "    lib_name = get_config().get(\"lib_name\")\n",
    "    module_name = find_default_export(nb[\"cells\"])\n",
    "    if not module_name:\n",
    "        return\n",
    "    module_name = module_name\n",
    "    path_sep_module_name = module_name.replace(\".\", \"/\")\n",
    "    nb_name = os.path.basename(nb_path)\n",
    "    exported_module = os.path.join(\n",
    "        get_config().path(\"lib_path\"), f\"{path_sep_module_name}.py\"\n",
    "    )\n",
    "    steps = extract_steps(exported_module)\n",
    "    if len(steps) == 0:\n",
    "        return\n",
    "    orig_step_names = [step.name for step in steps]\n",
    "    if len(steps) == 1:\n",
    "        steps.append(FuncDetails(\"end\", None, None, False, \"\", \"pass\"))\n",
    "    params = params_as_dict(nb_path)\n",
    "    if len(params) == 0:\n",
    "        print(f\"No params cell found for: {os.path.basename(nb_path)}\")\n",
    "    flow_class_name = f\"{titleize(extract_module_only(module_name))}Flow\"\n",
    "    rename_steps_for_metaflow(steps)\n",
    "    write_module_to_file(\n",
    "        flow_path,\n",
    "        flow_class_name,\n",
    "        lib_name,\n",
    "        module_name,\n",
    "        orig_step_names,\n",
    "        steps,\n",
    "        params,\n",
    "        track_experiment,\n",
    "    )\n",
    "    if not silent:\n",
    "        print(\n",
    "            f\"Converted {nb_name} to {flow_class_name} in: {os.path.basename(flow_path)}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkg_module_name = \"test.module\"\n",
    "module_name = \"module\"\n",
    "path_sep_module_name = module_name.replace(\".\", \"/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "def extract_module_only(package_module_name):\n",
    "    module_name = package_module_name\n",
    "    if \".\" in module_name:\n",
    "        package_name, module_name = module_name.split(\".\")\n",
    "    return module_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert \"module\" == extract_module_only(module_name)\n",
    "assert \"module\" == extract_module_only(pkg_module_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  export\n",
    "\n",
    "\n",
    "def write_module_to_file(\n",
    "    flow_path: Path,\n",
    "    flow_class_name: str,\n",
    "    lib_name: str,\n",
    "    module_name: str,\n",
    "    orig_step_names: Iterable[str],\n",
    "    steps: Iterable[FuncDetails],\n",
    "    params: dict,\n",
    "    track_experiment: bool,\n",
    "):\n",
    "    if not os.path.exists(flow_path.parent):\n",
    "        os.mkdir(flow_path.parent)\n",
    "    fq_module_name = f\"{lib_name}.{module_name}\"\n",
    "    param_meta = extract_param_meta(fq_module_name, params)\n",
    "    with open(flow_path, \"w\") as flow_file:\n",
    "        flow_file.write(\"#!/usr/bin/env python\\n\")\n",
    "        flow_file.write(\"# coding=utf-8\\n\")\n",
    "        flow_file.write(\"# SCIFLOW GENERATED FILE - EDIT COMPANION NOTEBOOK\\n\")\n",
    "        has_mf_param = any((p.has_metaflow_param for p in param_meta.values()))\n",
    "        has_json_param = any((p.is_json_type for p in param_meta.values()))\n",
    "        mf_params_import = \"from metaflow import FlowSpec, step, current\"\n",
    "        if has_mf_param:\n",
    "            mf_params_import += \", Parameter\"\n",
    "        if has_json_param:\n",
    "            mf_params_import += \", JSONType\"\n",
    "            flow_file.write(\"import json\\n\")\n",
    "        flow_file.write(mf_params_import + \"\\n\")\n",
    "        flow_file.write(f\"from {fq_module_name} import {', '.join(orig_step_names)}\\n\")\n",
    "        if len(params) > 0:\n",
    "            flow_file.write(\n",
    "                f\"from {fq_module_name} import {', '.join(params.keys())}\\n\"\n",
    "            )\n",
    "\n",
    "        if track_experiment:\n",
    "            flow_file.write(\"from sacred import Experiment\\n\")\n",
    "            flow_file.write(\n",
    "                \"from sciflow.experiment.lake_observer import AWSLakeObserver\\n\"\n",
    "            )\n",
    "            flow_file.write(\"import time\")\n",
    "            write_observers(\n",
    "                lib_name,\n",
    "                flow_file,\n",
    "                module_name,\n",
    "                os.environ[\"SCIFLOW_BUCKET\"],\n",
    "                get_config().get(\"lib_name\"),\n",
    "            )\n",
    "\n",
    "        flow_file.write(f\"\\n\\nclass {flow_class_name}(FlowSpec):\\n\")\n",
    "        single_indent = \"    \"\n",
    "        write_params(flow_file, param_meta, single_indent)\n",
    "        if track_experiment:\n",
    "            flow_file.write(f\"{single_indent}artifacts = []\\n\")\n",
    "            flow_file.write(f\"{single_indent}metrics = []\\n\")\n",
    "        flow_file.write(\"\\n\")\n",
    "        write_steps(\n",
    "            flow_file,\n",
    "            steps,\n",
    "            orig_step_names,\n",
    "            param_meta,\n",
    "            single_indent,\n",
    "            track_experiment,\n",
    "        )\n",
    "        write_track_flow(flow_file, track_experiment)\n",
    "        flow_file.write(\"\\n\")\n",
    "\n",
    "        flow_file.write('if __name__ == \"__main__\":\\n')\n",
    "        flow_file.write(f\"{single_indent}{flow_class_name}()\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "def write_observers(lib_name, flow_file, module_name, bucket_name, project):\n",
    "    experiment_name = extract_module_only(module_name)\n",
    "    sacred_setup = f\"\"\"\n",
    "\n",
    "ex = Experiment(\"{experiment_name}\")\n",
    "# TODO inject observers\n",
    "obs = AWSLakeObserver(project=\"{lib_name}\", experiment_name=\"{experiment_name}\")\n",
    "ex.observers.append(obs)\n",
    "\n",
    "@ex.config\n",
    "def config():\n",
    "    flow_run_id = None\n",
    "    artifacts = []\n",
    "    metrics = []\n",
    "    \"\"\"\n",
    "    flow_file.write(sacred_setup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "def write_track_flow(flow_file, track_experiment):\n",
    "    track_flow = \"\"\"\n",
    "    @step\n",
    "    def end(self):\n",
    "        flow_info = {\n",
    "            \"flow_name\": current.flow_name,\n",
    "            \"run_id\": current.run_id,\n",
    "            \"pathspec\": current.pathspec,\n",
    "            \"namespace\": current.namespace,\n",
    "            \"username\": current.username,\n",
    "            \"flow_parameters\": str(current.parameter_names),\n",
    "            \"run_time_mins\": round((time.time() - self.__getattr__('start_time')) / 60.0, 1)\n",
    "        }\n",
    "    \n",
    "        run = ex.run(config_updates={'artifacts': self.__getattr__('artifacts'),\n",
    "                                    'metrics': self.__getattr__('metrics')},\n",
    "                     meta_info = flow_info)\n",
    "        \n",
    "    @ex.main\n",
    "    def track_flow(artifacts, metrics, _run):\n",
    "        for artifact in artifacts:\n",
    "            _run.add_artifact(artifact)\n",
    "        for metric_name, metric_value, step in metrics:\n",
    "            _run.log_scalar(metric_name, metric_value, step)\n",
    "    \"\"\"\n",
    "    if not track_experiment:\n",
    "        track_flow = \"\"\"\n",
    "    @step\n",
    "    def end(self):\n",
    "        pass\n",
    "    \"\"\"\n",
    "    flow_file.write(track_flow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  export\n",
    "\n",
    "\n",
    "def write_params(flow_file, param_meta, single_indent):\n",
    "    for param in param_meta.keys():\n",
    "        if param_meta[param].is_scalar:\n",
    "            flow_file.write(\n",
    "                f\"{single_indent}{param} = Parameter('{param}', default={param})\\n\"\n",
    "            )\n",
    "        elif param_meta[param].is_json_type:\n",
    "            flow_file.write(\n",
    "                f\"{single_indent}{param} = Parameter('{param}', default=json.dumps({param}), type=JSONType)\\n\"\n",
    "            )\n",
    "        elif param_meta[param].instance_type == PosixPath:\n",
    "            flow_file.write(\n",
    "                f\"{single_indent}{param} = Parameter('{param}', default=str({param}))\\n\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_path = os.path.join(Path(\".\").resolve(), \"test\", \"test_data_handling.ipynb\")\n",
    "params = params_as_dict(nb_path)\n",
    "param_meta = extract_param_meta(\"sciflow.test.test_data_handling\", params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert any((p.has_metaflow_param for p in param_meta.values()))\n",
    "assert any((p.is_json_type for p in param_meta.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "def format_arg(arg, param_meta):\n",
    "    if arg in param_meta and not param_meta[arg].has_metaflow_param:\n",
    "        result = arg\n",
    "    else:\n",
    "        result = \"self.\" + arg\n",
    "    return result\n",
    "\n",
    "\n",
    "def write_steps(\n",
    "    flow_file, steps, orig_step_names, param_meta, single_indent, track_experiment\n",
    "):\n",
    "    for i, step in enumerate(steps):\n",
    "        flow_file.write(f\"{single_indent}@step\\n\")\n",
    "        flow_file.write(f\"{single_indent}def {step.name}(self):\\n\")\n",
    "        if step.docstring:\n",
    "            flow_file.write(f\"{indent_multiline(step.docstring, 2)}\\n\")\n",
    "        # Check for padded step\n",
    "        if i < len(orig_step_names):\n",
    "            flow_step_args = \"\"\n",
    "            if len(step.args) > 0:\n",
    "                flow_step_args = \", \".join(\n",
    "                    [format_arg(a, param_meta) for a in step.args.split(\",\")]\n",
    "                )\n",
    "            if not step.has_return:\n",
    "                flow_file.write(\n",
    "                    f\"{single_indent}{single_indent}{orig_step_names[i]}({flow_step_args})\\n\"\n",
    "                )\n",
    "            else:\n",
    "                if step.return_stmt in param_meta:\n",
    "                    raise ValueError(\n",
    "                        f\"[{os.path.basename(flow_file.name)}] step return variable {step.return_stmt} shadows a parameter name - parameters must be unique\"\n",
    "                    )\n",
    "                flow_file.write(\n",
    "                    f\"{single_indent}{single_indent}results = {orig_step_names[i]}({flow_step_args})\\n\"\n",
    "                )\n",
    "                write_track_capture(flow_file)\n",
    "        if i == 0 and track_experiment:\n",
    "            flow_file.write(\n",
    "                f\"{single_indent}{single_indent}self.start_time = time.time()\\n\"\n",
    "            )\n",
    "        if i < len(steps):\n",
    "            next_step = \"end\" if i == len(steps) - 1 else steps[i + 1].name\n",
    "            flow_file.write(\n",
    "                f\"{single_indent}{single_indent}self.next(self.{next_step})\\n\"\n",
    "            )\n",
    "        flow_file.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "def write_track_capture(flow_file):\n",
    "    flow_file.write(\n",
    "        f\"\"\"\n",
    "        for key in results.keys():\n",
    "            if key in self.__dict__:\n",
    "                self.__dict__[key] = self.__dict__[key] + results[key]\n",
    "            else:\n",
    "                self.__dict__[key] = results[key]\n",
    "\n",
    "\"\"\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "def get_module_name(nb_path):\n",
    "    nb = read_nb(nb_path)\n",
    "    module_name = find_default_export(nb[\"cells\"])\n",
    "    return module_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Flow Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_flow_path(nb_path):\n",
    "    return Path(\n",
    "        os.path.join(\n",
    "            get_config().path(\"flows_path\"),\n",
    "            f\"{get_module_name(nb_path).split('.')[-1]}.py\",\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert Path(\n",
    "    \"/home/jovyan/git/sciflow/nbs/test/flows/test_data_handling.py\"\n",
    ") == get_flow_path(nb_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "flow_path = Path(\n",
    "    os.path.join(\n",
    "        Path(\".\").resolve(),\n",
    "        \"test\",\n",
    "        \"flows\",\n",
    "        f\"{get_module_name(nb_path).split('.')[-1]}.py\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (\n",
    "    Path(os.path.join(Path(\".\").resolve(), \"test\", \"flows\", \"test_data_handling.py\",))\n",
    "    == flow_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted test_data_handling.ipynb to TestDataHandlingFlow in: test_data_handling.py\n"
     ]
    }
   ],
   "source": [
    "nb_to_metaflow(nb_path, flow_path, silent=False, track_experiment=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted test_clustering.ipynb to TestClusteringFlow in: test_clustering.py\n"
     ]
    }
   ],
   "source": [
    "nb_to_metaflow(\n",
    "    Path(\"/home/jovyan/git/sciflow/nbs/test/test_clustering.ipynb\"),\n",
    "    Path(\"/home/jovyan/git/sciflow/nbs/test/flows/test_clustering.py\"),\n",
    "    silent=False,\n",
    "    track_experiment=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ignore notebooks without Sciflow steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_to_metaflow(\"packaging.ipynb\", flow_path, silent=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "def generate_flows(config, track_experiment=True):\n",
    "    flows_dir = get_config().path(\"flows_path\")\n",
    "    nb_paths = nbglob(recursive=True)\n",
    "    for nb_path in nb_paths:\n",
    "        flow_module_name = os.path.basename(nb_path).replace(\"ipynb\", \"py\")\n",
    "        nb_to_metaflow(\n",
    "            nb_path,\n",
    "            Path(os.path.join(flows_dir, flow_module_name)),\n",
    "            track_experiment=track_experiment,\n",
    "            silent=False,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No params cell found for: test_clustering_no_params.ipynb\n",
      "Converted test_clustering_no_params.ipynb to TestClusteringNoParamsFlow in: test_clustering_no_params.py\n",
      "Converted test_clustering.ipynb to TestClusteringFlow in: test_clustering.py\n",
      "Converted test_export.ipynb to TestExportFlow in: test_export.py\n",
      "Converted test_module.ipynb to TestModuleFlow in: test_module.py\n",
      "Converted test_data_handling.ipynb to TestDataHandlingFlow in: test_data_handling.py\n"
     ]
    }
   ],
   "source": [
    "generate_flows(get_config(cfg_name=\"test/settings.ini\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No params cell found for: test_clustering_no_params.ipynb\n",
      "Converted test_clustering_no_params.ipynb to TestClusteringNoParamsFlow in: test_clustering_no_params.py\n",
      "Converted test_clustering.ipynb to TestClusteringFlow in: test_clustering.py\n",
      "Converted test_export.ipynb to TestExportFlow in: test_export.py\n",
      "Converted test_module.ipynb to TestModuleFlow in: test_module.py\n",
      "Converted test_data_handling.ipynb to TestDataHandlingFlow in: test_data_handling.py\n"
     ]
    }
   ],
   "source": [
    "generate_flows(get_config(cfg_name=\"test/settings.ini\"), track_experiment=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "@call_parse\n",
    "def sciflow_generate(track: Param(\"Track flows as sacred experiments\", bool) = True):\n",
    "    generate_flows(get_config(), track)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verify Flow Generation\n",
    "\n",
    "> Check that flows are valid and that they run. Running is a longer operation so be careful not to overload your build with slow cycles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "def check_flows(\n",
    "    config, flow_command=\"show\", ignore_suffix=\"_no_params.py\", exit_on_error=True\n",
    "):\n",
    "    flow_results = {}\n",
    "    flows_dir = config.path(\"flows_path\")\n",
    "    if ignore_suffix:\n",
    "        flow_paths = [p for p in os.listdir(flows_dir) if not p.endswith(ignore_suffix)]\n",
    "    else:\n",
    "        flow_paths = os.listdir(flows_dir)\n",
    "    ret_codes = []\n",
    "    exit_code = 0\n",
    "    for flow_path in flow_paths:\n",
    "        flow_name = os.path.basename(flow_path)\n",
    "        if flow_path.endswith(\".py\"):\n",
    "            ret_code, output = check_flow(\n",
    "                flows_dir, flow_path, flow_command=flow_command\n",
    "            )\n",
    "            flow_results[flow_name] = ret_code, output\n",
    "            if ret_code == 0:\n",
    "                print(f\"Flow: {flow_name} {flow_command} verified\")\n",
    "            else:\n",
    "                print(\n",
    "                    f\"Flow: {flow_name} {flow_command} verification failed\\nDetails:\\n{output}\"\n",
    "                )\n",
    "            ret_codes.append(ret_code)\n",
    "    if any([rc != 0 for rc in ret_codes]):\n",
    "        exit_code = 1\n",
    "        try:\n",
    "            # Exit with an error code if running from a non interactive Python environment.\n",
    "            get_ipython().__class__.__name__\n",
    "        except NameError:\n",
    "            if exit_on_error:\n",
    "                return sys.exit(exit_code)\n",
    "    return exit_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "def prep_mf_env():\n",
    "    if \"USER\" not in os.environ:\n",
    "        try:\n",
    "            os.environ[\"USER\"] = os.environ[\"GIT_COMMITTER_NAME\"]\n",
    "        except KeyError:\n",
    "            raise EnvironmentError(\n",
    "                \"Metaflow requires a known user for tracked execution. Add USER or GIT_COMMITTER_NAME to Jupyter environment variables\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "def run_shell_cmd(script):\n",
    "    pipe = subprocess.Popen(\n",
    "        \"%s\" % script, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, shell=True\n",
    "    )\n",
    "    output = pipe.communicate()[0]\n",
    "    return pipe, output.decode(\"utf-8\").strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "def check_flow(flows_dir, flow_module, flow_command=\"show\", params=None):\n",
    "    prep_mf_env()\n",
    "    if params:\n",
    "        args = \" \".join([f\"--{p[0]} {p[1]}\" for p in params])\n",
    "        script = (\n",
    "            f\"python '{os.path.join(flows_dir, flow_module)}' {flow_command} {args}\"\n",
    "        )\n",
    "    else:\n",
    "        script = f\"python '{os.path.join(flows_dir, flow_module)}' {flow_command}\"\n",
    "    pipe, output = run_shell_cmd(script)\n",
    "    return pipe.returncode, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = [(\"speed\", \"fast\"), (\"traffic_percent\", 10)]\n",
    "assert \"--speed fast --traffic_percent 10\" == \" \".join(\n",
    "    [f\"--{p[0]} {p[1]}\" for p in params]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flow: test_module.py show verified\n",
      "Flow: test_clustering_no_params.py show verified\n",
      "Flow: test_clustering.py show verified\n",
      "Flow: test_data_handling.py show verified\n",
      "Flow: test_export.py show verified\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_flows(get_config(cfg_name=\"test/settings.ini\"), ignore_suffix=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flow: test_module.py --no-pylint run verified\n",
      "Flow: test_clustering.py --no-pylint run verification failed\n",
      "Details:\n",
      "Metaflow 2.4.1 executing TestClusteringFlow for user:Donal Simmie\n",
      "Validating your flow...\n",
      "    The graph looks good!\n",
      "2022-01-14 20:05:40.564 Workflow starting (run-id 1642190740561556):\n",
      "2022-01-14 20:05:40.569 [1642190740561556/start/1 (pid 11003)] Task is starting.\n",
      "2022-01-14 20:05:41.320 [1642190740561556/start/1 (pid 11003)] The first step\n",
      "2022-01-14 20:05:41.444 [1642190740561556/start/1 (pid 11003)] Task finished successfully.\n",
      "2022-01-14 20:05:41.449 [1642190740561556/preprocess/2 (pid 11023)] Task is starting.\n",
      "2022-01-14 20:05:42.196 [1642190740561556/preprocess/2 (pid 11023)] <flow TestClusteringFlow step preprocess> failed:\n",
      "2022-01-14 20:05:42.198 [1642190740561556/preprocess/2 (pid 11023)] Internal error\n",
      "2022-01-14 20:05:42.199 [1642190740561556/preprocess/2 (pid 11023)] Traceback (most recent call last):\n",
      "2022-01-14 20:05:42.199 [1642190740561556/preprocess/2 (pid 11023)] File \"/opt/conda/envs/sciflow/lib/python3.7/site-packages/metaflow/cli.py\", line 973, in main\n",
      "2022-01-14 20:05:42.199 [1642190740561556/preprocess/2 (pid 11023)] start(auto_envvar_prefix='METAFLOW', obj=state)\n",
      "2022-01-14 20:05:42.199 [1642190740561556/preprocess/2 (pid 11023)] File \"/opt/conda/envs/sciflow/lib/python3.7/site-packages/click/core.py\", line 1137, in __call__\n",
      "2022-01-14 20:05:42.199 [1642190740561556/preprocess/2 (pid 11023)] return self.main(args, kwargs)\n",
      "2022-01-14 20:05:42.314 [1642190740561556/preprocess/2 (pid 11023)] File \"/opt/conda/envs/sciflow/lib/python3.7/site-packages/click/core.py\", line 1062, in main\n",
      "2022-01-14 20:05:42.314 [1642190740561556/preprocess/2 (pid 11023)] rv = self.invoke(ctx)\n",
      "2022-01-14 20:05:42.315 [1642190740561556/preprocess/2 (pid 11023)] File \"/opt/conda/envs/sciflow/lib/python3.7/site-packages/click/core.py\", line 1668, in invoke\n",
      "2022-01-14 20:05:42.315 [1642190740561556/preprocess/2 (pid 11023)] return _process_result(sub_ctx.command.invoke(sub_ctx))\n",
      "2022-01-14 20:05:42.315 [1642190740561556/preprocess/2 (pid 11023)] File \"/opt/conda/envs/sciflow/lib/python3.7/site-packages/click/core.py\", line 1404, in invoke\n",
      "2022-01-14 20:05:42.315 [1642190740561556/preprocess/2 (pid 11023)] return ctx.invoke(self.callback, ctx.params)\n",
      "2022-01-14 20:05:42.315 [1642190740561556/preprocess/2 (pid 11023)] File \"/opt/conda/envs/sciflow/lib/python3.7/site-packages/click/core.py\", line 763, in invoke\n",
      "2022-01-14 20:05:42.315 [1642190740561556/preprocess/2 (pid 11023)] return __callback(args, kwargs)\n",
      "2022-01-14 20:05:42.315 [1642190740561556/preprocess/2 (pid 11023)] File \"/opt/conda/envs/sciflow/lib/python3.7/site-packages/click/decorators.py\", line 26, in new_func\n",
      "2022-01-14 20:05:42.315 [1642190740561556/preprocess/2 (pid 11023)] return f(get_current_context(), args, kwargs)\n",
      "2022-01-14 20:05:42.315 [1642190740561556/preprocess/2 (pid 11023)] File \"/opt/conda/envs/sciflow/lib/python3.7/site-packages/metaflow/cli.py\", line 484, in step\n",
      "2022-01-14 20:05:42.315 [1642190740561556/preprocess/2 (pid 11023)] max_user_code_retries)\n",
      "2022-01-14 20:05:42.315 [1642190740561556/preprocess/2 (pid 11023)] File \"/opt/conda/envs/sciflow/lib/python3.7/site-packages/metaflow/task.py\", line 427, in run_step\n",
      "2022-01-14 20:05:42.315 [1642190740561556/preprocess/2 (pid 11023)] self._exec_step_function(step_func)\n",
      "2022-01-14 20:05:42.315 [1642190740561556/preprocess/2 (pid 11023)] File \"/opt/conda/envs/sciflow/lib/python3.7/site-packages/metaflow/task.py\", line 51, in _exec_step_function\n",
      "2022-01-14 20:05:42.315 [1642190740561556/preprocess/2 (pid 11023)] step_function()\n",
      "2022-01-14 20:05:42.315 [1642190740561556/preprocess/2 (pid 11023)] File \"/home/jovyan/git/sciflow/nbs/test/flows/test_clustering.py\", line 22, in preprocess\n",
      "2022-01-14 20:05:42.315 [1642190740561556/preprocess/2 (pid 11023)] results = preprocess(self.odbc_conn, self.model_level, self.min_date, self.traffic_percent)\n",
      "2022-01-14 20:05:42.316 [1642190740561556/preprocess/2 (pid 11023)] File \"/opt/conda/envs/sciflow/lib/python3.7/site-packages/metaflow/flowspec.py\", line 136, in __getattr__\n",
      "2022-01-14 20:05:42.316 [1642190740561556/preprocess/2 (pid 11023)] (self.name, name))\n",
      "2022-01-14 20:05:42.316 [1642190740561556/preprocess/2 (pid 11023)] AttributeError: Flow TestClusteringFlow has no attribute 'odbc_conn'\n",
      "2022-01-14 20:05:42.316 [1642190740561556/preprocess/2 (pid 11023)] \n",
      "2022-01-14 20:05:42.316 [1642190740561556/preprocess/2 (pid 11023)] Task failed.\n",
      "2022-01-14 20:05:42.316 Workflow failed.\n",
      "2022-01-14 20:05:42.316 Terminating 0 active tasks...\n",
      "2022-01-14 20:05:42.316 Flushing logs...\n",
      "    Step failure:\n",
      "    Step preprocess (task-id 2) failed.\n",
      "Flow: test_data_handling.py --no-pylint run verified\n"
     ]
    }
   ],
   "source": [
    "# slow\n",
    "\n",
    "check_flows(get_config(cfg_name=\"test/settings.ini\"), \"--no-pylint run\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "def run_flow(nb_path, params=None):\n",
    "    flow_path = get_flow_path(nb_path)\n",
    "    print(f\"Running flow: {os.path.basename(flow_path)}\")\n",
    "    ret_code, output = check_flow(\n",
    "        flow_path.parent,\n",
    "        os.path.basename(flow_path),\n",
    "        flow_command=\"--no-pylint run\",\n",
    "        params=params,\n",
    "    )\n",
    "    return ret_code, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slow\n",
    "\n",
    "ret_code, output = run_flow(\n",
    "    Path(\"/home/jovyan/git/sciflow/nbs/test/test_clustering.ipynb\"),\n",
    "    params=[(\"traffic_percent\", 10), (\"speed\", \"fast\")],\n",
    ")\n",
    "print(output)\n",
    "assert ret_code == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aynsc Flow Running\n",
    "\n",
    "> Run the flow you are working on from the notebook you are working on. This maximises the amount fo experiments you can run as you don't have down time. While long running tasks are running you can keep exploring! :-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "async def run_flow_task(flow_path, param_grid=None):\n",
    "    flows_dir = flow_path.parent\n",
    "    flow_module = os.path.basename(flow_path)\n",
    "    flow_command = \"--no-pylint run\"\n",
    "    prep_mf_env()\n",
    "    if params:\n",
    "        args = \" \".join([f\"--{k} {v}\" for k, v in param_grid.items()])\n",
    "        cmd = f\"python '{os.path.join(flows_dir, flow_module)}' {flow_command} {args}\"\n",
    "    else:\n",
    "        cmd = f\"python '{os.path.join(flows_dir, flow_module)}' {flow_command}\"\n",
    "    proc = await asyncio.create_subprocess_shell(\n",
    "        cmd, stdout=asyncio.subprocess.PIPE, stderr=asyncio.subprocess.PIPE\n",
    "    )\n",
    "\n",
    "    stdout, stderr = await proc.communicate()\n",
    "\n",
    "    print(f\"[{cmd!r} exited with {proc.returncode}]\")\n",
    "    if stdout:\n",
    "        output = stdout.decode(\"utf-8\").strip()\n",
    "        print(f\"[stdout]\\n{output}\")\n",
    "    if stderr:\n",
    "        print(f'[stderr]\\n{stderr.decode(\"utf-8\").strip()}')\n",
    "\n",
    "    return proc.returncode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "def run_flow_async(nb_path, params=None):\n",
    "    flow_path = get_flow_path(nb_path)\n",
    "    loop = asyncio.get_event_loop()\n",
    "    task = loop.create_task(run_flow_task(flow_path, params))\n",
    "    return task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slow\n",
    "\n",
    "task = run_flow_async(\n",
    "    os.path.join(\"test\", \"test_clustering.ipynb\"),\n",
    "    params={\"traffic_percent\": 10, \"speed\": \"fast\"},\n",
    ")\n",
    "task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slow\n",
    "await task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    \"traffic_percent\": [1, 5, 10, 20, 50, 100],\n",
    "    \"speed\": [\"fast\", \"slow\"],\n",
    "    \"workers\": [1],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "def iter_param_grid(param_grid):\n",
    "    # https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/model_selection/_search.py\n",
    "    for p in [param_grid]:\n",
    "        # Always sort the keys of a dictionary, for reproducibility\n",
    "        items = sorted(p.items())\n",
    "        if not items:\n",
    "            yield {}\n",
    "        else:\n",
    "            keys, values = zip(*items)\n",
    "            for v in product(*values):\n",
    "                params = dict(zip(keys, v))\n",
    "                yield params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert [{\"a\": 1, \"b\": 1, \"c\": \"hello\"}, {\"a\": 2, \"b\": 1, \"c\": \"hello\"}] == list(\n",
    "    iter_param_grid({\"a\": [1, 2], \"b\": [1], \"c\": [\"hello\"]})\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "def sample_grid_space(param_grid: Dict[str, Iterable[Any]], num_samples: int):\n",
    "    samples = []\n",
    "    for i, sample in enumerate(iter_param_grid(param_grid)):\n",
    "        samples.append(sample)\n",
    "    if num_samples < len(samples):\n",
    "        samples = pd.Series(samples).sample(num_samples).tolist()\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_space = sample_grid_space({\"a\": [1, 2], \"b\": [1], \"c\": [\"hello\"]}, 1)\n",
    "assert sample_space[0][\"b\"] == 1\n",
    "assert sample_space[0][\"c\"] == \"hello\"\n",
    "assert sample_space[0][\"a\"] == 1 or sample_space[0][\"a\"] == 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "def search_flow_grid(nb_path, param_grid, num_procs=None):\n",
    "    max_process_count = int((multiprocessing.cpu_count() / 2) - 1)\n",
    "    param_sample_space = sample_grid_space(param_grid, max_process_count)\n",
    "    tasks = []\n",
    "    for param_sample in param_sample_space:\n",
    "        tasks.append(run_flow_async(nb_path, params=param_sample))\n",
    "    return tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slow\n",
    "\n",
    "nb_path = \"/home/jovyan/git/sciflow/nbs/test/test_clustering.ipynb\"\n",
    "tasks = search_flow_grid(\n",
    "    nb_path,\n",
    "    {\n",
    "        \"traffic_percent\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 20, 30, 40, 50],\n",
    "        \"speed\": [\"fast\"],\n",
    "        \"workers\": [1],\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slow\n",
    "\n",
    "[t.done() for t in tasks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slow\n",
    "\n",
    "[t.result() for t in tasks]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Folding @ Home Type Exploration\n",
    "\n",
    "> Explore wider search space in background. Try to always be making some use of resource. Needs persistent search space tracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "@call_parse\n",
    "def sciflow_check_flows():\n",
    "    check_flows(get_config())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "@call_parse\n",
    "def sciflow_run_flows():\n",
    "    check_flows(get_config(), \"--no-pylint run\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:sciflow]",
   "language": "python",
   "name": "conda-env-sciflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
