{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp metaflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "import os\n",
    "import subprocess\n",
    "from pathlib import Path, PosixPath\n",
    "from typing import Iterable\n",
    "import asyncio\n",
    "import multiprocessing\n",
    "from typing import Dict, Iterable, Any\n",
    "\n",
    "from fastcore.script import call_parse\n",
    "from nbdev.export import Config, find_default_export, nbglob, read_nb\n",
    "from sciflow.data_handler import extract_param_meta\n",
    "from sciflow.params import params_as_dict\n",
    "from sciflow.parse_module import FuncDetails, extract_steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sciflow Notebook to MetaFlow Flow\n",
    "\n",
    "> Converts from a `sciflow` format notebook to a `metaflow` flow. \n",
    "\n",
    "Supported features:\n",
    "\n",
    "* Linear/sequential DAGs\n",
    "* Simple `Parameters`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_path = Path(os.path.join(\"test\", \"test_export.ipynb\"))\n",
    "nb = read_nb(nb_path)\n",
    "module_name = find_default_export(nb[\"cells\"]).replace(\".\", \"/\")\n",
    "test_module = os.path.join(Config().path(\"lib_path\"), f\"{module_name}.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "def titleize(name):\n",
    "    return name.title().replace(\"_\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert titleize(\"snake_case\") == \"SnakeCase\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "def rename_steps_for_metaflow(steps):\n",
    "    for i, step in enumerate(steps):\n",
    "        if i == 0:\n",
    "            step.name = \"start\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = extract_steps(test_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_steps = extract_steps(os.path.join(Config().path(\"lib_path\"), f\"_nbdev.py\"))\n",
    "assert len(no_steps) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert [\"first\", \"preprocess\", \"train\", \"last\"] == [step.name for step in steps]\n",
    "rename_steps_for_metaflow(steps)\n",
    "assert [\"start\", \"preprocess\", \"train\", \"last\"] == [step.name for step in steps]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "def indent_multiline(multiline_text, indent=1):\n",
    "    lines = multiline_text.strip().split(\"\\n\")\n",
    "    spaces = \"\".join([\"    \" for _ in range(indent)])\n",
    "    for i in range(len(lines)):\n",
    "        prefix = spaces if i > 0 else spaces + '\"\"\"'\n",
    "        lines[i] = prefix + lines[i]\n",
    "    return \"\\n\".join(lines) + '\"\"\"'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "Some text\n",
    ":param param: text\n",
    "\"\"\"\n",
    "assert '    \"\"\"Some text\\n    :param param: text\"\"\"' == indent_multiline(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_path = Path(os.path.join(\"test\", \"test_clustering.ipynb\"))\n",
    "nb = read_nb(nb_path)\n",
    "module_name = find_default_export(nb[\"cells\"]).replace(\".\", \"/\")\n",
    "test_module = os.path.join(Config().path(\"lib_path\"), f\"{module_name}.py\")\n",
    "steps = extract_steps(test_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "def nb_to_metaflow(nb_path: Path, flow_path: Path, silent=True, track_experiment=True):\n",
    "    nb = read_nb(nb_path)\n",
    "    lib_name = Config().lib_name\n",
    "    module_name = find_default_export(nb[\"cells\"])\n",
    "    if not module_name:\n",
    "        return\n",
    "    module_name = module_name\n",
    "    path_sep_module_name = module_name.replace(\".\", \"/\")\n",
    "    nb_name = os.path.basename(nb_path)\n",
    "    exported_module = os.path.join(\n",
    "        Config().path(\"lib_path\"), f\"{path_sep_module_name}.py\"\n",
    "    )\n",
    "    steps = extract_steps(exported_module)\n",
    "    if len(steps) == 0:\n",
    "        return\n",
    "    orig_step_names = [step.name for step in steps]\n",
    "    if len(steps) == 1:\n",
    "        steps.append(FuncDetails(\"end\", None, None, False, \"\", \"pass\"))\n",
    "    params = params_as_dict(nb_path)\n",
    "    if len(params) == 0:\n",
    "        print(f\"No params cell found for: {os.path.basename(nb_path)}\")\n",
    "    flow_class_name = f\"{titleize(extract_module_only(module_name))}Flow\"\n",
    "    rename_steps_for_metaflow(steps)\n",
    "    write_module_to_file(\n",
    "        flow_path,\n",
    "        flow_class_name,\n",
    "        lib_name,\n",
    "        module_name,\n",
    "        orig_step_names,\n",
    "        steps,\n",
    "        params,\n",
    "        track_experiment,\n",
    "    )\n",
    "    if not silent:\n",
    "        print(\n",
    "            f\"Converted {nb_name} to {flow_class_name} in: {os.path.basename(flow_path)}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkg_module_name = \"test.module\"\n",
    "module_name = \"module\"\n",
    "path_sep_module_name = module_name.replace(\".\", \"/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "def extract_module_only(package_module_name):\n",
    "    module_name = package_module_name\n",
    "    if \".\" in module_name:\n",
    "        package_name, module_name = module_name.split(\".\")\n",
    "    return module_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert \"module\" == extract_module_only(module_name)\n",
    "assert \"module\" == extract_module_only(pkg_module_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  export\n",
    "\n",
    "\n",
    "def write_module_to_file(\n",
    "    flow_path: Path,\n",
    "    flow_class_name: str,\n",
    "    lib_name: str,\n",
    "    module_name: str,\n",
    "    orig_step_names: Iterable[str],\n",
    "    steps: Iterable[FuncDetails],\n",
    "    params: dict,\n",
    "    track_experiment: bool,\n",
    "):\n",
    "    if not os.path.exists(flow_path.parent):\n",
    "        os.mkdir(flow_path.parent)\n",
    "    fq_module_name = f\"{lib_name}.{module_name}\"\n",
    "    param_meta = extract_param_meta(fq_module_name, params)\n",
    "    with open(flow_path, \"w\") as flow_file:\n",
    "        flow_file.write(\"#!/usr/bin/env python\\n\")\n",
    "        flow_file.write(\"# coding=utf-8\\n\")\n",
    "        flow_file.write(\"# SCIFLOW GENERATED FILE - EDIT COMPANION NOTEBOOK\\n\")\n",
    "        has_mf_param = any((p.has_metaflow_param for p in param_meta.values()))\n",
    "        has_json_param = any((p.is_json_type for p in param_meta.values()))\n",
    "        mf_params_import = \"from metaflow import FlowSpec, step, current\"\n",
    "        if has_mf_param:\n",
    "            mf_params_import += \", Parameter\"\n",
    "        if has_json_param:\n",
    "            mf_params_import += \", JSONType\"\n",
    "            flow_file.write(\"import json\\n\")\n",
    "        flow_file.write(mf_params_import + \"\\n\")\n",
    "        flow_file.write(f\"from {fq_module_name} import {', '.join(orig_step_names)}\\n\")\n",
    "        if len(params) > 0:\n",
    "            flow_file.write(\n",
    "                f\"from {fq_module_name} import {', '.join(params.keys())}\\n\"\n",
    "            )\n",
    "\n",
    "        if track_experiment:\n",
    "            flow_file.write(\"from sacred import Experiment\\n\")\n",
    "            flow_file.write(\"from sciflow.lake_observer import AWSLakeObserver\\n\")\n",
    "            flow_file.write(\"import time\")\n",
    "            write_observers(flow_file, module_name, Config().bucket, Config().lib_name)\n",
    "\n",
    "        flow_file.write(f\"\\n\\nclass {flow_class_name}(FlowSpec):\\n\")\n",
    "        single_indent = \"    \"\n",
    "        write_params(flow_file, param_meta, single_indent)\n",
    "        flow_file.write(f\"{single_indent}artifacts = []\\n\")\n",
    "        flow_file.write(f\"{single_indent}metrics = []\\n\")\n",
    "        flow_file.write(\"\\n\")\n",
    "        write_steps(flow_file, steps, orig_step_names, param_meta, single_indent)\n",
    "        write_track_flow(flow_file, track_experiment)\n",
    "        flow_file.write(\"\\n\")\n",
    "\n",
    "        flow_file.write('if __name__ == \"__main__\":\\n')\n",
    "        flow_file.write(f\"{single_indent}{flow_class_name}()\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "def write_observers(flow_file, module_name, bucket_name, project):\n",
    "    experiment_name = extract_module_only(module_name)\n",
    "    sacred_setup = f\"\"\"\n",
    "\n",
    "ex = Experiment(\"{experiment_name}\")\n",
    "# TODO inject observers\n",
    "obs = AWSLakeObserver(\n",
    "    bucket_name=\"{bucket_name}\",\n",
    "    experiment_dir=\"experiments/{project}/{experiment_name}\",\n",
    "    region=\"eu-west-1\",\n",
    ")\n",
    "ex.observers.append(obs)\n",
    "\n",
    "@ex.config\n",
    "def config():\n",
    "    flow_run_id = None\n",
    "    artifacts = []\n",
    "    metrics = []\n",
    "    \"\"\"\n",
    "    flow_file.write(sacred_setup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "def write_track_flow(flow_file, track_experiment):\n",
    "    track_flow = \"\"\"\n",
    "    @step\n",
    "    def end(self):\n",
    "        flow_info = {\n",
    "            \"flow_name\": current.flow_name,\n",
    "            \"run id\": current.run_id,\n",
    "            \"origin run id\": current.origin_run_id,\n",
    "            \"pathspec\": current.pathspec,\n",
    "            \"namespace\": current.namespace,\n",
    "            \"username\": current.username,\n",
    "            \"flow parameters\": str(current.parameter_names),\n",
    "            \"run_time_mins\": round((time.time() - self.__getattr__('start_time')) / 60.0, 1)\n",
    "        }\n",
    "    \n",
    "        run = ex.run(config_updates={'artifacts': self.__getattr__('artifacts'),\n",
    "                                    'metrics': self.__getattr__('metrics')},\n",
    "                     meta_info = flow_info)\n",
    "        \n",
    "    @ex.main\n",
    "    def track_flow(artifacts, metrics, _run):\n",
    "        for artifact in artifacts:\n",
    "            _run.add_artifact(artifact)\n",
    "        for metric_name, metric_value, step in metrics:\n",
    "            _run.log_scalar(metric_name, metric_value, step)\n",
    "    \"\"\"\n",
    "    if not track_experiment:\n",
    "        track_flow = \"\"\"\n",
    "    @step\n",
    "    def end(self):\n",
    "        pass\n",
    "    \"\"\"\n",
    "    flow_file.write(track_flow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  export\n",
    "\n",
    "\n",
    "def write_params(flow_file, param_meta, single_indent):\n",
    "    for param in param_meta.keys():\n",
    "        if param_meta[param].is_scalar:\n",
    "            flow_file.write(\n",
    "                f\"{single_indent}{param} = Parameter('{param}', default={param})\\n\"\n",
    "            )\n",
    "        elif param_meta[param].is_json_type:\n",
    "            flow_file.write(\n",
    "                f\"{single_indent}{param} = Parameter('{param}', default=json.dumps({param}), type=JSONType)\\n\"\n",
    "            )\n",
    "        elif param_meta[param].instance_type == PosixPath:\n",
    "            flow_file.write(\n",
    "                f\"{single_indent}{param} = Parameter('{param}', default=str({param}))\\n\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_path = os.path.join(Path(\".\").resolve(), \"test\", \"test_data_handling.ipynb\")\n",
    "params = params_as_dict(nb_path)\n",
    "param_meta = extract_param_meta(\"sciflow.test.test_data_handling\", params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert any((p.has_metaflow_param for p in param_meta.values()))\n",
    "assert any((p.is_json_type for p in param_meta.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "def format_arg(arg, param_meta):\n",
    "    if arg in param_meta and not param_meta[arg].has_metaflow_param:\n",
    "        result = arg\n",
    "    else:\n",
    "        result = \"self.\" + arg\n",
    "    return result\n",
    "\n",
    "\n",
    "def write_steps(flow_file, steps, orig_step_names, param_meta, single_indent):\n",
    "    for i, step in enumerate(steps):\n",
    "        flow_file.write(f\"{single_indent}@step\\n\")\n",
    "        flow_file.write(f\"{single_indent}def {step.name}(self):\\n\")\n",
    "        if step.docstring:\n",
    "            flow_file.write(f\"{indent_multiline(step.docstring, 2)}\\n\")\n",
    "        # Check for padded step\n",
    "        if i < len(orig_step_names):\n",
    "            flow_step_args = \"\"\n",
    "            if len(step.args) > 0:\n",
    "                flow_step_args = \", \".join(\n",
    "                    [\n",
    "                        format_arg(a, param_meta)\n",
    "                        for a in step.args.split(\",\")\n",
    "                    ]\n",
    "                )\n",
    "            if not step.has_return:\n",
    "                flow_file.write(\n",
    "                    f\"{single_indent}{single_indent}{orig_step_names[i]}({flow_step_args})\\n\"\n",
    "                )\n",
    "            else:\n",
    "                if step.return_stmt in param_meta:\n",
    "                    raise ValueError(\n",
    "                        f\"[{os.path.basename(flow_file.name)}] step return variable {step.return_stmt} shadows a parameter name - parameters must be unique\"\n",
    "                    )\n",
    "                flow_file.write(\n",
    "                    f\"{single_indent}{single_indent}results = {orig_step_names[i]}({flow_step_args})\\n\"\n",
    "                )\n",
    "                write_track_capture(flow_file)\n",
    "        if i == 0:\n",
    "            flow_file.write(\n",
    "                f\"{single_indent}{single_indent}self.start_time = time.time()\\n\"\n",
    "            )\n",
    "        if i < len(steps):\n",
    "            next_step = \"end\" if i == len(steps) - 1 else steps[i + 1].name\n",
    "            flow_file.write(\n",
    "                f\"{single_indent}{single_indent}self.next(self.{next_step})\\n\"\n",
    "            )\n",
    "        flow_file.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "def write_track_capture(flow_file):\n",
    "    flow_file.write(\n",
    "        f\"\"\"\n",
    "        for key in results.keys():\n",
    "            if key in self.__dict__:\n",
    "                self.__dict__[key] = self.__dict__[key] + results[key]\n",
    "            else:\n",
    "                self.__dict__[key] = results[key]\n",
    "\n",
    "\"\"\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "def get_module_name(nb_path):\n",
    "    nb = read_nb(nb_path)\n",
    "    module_name = find_default_export(nb[\"cells\"])\n",
    "    return module_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Flow Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_flow_path(nb_path):\n",
    "    return Path(\n",
    "        os.path.join(Config().path(\"flows_path\"), \n",
    "        f\"{get_module_name(nb_path).split('.')[-1]}.py\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(Path('/home/jovyan/git/sciflow/nbs/test/flows/test_data_handling.py') == get_flow_path(nb_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "flow_path = Path(\n",
    "    os.path.join(\n",
    "        Path(\".\").resolve(),\n",
    "        \"test\",\n",
    "        \"flows\",\n",
    "        f\"{get_module_name(nb_path).split('.')[-1]}.py\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (\n",
    "    Path(os.path.join(Path(\".\").resolve(), \"test\", \"flows\", \"test_data_handling.py\",))\n",
    "    == flow_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted test_data_handling.ipynb to TestDataHandlingFlow in: test_data_handling.py\n"
     ]
    }
   ],
   "source": [
    "nb_to_metaflow(nb_path, flow_path, silent=False, track_experiment=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted test_clustering.ipynb to TestClusteringFlow in: test_clustering.py\n"
     ]
    }
   ],
   "source": [
    "nb_to_metaflow(\n",
    "    Path(\"/home/jovyan/git/sciflow/nbs/test/test_clustering.ipynb\"),\n",
    "    Path(\"/home/jovyan/git/sciflow/nbs/test/flows/test_clustering.py\"),\n",
    "    silent=False,\n",
    "    track_experiment=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ignore notebooks without Sciflow steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_to_metaflow(\"packaging.ipynb\", flow_path, silent=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "def generate_flows(config: Config):\n",
    "    flows_dir = config.path(\"flows_path\")\n",
    "    nb_paths = nbglob(recursive=True)\n",
    "    for nb_path in nb_paths:\n",
    "        flow_module_name = os.path.basename(nb_path).replace(\"ipynb\", \"py\")\n",
    "        nb_to_metaflow(\n",
    "            nb_path, Path(os.path.join(flows_dir, flow_module_name)), silent=False\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO revisit END PADDING logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No params cell found for: test_clustering_no_params.ipynb\n",
      "Converted test_clustering_no_params.ipynb to TestClusteringNoParamsFlow in: test_clustering_no_params.py\n",
      "Converted test_clustering.ipynb to TestClusteringFlow in: test_clustering.py\n",
      "Converted test_export.ipynb to TestExportFlow in: test_export.py\n",
      "Converted test_module.ipynb to TestModuleFlow in: test_module.py\n",
      "Converted test_data_handling.ipynb to TestDataHandlingFlow in: test_data_handling.py\n"
     ]
    }
   ],
   "source": [
    "generate_flows(Config(cfg_name=\"test/settings.ini\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "@call_parse\n",
    "def sciflow_generate():\n",
    "    generate_flows(Config())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verify Flow Generation\n",
    "\n",
    "> Check that flows are valid and that they run. Running is a longer operation so be careful not to overload your build with slow cycles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "def check_flows(config, flow_command=\"show\", ignore_suffix=\"_no_params.py\"):\n",
    "    flow_results = {}\n",
    "    flows_dir = config.path(\"flows_path\")\n",
    "    if ignore_suffix:\n",
    "        flow_paths = [p for p in os.listdir(flows_dir) if not p.endswith(ignore_suffix)]\n",
    "    else:\n",
    "        flow_paths = os.listdir(flows_dir)\n",
    "    for flow_path in flow_paths:\n",
    "        flow_name = os.path.basename(flow_path)\n",
    "        if flow_path.endswith(\".py\"):\n",
    "            ret_code, output = check_flow(\n",
    "                flows_dir, flow_path, flow_command=flow_command\n",
    "            )\n",
    "            flow_results[flow_name] = ret_code, output\n",
    "            if ret_code == 0:\n",
    "                print(f\"Flow: {flow_name} {flow_command} verified\")\n",
    "            else:\n",
    "                print(\n",
    "                    f\"Flow: {flow_name} {flow_command} verification failed\\nDetails:\\n{output}\"\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "def prep_mf_env():\n",
    "    if \"USER\" not in os.environ:\n",
    "        try:\n",
    "            os.environ[\"USER\"] = os.environ[\"GIT_COMMITTER_NAME\"]\n",
    "        except KeyError:\n",
    "            raise EnvironmentError(\n",
    "                \"Metaflow requires a known user for tracked execution. Add USER or GIT_COMMITTER_NAME to Jupyter environment variables\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "def run_shell_cmd(script):\n",
    "    pipe = subprocess.Popen(\n",
    "        \"%s\" % script, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, shell=True\n",
    "    )\n",
    "    output = pipe.communicate()[0]\n",
    "    return pipe, output.decode(\"utf-8\").strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "def check_flow(flows_dir, flow_module, flow_command=\"show\", params=None):\n",
    "    prep_mf_env()\n",
    "    if params:\n",
    "        args = ' '.join([f'--{p[0]} {p[1]}' for p in params])\n",
    "        script = f\"python '{os.path.join(flows_dir, flow_module)}' {flow_command} {args}\"\n",
    "    else:\n",
    "        script = f\"python '{os.path.join(flows_dir, flow_module)}' {flow_command}\"\n",
    "    pipe, output = run_shell_cmd(script)\n",
    "    return pipe.returncode, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = [('speed', 'fast'), ('traffic_percent', 10)]\n",
    "assert('--speed fast --traffic_percent 10' == ' '.join([f'--{p[0]} {p[1]}' for p in params]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flow: test_module.py show verified\n",
      "Flow: test_clustering_no_params.py show verified\n",
      "Flow: test_clustering.py show verified\n",
      "Flow: test_data_handling.py show verified\n",
      "Flow: test_export.py show verified\n"
     ]
    }
   ],
   "source": [
    "check_flows(Config(cfg_name=\"test/settings.ini\"), ignore_suffix=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flow: test_module.py --no-pylint run verified\n",
      "Flow: test_clustering.py --no-pylint run verified\n",
      "Flow: test_data_handling.py --no-pylint run verified\n",
      "Flow: test_export.py --no-pylint run verified\n"
     ]
    }
   ],
   "source": [
    "# slow\n",
    "\n",
    "check_flows(Config(cfg_name=\"test/settings.ini\"), \"--no-pylint run\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "def run_flow(nb_path, params=None):\n",
    "    flow_path = get_flow_path(nb_path)\n",
    "    print(f'Running flow: {os.path.basename(flow_path)}')\n",
    "    ret_code, output = check_flow(flow_path.parent, os.path.basename(flow_path), flow_command=\"--no-pylint run\", params=params)\n",
    "    return ret_code, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running flow: test_clustering.py\n",
      "Metaflow 2.2.7 executing TestClusteringFlow for user:e02079\n",
      "Validating your flow...\n",
      "    The graph looks good!\n",
      "2021-04-22 11:04:56.131 Workflow starting (run-id 1619089496127905):\n",
      "2021-04-22 11:04:56.136 [1619089496127905/start/1 (pid 1913)] Task is starting.\n",
      "2021-04-22 11:04:58.328 [1619089496127905/start/1 (pid 1913)] Task finished successfully.\n",
      "2021-04-22 11:04:58.334 [1619089496127905/preprocess/2 (pid 1928)] Task is starting.\n",
      "2021-04-22 11:05:26.503 [1619089496127905/preprocess/2 (pid 1928)] Task finished successfully.\n",
      "2021-04-22 11:05:26.509 [1619089496127905/fit/3 (pid 1965)] Task is starting.\n",
      "2021-04-22 11:05:28.702 [1619089496127905/fit/3 (pid 1965)] Task finished successfully.\n",
      "2021-04-22 11:05:28.708 [1619089496127905/evaluate/4 (pid 1980)] Task is starting.\n",
      "2021-04-22 11:05:30.986 [1619089496127905/evaluate/4 (pid 1980)] Task finished successfully.\n",
      "2021-04-22 11:05:30.992 [1619089496127905/end/5 (pid 1995)] Task is starting.\n",
      "2021-04-22 11:05:33.103 [1619089496127905/end/5 (pid 1995)] INFO:test_clustering:Running command 'track_flow'\n",
      "2021-04-22 11:05:33.504 [1619089496127905/end/5 (pid 1995)] INFO:test_clustering:Started run with ID \"37\"\n",
      "2021-04-22 11:05:33.659 [1619089496127905/end/5 (pid 1995)] INFO:test_clustering:Completed after 0:00:01\n",
      "2021-04-22 11:05:34.326 [1619089496127905/end/5 (pid 1995)] Task finished successfully.\n",
      "2021-04-22 11:05:34.327 Done!\n"
     ]
    }
   ],
   "source": [
    "# slow\n",
    "\n",
    "ret_code, output = run_flow(Path('/home/jovyan/git/sciflow/nbs/test/test_clustering.ipynb'), \n",
    "         params=[('traffic_percent', 10), ('speed', 'fast')]\n",
    "        )\n",
    "assert(ret_code == 0)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aynsc Flow Running\n",
    "\n",
    "> Run the flow you are working on from the notebook you are working on. This maximises the amount fo experiments you can run as you don't have down time. While long running tasks are running you can keep exploring! :-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "async def run_flow_task(flow_path, param_grid=None):\n",
    "    experiment_id = None\n",
    "    flows_dir = flow_path.parent\n",
    "    flow_module = os.path.basename(flow_path)\n",
    "    flow_command=\"--no-pylint run\"\n",
    "    prep_mf_env()\n",
    "    if params:\n",
    "        args = ' '.join([f'--{k} {v}' for k, v in param_grid.items()])\n",
    "        cmd = f\"python '{os.path.join(flows_dir, flow_module)}' {flow_command} {args}\"\n",
    "    else:\n",
    "        cmd = f\"python '{os.path.join(flows_dir, flow_module)}' {flow_command}\"\n",
    "    proc = await asyncio.create_subprocess_shell(\n",
    "        cmd,\n",
    "        stdout=asyncio.subprocess.PIPE,\n",
    "        stderr=asyncio.subprocess.PIPE)\n",
    "\n",
    "    stdout, stderr = await proc.communicate()\n",
    "\n",
    "    print(f'[{cmd!r} exited with {proc.returncode}]')\n",
    "    if stdout:\n",
    "        output = stdout.decode(\"utf-8\").strip()\n",
    "        experiment_id = [s.strip('\"') for s in re.search('Started run with ID \"\\d+\"', output).group(0).split('ID ')][1]\n",
    "        print(f'[stdout]\\n{output}')\n",
    "    if stderr:\n",
    "        print(f'[stderr]\\n{stderr.decode(\"utf-8\").strip()}')\n",
    "        \n",
    "    return experiment_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "def run_flow_async(nb_path, params=None):\n",
    "    flow_path = get_flow_path(nb_path)\n",
    "    loop = asyncio.get_event_loop()\n",
    "    task = loop.create_task(run_flow_task(flow_path, params))\n",
    "    task.add_done_callback(lambda x: print(f'Task {x.result()} finished'))\n",
    "    return task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = run_flow_async(os.path.join(\"test\", \"test_clustering.ipynb\"),\n",
    "                     params={'traffic_percent': 10, 'speed': 'fast'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task.done()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'39'"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'traffic_percent':[1,5,10,20,50,100], 'speed': ['fast', 'slow'], 'workers': [1]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "def iter_param_grid(param_grid):\n",
    "    # https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/model_selection/_search.py\n",
    "    for p in [param_grid]:\n",
    "        # Always sort the keys of a dictionary, for reproducibility\n",
    "        items = sorted(p.items())\n",
    "        if not items:\n",
    "            yield {}\n",
    "        else:\n",
    "            keys, values = zip(*items)\n",
    "            for v in product(*values):\n",
    "                params = dict(zip(keys, v))\n",
    "                yield params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert( [{'a': 1, 'b': 1, 'c': 'hello'}, {'a': 2, 'b': 1, 'c': 'hello'}] == list(iter_param_grid({'a': [1,2], 'b': [1], 'c': ['hello']})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_grid_space(param_grid: Dict[str, Iterable[Any]], num_samples: int):\n",
    "    samples = []\n",
    "    for i, sample in enumerate(iter_param_grid(param_grid)):\n",
    "        samples.append(sample)\n",
    "    if num_samples < len(samples):\n",
    "        samples = pd.Series(samples).sample(num_samples).tolist()\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_space = sample_grid_space({'a': [1,2], 'b': [1], 'c': ['hello']}, 1)\n",
    "assert(sample_space[0]['b'] == 1)\n",
    "assert(sample_space[0]['c'] == 'hello')\n",
    "assert(sample_space[0]['a'] == 1 or sample_space[0]['a'] == 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "def search_flow_grid(nb_path, param_grid, num_procs=None):\n",
    "    max_process_count = int((multiprocessing.cpu_count()/2) - 1)\n",
    "    param_sample_space = sample_grid_space(param_grid, max_process_count)\n",
    "    tasks = []\n",
    "    for param_sample in param_sample_space:\n",
    "        tasks.append(run_flow_async(nb_path, params=param_sample))\n",
    "    return tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_path = '/home/jovyan/git/sciflow/nbs/test/test_clustering.ipynb'\n",
    "tasks = search_flow_grid(nb_path, {'traffic_percent': [1,2,3,4,5,6,7,8,9,10,20,30,40,50], 'speed': ['fast'], 'workers': [1]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[True, True, True, True, True, True, True]"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[t.done() for t in tasks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['47', '48', '50', '49', '52', '53', '51']"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[t.result() for t in tasks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "@call_parse\n",
    "def sciflow_check_flows():\n",
    "    check_flows(Config())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "@call_parse\n",
    "def sciflow_run_flows():\n",
    "    check_flows(Config(), \"--no-pylint run\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:jovyan-discovery]",
   "language": "python",
   "name": "conda-env-jovyan-discovery-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
