{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp to_sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Iterable\n",
    "\n",
    "from nbdev.export import find_default_export, get_config, read_nb\n",
    "\n",
    "from sciflow.data_handler import extract_param_meta\n",
    "from sciflow.params import params_as_dict\n",
    "from sciflow.parse_module import FuncDetails, extract_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sciflow.to_metaflow import (\n",
    "    extract_module_only,\n",
    "    get_flow_path,\n",
    "    titleize,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sciflow Notebook to Sagemaker Pipeline\n",
    "\n",
    "> Converts from a `sciflow` format notebook to a `sagemaker` pipeline. \n",
    "\n",
    "Currently Supported features:\n",
    "\n",
    "* Linear/sequential DAGs\n",
    "* Simple `Parameters`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_path = Path(os.path.join(\"test\", \"test_clustering.ipynb\"))\n",
    "nb = read_nb(nb_path)\n",
    "module_name = find_default_export(nb[\"cells\"]).replace(\".\", \"/\")\n",
    "test_module = os.path.join(get_config().path(\"lib_path\"), f\"{module_name}.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "flow_path = get_flow_path(nb_path, flow_provider=\"sagemaker\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Path('/home/sagemaker-user/git/sciflow/nbs/test/flows/sagemaker/test_clustering.py')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flow_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = extract_steps(test_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "def nb_to_sagemaker_pipeline(\n",
    "    nb_path: Path, flow_path: Path, silent=True, track_experiment=True\n",
    "):\n",
    "    nb = read_nb(nb_path)\n",
    "    lib_name = get_config().get(\"lib_name\")\n",
    "    module_name = find_default_export(nb[\"cells\"])\n",
    "    if not module_name:\n",
    "        return\n",
    "    module_name = module_name\n",
    "    path_sep_module_name = module_name.replace(\".\", \"/\")\n",
    "    nb_name = os.path.basename(nb_path)\n",
    "    exported_module = os.path.join(\n",
    "        get_config().path(\"lib_path\"), f\"{path_sep_module_name}.py\"\n",
    "    )\n",
    "    steps = extract_steps(exported_module)\n",
    "    if len(steps) == 0:\n",
    "        print(\"Skipping sagemaker conversion - not steps found\")\n",
    "        return\n",
    "    params = params_as_dict(nb_path)\n",
    "    if len(params) == 0:\n",
    "        print(f\"No params cell found for: {os.path.basename(nb_path)}\")\n",
    "    pipeline_class_name = f\"{titleize(extract_module_only(module_name))}Pipeline\"\n",
    "    write_pipeline_to_files(\n",
    "        flow_path,\n",
    "        pipeline_class_name,\n",
    "        lib_name,\n",
    "        module_name,\n",
    "        steps,\n",
    "        params,\n",
    "        track_experiment,\n",
    "    )\n",
    "    if not silent:\n",
    "        print(\n",
    "            f\"Converted {nb_name} to {pipeline_class_name} in: {os.path.basename(flow_path)}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "def is_train_step(step):\n",
    "    return any(step.name.startswith(prefix) for prefix in ('fit', 'train'))\n",
    "\n",
    "def is_processing_step(step):\n",
    "    return not is_train_step(step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_step = FuncDetails('fit_something', docstring=None, args=None, \n",
    "                            has_return=False, return_stmt=None, code='')\n",
    "proc_step = FuncDetails('blabla', docstring=None, args=None, \n",
    "                            has_return=False, return_stmt=None, code='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(not is_processing_step(training_step))\n",
    "assert(is_train_step(training_step))\n",
    "assert(is_processing_step(proc_step))\n",
    "assert(not is_train_step(proc_step))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "lib_name = get_config().get(\"lib_name\")\n",
    "module_name = find_default_export(nb[\"cells\"])\n",
    "fq_module_name = f\"{lib_name}.{module_name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  export\n",
    "\n",
    "\n",
    "def write_pipeline_to_files(\n",
    "    flow_path: Path,\n",
    "    pipeline_class_name: str,\n",
    "    lib_name: str,\n",
    "    module_name: str,\n",
    "    steps: Iterable[FuncDetails],\n",
    "    params: dict,\n",
    "    track_experiment: bool,\n",
    "):\n",
    "    if not os.path.exists(flow_path.parent):\n",
    "        os.mkdir(flow_path.parent)\n",
    "    fq_module_name = f\"{lib_name}.{module_name}\"\n",
    "    param_meta = extract_param_meta(fq_module_name, params)\n",
    "    with open(flow_path, \"w\") as flow_file:\n",
    "        flow_file.write(\"#!/usr/bin/env python\\n\")\n",
    "        flow_file.write(\"# coding=utf-8\\n\")\n",
    "        flow_file.write(\"# SCIFLOW GENERATED FILE - EDIT COMPANION NOTEBOOK\\n\")\n",
    "\n",
    "        flow_file.write(\"import os\\n\")\n",
    "\n",
    "        flow_file.write(\"import sagemaker\\n\")\n",
    "        flow_file.write(\"from sagemaker.session import Session\\n\")\n",
    "        flow_file.write(\"from sagemaker.workflow.pipeline import Pipeline\\n\")\n",
    "\n",
    "        has_train_step = any(\n",
    "            [is_train_step(s) for s in steps]\n",
    "        )\n",
    "        has_processing_step = sum(\n",
    "            [is_processing_step(s) for s in steps]\n",
    "        ) != len(steps)\n",
    "\n",
    "        if has_train_step and has_processing_step:\n",
    "            flow_file.write(\n",
    "                \"from sagemaker.workflow.steps import ProcessingStep, TrainingStep\\n\"\n",
    "            )\n",
    "        if has_processing_step:\n",
    "            flow_file.write(\n",
    "                \"from sagemaker.processing import ScriptProcessor, ProcessingInput, ProcessingOutput\\n\"\n",
    "            )\n",
    "            flow_file.write(\"from sagemaker.workflow.pipeline import Pipeline\\n\")\n",
    "        if has_train_step:\n",
    "            flow_file.write(\"from sagemaker.inputs import TrainingInput\\n\")\n",
    "            flow_file.write(\"from sagemaker.estimator import Estimator\\n\")\n",
    "\n",
    "        has_sm_param = any((p.has_sagemaker_param for p in param_meta.values()))\n",
    "        if has_sm_param:\n",
    "            instance_types = [p.instance_type for p in param_meta.values()]\n",
    "            sm_params_import = \"from sagemaker.workflow.parameters import \"\n",
    "            if int in instance_types:\n",
    "                sm_params_import += \"ParameterInteger\"\n",
    "                if float in instance_types or str in instance_types:\n",
    "                    sm_params_import += \", \"\n",
    "            if float in instance_types:\n",
    "                sm_params_import += \"ParameterFloat\"\n",
    "                if str in instance_types:\n",
    "                    sm_params_import += \", \"\n",
    "            if str in instance_types:\n",
    "                sm_params_import += \"ParameterString\"\n",
    "\n",
    "            flow_file.write(sm_params_import + \"\\n\")\n",
    "\n",
    "        flow_file.write(\"\\n\")\n",
    "        flow_file.write(\n",
    "            f\"from {fq_module_name} import {', '.join([s.name for s in steps])}\\n\"\n",
    "        )\n",
    "        if len(params) > 0:\n",
    "            flow_file.write(\n",
    "                f\"from {fq_module_name} import {', '.join(params.keys())}\\n\"\n",
    "            )\n",
    "\n",
    "        flow_file.write(f\"\\n\\nclass {pipeline_class_name}():\\n\")\n",
    "        single_indent = \"    \"\n",
    "        write_params(flow_file, param_meta, single_indent)\n",
    "        flow_file.write(\"\\n\")\n",
    "        write_steps(\n",
    "            fq_module_name,\n",
    "            flow_file,\n",
    "            steps,\n",
    "            param_meta,\n",
    "            single_indent,\n",
    "            track_experiment,\n",
    "        )\n",
    "        flow_file.write(\"\\n\")\n",
    "\n",
    "        flow_file.write('if __name__ == \"__main__\":\\n')\n",
    "        flow_file.write(f\"{single_indent}{pipeline_class_name}()\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "def write_observers(lib_name, flow_file, module_name, bucket_name, project):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "def write_track_flow(flow_file, track_experiment):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  export\n",
    "\n",
    "\n",
    "def write_params(flow_file, param_meta, single_indent):\n",
    "    for param in param_meta.keys():\n",
    "        if param_meta[param].instance_type == int:\n",
    "            flow_file.write(\n",
    "                f\"{single_indent}{param} = ParameterInteger(name='{param}', default_value={param})\\n\"\n",
    "            )\n",
    "        elif param_meta[param].instance_type == float:\n",
    "            flow_file.write(\n",
    "                f\"{single_indent}{param} = ParameterFloat(name='{param}', default_value={param})\\n\"\n",
    "            )\n",
    "        elif param_meta[param].instance_type == str:\n",
    "            flow_file.write(\n",
    "                f\"{single_indent}{param} = ParameterString(name='{param}', default_value={param})\\n\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_script_processor(flow_file):\n",
    "    flow_file.write(\n",
    "        \"\"\"\n",
    "        script_processor = ScriptProcessor(\n",
    "                command=['python3'],\n",
    "                image_uri=\"141502667606.dkr.ecr.eu-west-1.amazonaws.com/sagemaker-scikit-learn:0.23-1-cpu-py3\",\n",
    "                role=self.role,\n",
    "                instance_count=1,\n",
    "                instance_type=\"ml.m5.xlarge\",\n",
    "                sagemaker_session=self.sagemaker_session,\n",
    "                env={'AWS_DEFAULT_REGION': self.region},\n",
    "                base_job_name=f'processing-job/{__file__}'\n",
    "        )\n",
    "        \"\"\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "def format_arg(arg, param_meta):\n",
    "    if arg in param_meta and not param_meta[arg].has_metaflow_param:\n",
    "        result = arg\n",
    "    else:\n",
    "        result = \"self.\" + arg\n",
    "    return result\n",
    "\n",
    "\n",
    "def write_steps(\n",
    "    fq_module_name, flow_file, steps, param_meta, single_indent, track_experiment\n",
    "):\n",
    "    [s.name for s in steps]\n",
    "    for i, step in enumerate(steps):\n",
    "        flow_file.write(f\"{single_indent}def {step.name}(self):\\n\")\n",
    "        if step.docstring:\n",
    "            flow_file.write(f\"{indent_multiline(step.docstring, 2)}\\n\")\n",
    "        # Processing step\n",
    "        if not any([step.name.startswith(n) for n in (\"fit\", \"train\")]):\n",
    "            write_script_processor(flow_file)\n",
    "\n",
    "            flow_file.write(\"\\n\")\n",
    "            flow_file.write(\n",
    "                f\"{single_indent}{single_indent}{step.name}_step = ProcessingStep(\\n\"\n",
    "            )\n",
    "            flow_file.write(\n",
    "                f\"{single_indent}{single_indent}{single_indent}name={step.name}\\n\"\n",
    "            )\n",
    "            flow_file.write(\n",
    "                f\"{single_indent}{single_indent}{single_indent}processor = script_processor\\n\"\n",
    "            )\n",
    "            flow_file.write(\n",
    "                f\"{single_indent}{single_indent}{single_indent}code = {fq_module_name}_{step.name}.py\\n\"\n",
    "            )\n",
    "            flow_file.write(f\"{single_indent}{single_indent})\\n\")\n",
    "        else:\n",
    "            # Trainign step\n",
    "            pass\n",
    "        flow_file.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "def write_track_capture(flow_file):\n",
    "    flow_file.write(\n",
    "        f\"\"\"\n",
    "        for key in results.keys():\n",
    "            if key in self.__dict__:\n",
    "                self.__dict__[key] = self.__dict__[key] + results[key]\n",
    "            else:\n",
    "                self.__dict__[key] = results[key]\n",
    "\n",
    "\"\"\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('something', ''),\n",
       " ('preprocess', 'model_level,min_date,traffic_percent'),\n",
       " ('fit', 'documents,workers'),\n",
       " ('evaluate', 'model')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(s.name, s.args) for s in steps]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "950"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "steps[-1].code.find(\"results = \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "def get_return_var_names(step):\n",
    "    results_index = step.code.find(\"results =\")\n",
    "    if results_index == -1:\n",
    "        return []\n",
    "    return [\n",
    "        l.split(\":\")[1].strip(\", \\}\")\n",
    "        for l in step.code[results_index:].split(\"\\n\")\n",
    "        if l.strip().find(\":\") > -1\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "def something():\n",
      "    print(\"The first step\")\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(steps[0].code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "def preprocess(model_level=None, min_date=None, traffic_percent=100):\n",
      "    data = get_utterances(model_level, min_date, traffic_percent)\n",
      "    documents = data.tolist()\n",
      "    results = {\"documents\": documents}\n",
      "    return results\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(steps[1].code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "def fit(documents, workers=workers):\n",
      "    model = Topics(documents, workers=workers)\n",
      "    results = {\"model\": model}\n",
      "    return results\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(steps[2].code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[], ['documents'], ['model'], ['word_summaries', 'artifacts', 'metrics']]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[get_return_var_names(s) for s in steps]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"word_summaries\": \"word_summaries\", \"artifacts\": \"artifacts\", \"metrics\": \"metrics\"}'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json.dumps(\n",
    "    {\"word_summaries\": \"word_summaries\", \"artifacts\": \"artifacts\", \"metrics\": \"metrics\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'word_summaries': 'word_summaries',\n",
       " 'artifacts': 'artifacts',\n",
       " 'metrics': 'metrics'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json.loads(\n",
    "    '{\"word_summaries\": \"word_summaries\",\"artifacts\": \"artifacts\",\"metrics\": \"metrics\"}'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['FuncDetails(name=something,args=,has_return=False):\\ndef something():\\n    print(\"The first step\")',\n",
       " ('FuncDetails(name=preprocess,args=model_level,min_date,traffic_percent,has_return=True):\\n'\n",
       "  'def preprocess(model_level=None, min_date=None, traffic_percent=100):\\n'\n",
       "  '    data = get_utterances(model_level, min_date, traffic_percent)\\n'\n",
       "  '    documents = data.tolist()\\n'\n",
       "  '    results = {\"documents\": documents}\\n'\n",
       "  '    return results'),\n",
       " ('FuncDetails(name=fit,args=documents,workers,has_return=True):\\n'\n",
       "  'def fit(documents, workers=workers):\\n'\n",
       "  '    model = Topics(documents, workers=workers)\\n'\n",
       "  '    results = {\"model\": model}\\n'\n",
       "  '    return results'),\n",
       " ('FuncDetails(name=evaluate,args=model,has_return=True):\\n'\n",
       "  'def evaluate(model):\\n'\n",
       "  '    topic_words, word_scores, topic_nums = model.get_topics(model.get_num_topics())\\n'\n",
       "  '\\n'\n",
       "  '    topic_contains_non_empty_words = all([len(tw) > 0 for tw in topic_words])\\n'\n",
       "  '    word_scores_in_range = word_scores.min() >= 0.0 and word_scores.max() <= 1.0\\n'\n",
       "  '    as_many_items_as_topics = (\\n'\n",
       "  '        model.get_num_topics() == len(topic_words) == word_scores.shape[0]\\n'\n",
       "  '    )\\n'\n",
       "  '    word_summaries = (\\n'\n",
       "  '        topic_contains_non_empty_words\\n'\n",
       "  '        and word_scores_in_range\\n'\n",
       "  '        and as_many_items_as_topics\\n'\n",
       "  '    )\\n'\n",
       "  '    # You can add artifacts in a step that will be saved to block storage. Add the paths to the file on the local '\n",
       "  'filesystem\\n'\n",
       "  '    # and the artifact will be uploaded to remote storage.\\n'\n",
       "  '    artifacts = [lib_path(\"nbs\", \"test\", \"dataframe_artifact.csv\")]\\n'\n",
       "  '    # You can add step metrics too this time just add a list of 3-tuples where tuple order = (name, value, step)\\n'\n",
       "  '    metrics = [(\"mae\", 100, 0), (\"mae\", 67, 1), (\"mae\", 32, 2)]\\n'\n",
       "  '    results = {\\n'\n",
       "  '        \"word_summaries\": word_summaries,\\n'\n",
       "  '        \"artifacts\": artifacts,\\n'\n",
       "  '        \"metrics\": metrics,\\n'\n",
       "  '    }\\n'\n",
       "  '    return results')]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_args(step, param_names, flow_scope):\n",
    "    if len(step.args) > 0:\n",
    "        args = [x.strip() for x in step.args.split(\",\")]\n",
    "        input_vars = [a for a in args if a in param_names]\n",
    "        created_vars = [a for a in args if a in flow_scope]\n",
    "        #print(args)\n",
    "        #print(input_vars + created_vars)\n",
    "        unscoped_vars = set(args).difference(set(input_vars + created_vars))\n",
    "        #print(unscoped_vars)\n",
    "        if len(unscoped_vars) > 0:\n",
    "            raise ValueError(f'Step: {step.name} depends on variable(s), \"{unscoped_vars}\"  ,which not in the flow scope')\n",
    "        return input_vars, created_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['model_level', 'min_date', 'traffic_percent'], [])"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classify_args(steps[1], param_names, flow_scope)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "def get_sm_step_chain(steps, param_meta):\n",
    "    param_names = list(param_meta.keys())\n",
    "    flow_scope = []\n",
    "    for step in steps:\n",
    "        return_vars = get_return_var_names(step)\n",
    "        args_by_class = classify_args(step, param_names, flow_scope)\n",
    "        if args_by_class is not None:\n",
    "            input_vars, created_vars = args_by_class\n",
    "        if is_processing_step(step):\n",
    "            print(f\"Step: {step.name}\")\n",
    "            if len(step.args) > 0:\n",
    "                print(f\"Step Args: {step.args}\")\n",
    "            if args_by_class is not None:\n",
    "                print(f\"Args that are params: {input_vars}\")\n",
    "                print(f\"Args that are in flow scope: {created_vars}\")\n",
    "                print(f\"ProcessingInputs: {created_vars}\")\n",
    "            print(f\"ProcessingOutputs: {return_vars}\")\n",
    "        elif is_train_step(step):\n",
    "            print(f\"Step: {step.name}\")\n",
    "            if len(step.args) > 0:\n",
    "                print(f\"Step Args: {step.args}\")\n",
    "            if args_by_class is not None:\n",
    "                print(f\"Args that are params: {input_vars}\")\n",
    "                print(f\"Args that are in flow scope: {created_vars}\")\n",
    "                print(f\"TrainingInputs: {created_vars}\")\n",
    "        print(\"\\n\")\n",
    "        flow_scope.extend(return_vars)\n",
    "        \n",
    "    # ProcessingInput comes from params or from preceding steps\n",
    "    # To put it in scope need full URI\n",
    "        \n",
    "        \n",
    "        \n",
    "    # if is processing step\n",
    "    # determine processing inputs\n",
    "    # determine processing outputs\n",
    "    # pass params as job_arguments - if needed\n",
    "    # args are either processinginputs - which can be returned from another step function or loaded from a training step\n",
    "    # walk through steps?\n",
    "    # for each step what do you need?\n",
    "    # look at args\n",
    "    # look at return vars\n",
    "    # look at params\n",
    "    # match by name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = params_as_dict(nb_path)\n",
    "param_meta = extract_param_meta(fq_module_name, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_names = list(param_meta.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: something\n",
      "ProcessingOutputs: []\n",
      "\n",
      "\n",
      "Step: preprocess\n",
      "Step Args: model_level,min_date,traffic_percent\n",
      "Args that are params: ['model_level', 'min_date', 'traffic_percent']\n",
      "Args that are in flow scope: []\n",
      "ProcessingInputs: []\n",
      "ProcessingOutputs: ['documents']\n",
      "\n",
      "\n",
      "Step: fit\n",
      "Step Args: documents,workers\n",
      "Args that are params: ['workers']\n",
      "Args that are in flow scope: ['documents']\n",
      "TrainingInputs: ['documents']\n",
      "\n",
      "\n",
      "Step: evaluate\n",
      "Step Args: model\n",
      "Args that are params: []\n",
      "Args that are in flow scope: ['model']\n",
      "ProcessingInputs: ['model']\n",
      "ProcessingOutputs: ['word_summaries', 'artifacts', 'metrics']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "get_sm_step_chain(steps, param_meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [\n",
    "#     ('something', ''),\n",
    "#     ('preprocess', 'model_level,min_date,traffic_percent',\n",
    "#      {\n",
    "#          'processing_outputs': ['documents'],\n",
    "#          'job_arguments:' ['model_level,min_date,traffic_percent']\n",
    "#      }\n",
    "#     ),\n",
    "#     ('fit', 'documents,workers',\n",
    "#         {\n",
    "#           'training_inputs': ['documents']\n",
    "#         }\n",
    "#     ),\n",
    "#     ('evaluate', 'model',\n",
    "#      {\n",
    "#          'processing_inputs': ['fit_model'],\n",
    "#          'processing_outputs': ['word_summaries']\n",
    "#      }\n",
    "#     )\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted test_clustering.ipynb to TestClusteringPipeline in: test_clustering.py\n"
     ]
    }
   ],
   "source": [
    "nb_to_sagemaker_pipeline(nb_path, flow_path, silent=False, track_experiment=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.m5.xlarge",
  "kernelspec": {
   "display_name": "sciflow (sciflow/3)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:eu-west-1:368653567616:image-version/sciflow/3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
