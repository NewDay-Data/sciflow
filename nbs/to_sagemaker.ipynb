{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp to_sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Iterable\n",
    "\n",
    "from nbdev.export import find_default_export, get_config, read_nb\n",
    "\n",
    "from sciflow.data_handler import extract_param_meta\n",
    "from sciflow.params import params_as_dict\n",
    "from sciflow.parse_module import FuncDetails, extract_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sciflow.to_metaflow import extract_module_only, get_flow_path, titleize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sciflow Notebook to Sagemaker Pipeline\n",
    "\n",
    "> Converts from a `sciflow` format notebook to a `sagemaker` pipeline. \n",
    "\n",
    "Currently Supported features:\n",
    "\n",
    "* Linear/sequential DAGs\n",
    "* Simple `Parameters`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_path = Path(os.path.join(\"test\", \"test_clustering.ipynb\"))\n",
    "nb = read_nb(nb_path)\n",
    "module_name = find_default_export(nb[\"cells\"]).replace(\".\", \"/\")\n",
    "test_module = os.path.join(get_config().path(\"lib_path\"), f\"{module_name}.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "flow_path = get_flow_path(nb_path, flow_provider=\"sagemaker\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Path('/home/sagemaker-user/git/sciflow/nbs/test/flows/sagemaker/test_clustering.py')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flow_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = extract_steps(test_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "def nb_to_sagemaker_pipeline(\n",
    "    nb_path: Path, flow_path: Path, silent=True, track_experiment=True\n",
    "):\n",
    "    nb = read_nb(nb_path)\n",
    "    lib_name = get_config().get(\"lib_name\")\n",
    "    module_name = find_default_export(nb[\"cells\"])\n",
    "    if not module_name:\n",
    "        return\n",
    "    module_name = module_name\n",
    "    path_sep_module_name = module_name.replace(\".\", \"/\")\n",
    "    nb_name = os.path.basename(nb_path)\n",
    "    exported_module = os.path.join(\n",
    "        get_config().path(\"lib_path\"), f\"{path_sep_module_name}.py\"\n",
    "    )\n",
    "    steps = extract_steps(exported_module)\n",
    "    if len(steps) == 0:\n",
    "        print(\"Skipping sagemaker conversion - not steps found\")\n",
    "        return\n",
    "    params = params_as_dict(nb_path)\n",
    "    if len(params) == 0:\n",
    "        print(f\"No params cell found for: {os.path.basename(nb_path)}\")\n",
    "    pipeline_class_name = f\"{titleize(extract_module_only(module_name))}Pipeline\"\n",
    "    write_pipeline_to_files(\n",
    "        flow_path,\n",
    "        pipeline_class_name,\n",
    "        lib_name,\n",
    "        module_name,\n",
    "        steps,\n",
    "        params,\n",
    "        track_experiment,\n",
    "    )\n",
    "    if not silent:\n",
    "        print(\n",
    "            f\"Converted {nb_name} to {pipeline_class_name} in: {os.path.basename(flow_path)}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "def is_train_step(step):\n",
    "    return any(step.name.startswith(prefix) for prefix in (\"fit\", \"train\"))\n",
    "\n",
    "\n",
    "def is_processing_step(step):\n",
    "    return not is_train_step(step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_step = FuncDetails(\n",
    "    \"fit_something\",\n",
    "    docstring=None,\n",
    "    args=None,\n",
    "    has_return=False,\n",
    "    return_stmt=None,\n",
    "    code=\"\",\n",
    ")\n",
    "proc_step = FuncDetails(\n",
    "    \"blabla\", docstring=None, args=None, has_return=False, return_stmt=None, code=\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert not is_processing_step(training_step)\n",
    "assert is_train_step(training_step)\n",
    "assert is_processing_step(proc_step)\n",
    "assert not is_train_step(proc_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "lib_name = get_config().get(\"lib_name\")\n",
    "module_name = find_default_export(nb[\"cells\"])\n",
    "fq_module_name = f\"{lib_name}.{module_name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  export\n",
    "\n",
    "\n",
    "def write_pipeline_to_files(\n",
    "    flow_path: Path,\n",
    "    pipeline_class_name: str,\n",
    "    lib_name: str,\n",
    "    module_name: str,\n",
    "    steps: Iterable[FuncDetails],\n",
    "    params: dict,\n",
    "    track_experiment: bool,\n",
    "):\n",
    "    if not os.path.exists(flow_path.parent):\n",
    "        os.mkdir(flow_path.parent)\n",
    "    fq_module_name = f\"{lib_name}.{module_name}\"\n",
    "    param_meta = extract_param_meta(fq_module_name, params)\n",
    "    with open(flow_path, \"w\") as flow_file:\n",
    "        flow_file.write(\"#!/usr/bin/env python\\n\")\n",
    "        flow_file.write(\"# coding=utf-8\\n\")\n",
    "        flow_file.write(\"# SCIFLOW GENERATED FILE - EDIT COMPANION NOTEBOOK\\n\")\n",
    "\n",
    "        flow_file.write(\"import os\\n\")\n",
    "\n",
    "        flow_file.write(\"import sagemaker\\n\")\n",
    "        flow_file.write(\"from sagemaker.session import Session\\n\")\n",
    "        flow_file.write(\"from sagemaker.workflow.pipeline import Pipeline\\n\")\n",
    "\n",
    "        has_train_step = any([is_train_step(s) for s in steps])\n",
    "        has_processing_step = sum([is_processing_step(s) for s in steps]) != len(steps)\n",
    "\n",
    "        if has_train_step and has_processing_step:\n",
    "            flow_file.write(\n",
    "                \"from sagemaker.workflow.steps import ProcessingStep, TrainingStep\\n\"\n",
    "            )\n",
    "        if has_processing_step:\n",
    "            flow_file.write(\n",
    "                \"from sagemaker.processing import ScriptProcessor, ProcessingInput, ProcessingOutput\\n\"\n",
    "            )\n",
    "            flow_file.write(\"from sagemaker.workflow.pipeline import Pipeline\\n\")\n",
    "        if has_train_step:\n",
    "            flow_file.write(\"from sagemaker.inputs import TrainingInput\\n\")\n",
    "            flow_file.write(\"from sagemaker.estimator import Estimator\\n\")\n",
    "\n",
    "        has_sm_param = any((p.has_sagemaker_param for p in param_meta.values()))\n",
    "        if has_sm_param:\n",
    "            instance_types = [p.instance_type for p in param_meta.values()]\n",
    "            sm_params_import = \"from sagemaker.workflow.parameters import \"\n",
    "            if int in instance_types:\n",
    "                sm_params_import += \"ParameterInteger\"\n",
    "                if float in instance_types or str in instance_types:\n",
    "                    sm_params_import += \", \"\n",
    "            if float in instance_types:\n",
    "                sm_params_import += \"ParameterFloat\"\n",
    "                if str in instance_types:\n",
    "                    sm_params_import += \", \"\n",
    "            if str in instance_types:\n",
    "                sm_params_import += \"ParameterString\"\n",
    "\n",
    "            flow_file.write(sm_params_import + \"\\n\")\n",
    "\n",
    "        flow_file.write(\"\\n\")\n",
    "        flow_file.write(\n",
    "            f\"from {fq_module_name} import {', '.join([s.name for s in steps])}\\n\"\n",
    "        )\n",
    "        if len(params) > 0:\n",
    "            flow_file.write(\n",
    "                f\"from {fq_module_name} import {', '.join(params.keys())}\\n\"\n",
    "            )\n",
    "\n",
    "        flow_file.write(f\"\\n\\nclass {pipeline_class_name}():\\n\")\n",
    "        single_indent = \"    \"\n",
    "        write_params(flow_file, param_meta, single_indent)\n",
    "        flow_file.write(\"\\n\")\n",
    "        flow_file.write(f\"{single_indent}steps = {[s.name for s in steps]}\\n\")\n",
    "        flow_file.write(\"\\n\")\n",
    "        write_steps(\n",
    "            fq_module_name,\n",
    "            flow_file,\n",
    "            steps,\n",
    "            param_meta,\n",
    "            single_indent,\n",
    "            track_experiment,\n",
    "        )\n",
    "        flow_file.write(\"\\n\")\n",
    "\n",
    "        flow_file.write(\n",
    "            \"\"\"\n",
    "    def get_pipeline(self) -> Pipeline:\n",
    "        pipeline_steps = [getattr(self, step)() for step in self.steps]\n",
    "        pipeline = Pipeline(\n",
    "            name=\"test-clustering-sm-pipeline\",\n",
    "            parameters=[\n",
    "                self.traffic_percent,\n",
    "                self.workers,\n",
    "                self.model_level,\n",
    "                self.min_date \n",
    "            ],\n",
    "            steps = pipeline_steps,\n",
    "            sagemaker_session = self.sagemaker_session,\n",
    "        )\n",
    "        \n",
    "        return pipeline\n",
    "    \n",
    "    def run(self):\n",
    "        self.bucket = os.environ['SCIFLOW_BUCKET']\n",
    "        self.role = sagemaker.get_execution_role()\n",
    "        self.region = 'eu-west-1'\n",
    "        self.sagemaker_session = Session(default_bucket=self.bucket)\n",
    "        \n",
    "        self.flow_name = \"test-clustering\"\n",
    "        from datetime import datetime\n",
    "        run_timestamp = datetime.today().__str__().replace(':', '-').replace('.', '-').replace(' ', '-')[:-3]\n",
    "        self.flow_run_id = f\"pipeline-{run_timestamp}\"\n",
    "        self.s3_prefix = f\"{self.flow_name}/{self.flow_run_id}\"\n",
    "        self.flow_s3_uri = f\"s3://{self.bucket}/{self.s3_prefix}\"\n",
    "        \n",
    "        # Upload processing code\n",
    "        self.s3.upload_file('test_clustering_something.py', self.bucket, f\"{self.s3_prefix}/code/test_clustering_something.py\")\n",
    "        self.s3.upload_file('test_clustering_preprocess.py', self.bucket, f\"{self.s3_prefix}/code/test_clustering_preprocess.py\")\n",
    "        self.s3.upload_file('test_clustering_evaluate.py', self.bucket, f\"{self.s3_prefix}/code/test_clustering_evaluate.py\")\n",
    "  \n",
    "        from pathlib import Path\n",
    "    \n",
    "        # Upload library\n",
    "        self.upload_directory(Path('/home/sagemaker-user/git/sciflow/sciflow'), self.bucket, f\"{self.s3_prefix}/code/sciflow\")\n",
    "        \n",
    "        pipeline = self.get_pipeline()\n",
    "        pipeline.upsert(role_arn=self.role)\n",
    "        execution = pipeline.start()\n",
    "        execution.wait()\\n\n",
    "\"\"\"\n",
    "        )\n",
    "\n",
    "        flow_file.write('if __name__ == \"__main__\":\\n')\n",
    "        flow_file.write(f\"{single_indent}{pipeline_class_name}().run()\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "def write_observers(lib_name, flow_file, module_name, bucket_name, project):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "def write_track_flow(flow_file, track_experiment):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  export\n",
    "\n",
    "\n",
    "def write_params(flow_file, param_meta, single_indent):\n",
    "    for param in param_meta.keys():\n",
    "        if param_meta[param].instance_type == int:\n",
    "            flow_file.write(\n",
    "                f\"{single_indent}{param} = ParameterInteger(name='{param}', default_value={param})\\n\"\n",
    "            )\n",
    "        elif param_meta[param].instance_type == float:\n",
    "            flow_file.write(\n",
    "                f\"{single_indent}{param} = ParameterFloat(name='{param}', default_value={param})\\n\"\n",
    "            )\n",
    "        elif param_meta[param].instance_type == str:\n",
    "            flow_file.write(\n",
    "                f\"{single_indent}{param} = ParameterString(name='{param}', default_value={param})\\n\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "def upload_directory(s3_res, path, bucketname, prefix):\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        # Ignore non-python source files and IPython checkpoint files\n",
    "        for file in [\n",
    "            f\n",
    "            for f in files\n",
    "            if f.split(\".\")[-1] == \"py\" and root.find(\"ipynb_checkpoints\") == -1\n",
    "        ]:\n",
    "            pass\n",
    "            # print(os.path.join(root, file), bucketname, f\"{prefix}{file}\", file.find('ipynb_checkpoints'))\n",
    "            s3_res.upload_file(os.path.join(root, file), bucketname, f\"{prefix}{file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "def download_directory(bucketname, remote_key, local_dir):\n",
    "    from pathlib import Path\n",
    "\n",
    "    import boto3\n",
    "\n",
    "    s3 = boto3.client(\"s3\")\n",
    "    if not Path(local_dir).exists():\n",
    "        Path(local_dir).mkdir()\n",
    "    all_files = [\n",
    "        obj.key\n",
    "        for obj in boto3.resource(\"s3\")\n",
    "        .Bucket(bucketname)\n",
    "        .objects.filter(Prefix=remote_key)\n",
    "    ]\n",
    "    for file in all_files:\n",
    "        file_name = file.split(\"/\")[-1]\n",
    "        s3.download_file(bucketname, file, f\"{local_dir}/{file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_script_processor(flow_file):\n",
    "    flow_file.write(\n",
    "        \"\"\"\n",
    "        script_processor = ScriptProcessor(\n",
    "                command=['python3'],\n",
    "                image_uri=\"141502667606.dkr.ecr.eu-west-1.amazonaws.com/sagemaker-scikit-learn:0.23-1-cpu-py3\",\n",
    "                role=self.role,\n",
    "                instance_count=1,\n",
    "                instance_type=\"ml.m5.xlarge\",\n",
    "                sagemaker_session=self.sagemaker_session,\n",
    "                env={'AWS_DEFAULT_REGION': self.region},\n",
    "                base_job_name=f'processing-job/{__file__}'\n",
    "        )\n",
    "        \"\"\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "def extract_step_vars(step, param_names, processing_flow_scope, train_flow_scope):\n",
    "    if len(step.args) == 0:\n",
    "        result = {}\n",
    "    else:\n",
    "        args = [x.strip() for x in step.args.split(\",\")]\n",
    "        step_input = [a for a in args if a in param_names]\n",
    "        step_proc_vars = [a for a in args if a in processing_flow_scope]\n",
    "        step_train_vars = [a for a in args if a in train_flow_scope]\n",
    "        unscoped_vars = set(args).difference(\n",
    "            set(step_input + step_proc_vars + step_train_vars)\n",
    "        )\n",
    "        if len(unscoped_vars) > 0:\n",
    "            raise ValueError(\n",
    "                f'Step: {step.name} depends on variable(s), \"{unscoped_vars}\", which are not in the flow scope'\n",
    "            )\n",
    "        result = {\n",
    "            \"step_input\": step_input,\n",
    "            \"step_proc_vars\": step_proc_vars,\n",
    "            \"step_train_vars\": step_train_vars,\n",
    "        }\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_arg_step = FuncDetails(\n",
    "    \"no_arg_step\", docstring=None, args=\"\", has_return=False, return_stmt=None, code=\"\"\n",
    ")\n",
    "single_arg_step = FuncDetails(\n",
    "    \"fit_single_arg_step\",\n",
    "    docstring=None,\n",
    "    args=\"one_param\",\n",
    "    has_return=False,\n",
    "    return_stmt=None,\n",
    "    code=\"\",\n",
    ")\n",
    "multi_arg_step = FuncDetails(\n",
    "    \"multi_arg_step\",\n",
    "    docstring=None,\n",
    "    args=\"one_param,proc_param\",\n",
    "    has_return=False,\n",
    "    return_stmt=None,\n",
    "    code=\"\",\n",
    ")\n",
    "post_train_step = FuncDetails(\n",
    "    \"post_train_step\",\n",
    "    docstring=None,\n",
    "    args=\"one_param,proc_param,train_param\",\n",
    "    has_return=False,\n",
    "    return_stmt=None,\n",
    "    code=\"\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert extract_step_vars(no_arg_step, [\"one_param\"], [\"two_param\"], []) == {}\n",
    "assert extract_step_vars(no_arg_step, [], [], []) == {}\n",
    "raised = False\n",
    "try:\n",
    "    extract_step_vars(single_arg_step, [], [], []) == {}\n",
    "except:\n",
    "    raised = True\n",
    "assert raised\n",
    "assert extract_step_vars(single_arg_step, [\"one_param\"], [\"proc_param\"], []) == {\n",
    "    \"step_input\": [\"one_param\"],\n",
    "    \"step_proc_vars\": [],\n",
    "    \"step_train_vars\": [],\n",
    "}\n",
    "assert extract_step_vars(multi_arg_step, [\"one_param\"], [\"proc_param\"], []) == {\n",
    "    \"step_input\": [\"one_param\"],\n",
    "    \"step_proc_vars\": [\"proc_param\"],\n",
    "    \"step_train_vars\": [],\n",
    "}\n",
    "assert extract_step_vars(\n",
    "    post_train_step, [\"one_param\"], [\"proc_param\"], [\"train_param\"]\n",
    ") == {\n",
    "    \"step_input\": [\"one_param\"],\n",
    "    \"step_proc_vars\": [\"proc_param\"],\n",
    "    \"step_train_vars\": [\"train_param\"],\n",
    "}\n",
    "raised = False\n",
    "try:\n",
    "    assert extract_step_vars(multi_arg_step, [\"one_param\"], [], [])\n",
    "except:\n",
    "    raised = True\n",
    "assert raised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "def format_job_arguments(param_meta):\n",
    "    job_arg_values = [\n",
    "        f\"str(self.{p}.__int__())\"\n",
    "        if param_meta[p].instance_type == int\n",
    "        else f\"str(self.{p}.__float__())\"\n",
    "        if param_meta[p].instance_type == float\n",
    "        else f\"self.{p}.__str__()\"\n",
    "        if param_meta[p].instance_type == str\n",
    "        else f\"str(self.{p})\"\n",
    "        for p in param_meta.keys()\n",
    "    ]\n",
    "    stitched_args = list(zip([f\"--{p}\" for p in param_meta.keys()], job_arg_values))\n",
    "    flattened = [item for sublist in stitched_args for item in sublist]\n",
    "    return flattened"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://nexus_eut_svc:****@ndartifactory.jfrog.io/artifactory/api/pypi/pypi/simple\n",
      "Collecting ipylab\n",
      "\u001b[33m  WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='ndartifactory.jfrog.io', port=443): Read timed out. (read timeout=15)\")': /artifactory/api/pypi/pypi/packages/packages/97/95/0026b38011ef9d64aec4df16347166edbbf3bb0af70f8d8023551474b581/ipylab-0.5.2-py3-none-any.whl\u001b[0m\u001b[33m\n",
      "\u001b[0m  Downloading https://ndartifactory.jfrog.io/artifactory/api/pypi/pypi/packages/packages/97/95/0026b38011ef9d64aec4df16347166edbbf3bb0af70f8d8023551474b581/ipylab-0.5.2-py3-none-any.whl (822 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m822.6/822.6 KB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: ipywidgets<8,>=7.6.0 in /home/sagemaker-user/.local/lib/python3.9/site-packages (from ipylab) (7.6.5)\n",
      "Requirement already satisfied: ipython>=4.0.0 in /home/sagemaker-user/.local/lib/python3.9/site-packages (from ipywidgets<8,>=7.6.0->ipylab) (8.0.1)\n",
      "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /home/sagemaker-user/.local/lib/python3.9/site-packages (from ipywidgets<8,>=7.6.0->ipylab) (1.0.2)\n",
      "Requirement already satisfied: widgetsnbextension~=3.5.0 in /home/sagemaker-user/.local/lib/python3.9/site-packages (from ipywidgets<8,>=7.6.0->ipylab) (3.5.2)\n",
      "Requirement already satisfied: nbformat>=4.2.0 in /home/sagemaker-user/.local/lib/python3.9/site-packages (from ipywidgets<8,>=7.6.0->ipylab) (5.1.3)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /home/sagemaker-user/.local/lib/python3.9/site-packages (from ipywidgets<8,>=7.6.0->ipylab) (5.1.1)\n",
      "Requirement already satisfied: ipykernel>=4.5.1 in /home/sagemaker-user/.local/lib/python3.9/site-packages (from ipywidgets<8,>=7.6.0->ipylab) (6.9.1)\n",
      "Requirement already satisfied: ipython-genutils~=0.2.0 in /home/sagemaker-user/.local/lib/python3.9/site-packages (from ipywidgets<8,>=7.6.0->ipylab) (0.2.0)\n",
      "Requirement already satisfied: matplotlib-inline<0.2.0,>=0.1.0 in /home/sagemaker-user/.local/lib/python3.9/site-packages (from ipykernel>=4.5.1->ipywidgets<8,>=7.6.0->ipylab) (0.1.3)\n",
      "Requirement already satisfied: tornado<7.0,>=4.2 in /home/sagemaker-user/.local/lib/python3.9/site-packages (from ipykernel>=4.5.1->ipywidgets<8,>=7.6.0->ipylab) (6.1)\n",
      "Requirement already satisfied: nest-asyncio in /home/sagemaker-user/.local/lib/python3.9/site-packages (from ipykernel>=4.5.1->ipywidgets<8,>=7.6.0->ipylab) (1.5.4)\n",
      "Requirement already satisfied: jupyter-client<8.0 in /home/sagemaker-user/.local/lib/python3.9/site-packages (from ipykernel>=4.5.1->ipywidgets<8,>=7.6.0->ipylab) (7.1.2)\n",
      "Requirement already satisfied: debugpy<2.0,>=1.0.0 in /home/sagemaker-user/.local/lib/python3.9/site-packages (from ipykernel>=4.5.1->ipywidgets<8,>=7.6.0->ipylab) (1.5.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /home/sagemaker-user/.local/lib/python3.9/site-packages (from ipython>=4.0.0->ipywidgets<8,>=7.6.0->ipylab) (0.18.1)\n",
      "Requirement already satisfied: black in /home/sagemaker-user/.local/lib/python3.9/site-packages (from ipython>=4.0.0->ipywidgets<8,>=7.6.0->ipylab) (22.1.0)\n",
      "Requirement already satisfied: pexpect>4.3 in /home/sagemaker-user/.local/lib/python3.9/site-packages (from ipython>=4.0.0->ipywidgets<8,>=7.6.0->ipylab) (4.8.0)\n",
      "Requirement already satisfied: decorator in /home/sagemaker-user/.local/lib/python3.9/site-packages (from ipython>=4.0.0->ipywidgets<8,>=7.6.0->ipylab) (5.1.1)\n",
      "Requirement already satisfied: stack-data in /home/sagemaker-user/.local/lib/python3.9/site-packages (from ipython>=4.0.0->ipywidgets<8,>=7.6.0->ipylab) (0.2.0)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /home/sagemaker-user/.local/lib/python3.9/site-packages (from ipython>=4.0.0->ipywidgets<8,>=7.6.0->ipylab) (3.0.28)\n",
      "Requirement already satisfied: setuptools>=18.5 in /opt/conda/envs/kernel-env/lib/python3.9/site-packages (from ipython>=4.0.0->ipywidgets<8,>=7.6.0->ipylab) (60.9.3)\n",
      "Requirement already satisfied: backcall in /home/sagemaker-user/.local/lib/python3.9/site-packages (from ipython>=4.0.0->ipywidgets<8,>=7.6.0->ipylab) (0.2.0)\n",
      "Requirement already satisfied: pickleshare in /home/sagemaker-user/.local/lib/python3.9/site-packages (from ipython>=4.0.0->ipywidgets<8,>=7.6.0->ipylab) (0.7.5)\n",
      "Requirement already satisfied: pygments in /home/sagemaker-user/.local/lib/python3.9/site-packages (from ipython>=4.0.0->ipywidgets<8,>=7.6.0->ipylab) (2.11.2)\n",
      "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /home/sagemaker-user/.local/lib/python3.9/site-packages (from nbformat>=4.2.0->ipywidgets<8,>=7.6.0->ipylab) (4.4.0)\n",
      "Requirement already satisfied: jupyter-core in /home/sagemaker-user/.local/lib/python3.9/site-packages (from nbformat>=4.2.0->ipywidgets<8,>=7.6.0->ipylab) (4.9.2)\n",
      "Requirement already satisfied: notebook>=4.4.1 in /home/sagemaker-user/.local/lib/python3.9/site-packages (from widgetsnbextension~=3.5.0->ipywidgets<8,>=7.6.0->ipylab) (6.4.8)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /home/sagemaker-user/.local/lib/python3.9/site-packages (from jedi>=0.16->ipython>=4.0.0->ipywidgets<8,>=7.6.0->ipylab) (0.8.3)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /home/sagemaker-user/.local/lib/python3.9/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets<8,>=7.6.0->ipylab) (0.18.1)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /home/sagemaker-user/.local/lib/python3.9/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets<8,>=7.6.0->ipylab) (21.4.0)\n",
      "Requirement already satisfied: entrypoints in /home/sagemaker-user/.local/lib/python3.9/site-packages (from jupyter-client<8.0->ipykernel>=4.5.1->ipywidgets<8,>=7.6.0->ipylab) (0.4)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /home/sagemaker-user/.local/lib/python3.9/site-packages (from jupyter-client<8.0->ipykernel>=4.5.1->ipywidgets<8,>=7.6.0->ipylab) (2.8.2)\n",
      "Requirement already satisfied: pyzmq>=13 in /home/sagemaker-user/.local/lib/python3.9/site-packages (from jupyter-client<8.0->ipykernel>=4.5.1->ipywidgets<8,>=7.6.0->ipylab) (22.3.0)\n",
      "Requirement already satisfied: argon2-cffi in /home/sagemaker-user/.local/lib/python3.9/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<8,>=7.6.0->ipylab) (21.3.0)\n",
      "Requirement already satisfied: nbconvert in /home/sagemaker-user/.local/lib/python3.9/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<8,>=7.6.0->ipylab) (6.4.2)\n",
      "Requirement already satisfied: Send2Trash>=1.8.0 in /home/sagemaker-user/.local/lib/python3.9/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<8,>=7.6.0->ipylab) (1.8.0)\n",
      "Requirement already satisfied: prometheus-client in /home/sagemaker-user/.local/lib/python3.9/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<8,>=7.6.0->ipylab) (0.13.1)\n",
      "Requirement already satisfied: jinja2 in /home/sagemaker-user/.local/lib/python3.9/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<8,>=7.6.0->ipylab) (3.0.3)\n",
      "Requirement already satisfied: terminado>=0.8.3 in /home/sagemaker-user/.local/lib/python3.9/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<8,>=7.6.0->ipylab) (0.13.1)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /home/sagemaker-user/.local/lib/python3.9/site-packages (from pexpect>4.3->ipython>=4.0.0->ipywidgets<8,>=7.6.0->ipylab) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /home/sagemaker-user/.local/lib/python3.9/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=4.0.0->ipywidgets<8,>=7.6.0->ipylab) (0.2.5)\n",
      "Requirement already satisfied: platformdirs>=2 in /home/sagemaker-user/.local/lib/python3.9/site-packages (from black->ipython>=4.0.0->ipywidgets<8,>=7.6.0->ipylab) (2.5.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.4.3 in /home/sagemaker-user/.local/lib/python3.9/site-packages (from black->ipython>=4.0.0->ipywidgets<8,>=7.6.0->ipylab) (0.4.3)\n",
      "Requirement already satisfied: pathspec>=0.9.0 in /home/sagemaker-user/.local/lib/python3.9/site-packages (from black->ipython>=4.0.0->ipywidgets<8,>=7.6.0->ipylab) (0.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.10.0.0 in /home/sagemaker-user/.local/lib/python3.9/site-packages (from black->ipython>=4.0.0->ipywidgets<8,>=7.6.0->ipylab) (4.1.1)\n",
      "Requirement already satisfied: click>=8.0.0 in /home/sagemaker-user/.local/lib/python3.9/site-packages (from black->ipython>=4.0.0->ipywidgets<8,>=7.6.0->ipylab) (8.0.4)\n",
      "Requirement already satisfied: tomli>=1.1.0 in /home/sagemaker-user/.local/lib/python3.9/site-packages (from black->ipython>=4.0.0->ipywidgets<8,>=7.6.0->ipylab) (2.0.1)\n",
      "Requirement already satisfied: executing in /home/sagemaker-user/.local/lib/python3.9/site-packages (from stack-data->ipython>=4.0.0->ipywidgets<8,>=7.6.0->ipylab) (0.8.2)\n",
      "Requirement already satisfied: pure-eval in /home/sagemaker-user/.local/lib/python3.9/site-packages (from stack-data->ipython>=4.0.0->ipywidgets<8,>=7.6.0->ipylab) (0.2.2)\n",
      "Requirement already satisfied: asttokens in /home/sagemaker-user/.local/lib/python3.9/site-packages (from stack-data->ipython>=4.0.0->ipywidgets<8,>=7.6.0->ipylab) (2.0.5)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/envs/kernel-env/lib/python3.9/site-packages (from python-dateutil>=2.1->jupyter-client<8.0->ipykernel>=4.5.1->ipywidgets<8,>=7.6.0->ipylab) (1.16.0)\n",
      "Requirement already satisfied: argon2-cffi-bindings in /home/sagemaker-user/.local/lib/python3.9/site-packages (from argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<8,>=7.6.0->ipylab) (21.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/sagemaker-user/.local/lib/python3.9/site-packages (from jinja2->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<8,>=7.6.0->ipylab) (2.1.0)\n",
      "Requirement already satisfied: defusedxml in /home/sagemaker-user/.local/lib/python3.9/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<8,>=7.6.0->ipylab) (0.7.1)\n",
      "Requirement already satisfied: jupyterlab-pygments in /home/sagemaker-user/.local/lib/python3.9/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<8,>=7.6.0->ipylab) (0.1.2)\n",
      "Requirement already satisfied: bleach in /home/sagemaker-user/.local/lib/python3.9/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<8,>=7.6.0->ipylab) (4.1.0)\n",
      "Requirement already satisfied: mistune<2,>=0.8.1 in /home/sagemaker-user/.local/lib/python3.9/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<8,>=7.6.0->ipylab) (0.8.4)\n",
      "Requirement already satisfied: nbclient<0.6.0,>=0.5.0 in /home/sagemaker-user/.local/lib/python3.9/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<8,>=7.6.0->ipylab) (0.5.11)\n",
      "Requirement already satisfied: testpath in /home/sagemaker-user/.local/lib/python3.9/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<8,>=7.6.0->ipylab) (0.5.0)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /home/sagemaker-user/.local/lib/python3.9/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<8,>=7.6.0->ipylab) (1.5.0)\n",
      "Requirement already satisfied: cffi>=1.0.1 in /opt/conda/envs/kernel-env/lib/python3.9/site-packages (from argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<8,>=7.6.0->ipylab) (1.14.6)\n",
      "Requirement already satisfied: packaging in /home/sagemaker-user/.local/lib/python3.9/site-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<8,>=7.6.0->ipylab) (21.3)\n",
      "Requirement already satisfied: webencodings in /home/sagemaker-user/.local/lib/python3.9/site-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<8,>=7.6.0->ipylab) (0.5.1)\n",
      "Requirement already satisfied: pycparser in /opt/conda/envs/kernel-env/lib/python3.9/site-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<8,>=7.6.0->ipylab) (2.21)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/sagemaker-user/.local/lib/python3.9/site-packages (from packaging->bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<8,>=7.6.0->ipylab) (3.0.7)\n",
      "Installing collected packages: ipylab\n",
      "Successfully installed ipylab-0.5.2\n"
     ]
    }
   ],
   "source": [
    "!pip install ipylab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- # click on the button below to open reference:\n",
    "\n",
    "<button data-commandlinker-command=\"help:open\" data-commandlinker-args='{\"url\": \"https://jupyter.org/documentation\", \"text\": \"Jupyter Reference\"}'>Open Jupyter Documentation</button> -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from ipylab import JupyterFrontEnd\n",
    "\n",
    "# # https://github.com/jtpio/ipylab\n",
    "# # https://github.com/dask/dask-labextension\n",
    "# app = JupyterFrontEnd()\n",
    "# app.commands.execute('help:open', args={\"url\": \"https://jupyter.org/documentation\", \"text\": \"Jupyter Reference\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class MockParamMeta:\n",
    "    instance_type: type\n",
    "\n",
    "\n",
    "assert [\"--str_param\", \"self.str_param.__str__()\"] == format_job_arguments(\n",
    "    {\"str_param\": MockParamMeta(str)}\n",
    ")\n",
    "assert [\n",
    "    \"--int_param\",\n",
    "    \"str(self.int_param.__int__())\",\n",
    "    \"--float_param\",\n",
    "    \"str(self.float_param.__float__())\",\n",
    "] == format_job_arguments(\n",
    "    {\"int_param\": MockParamMeta(int), \"float_param\": MockParamMeta(float)}\n",
    ")\n",
    "assert [\"--int_param\", \"str(self.int_param.__int__())\"] == format_job_arguments(\n",
    "    {\"int_param\": MockParamMeta(int)}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "def format_arg(arg, param_meta):\n",
    "    if arg in param_meta and not param_meta[arg].has_metaflow_param:\n",
    "        result = arg\n",
    "    else:\n",
    "        result = \"self.\" + arg\n",
    "    return result\n",
    "\n",
    "\n",
    "def write_steps(\n",
    "    fq_module_name, flow_file, steps, param_meta, single_indent, track_experiment\n",
    "):\n",
    "    param_names = list(param_meta.keys())\n",
    "    proc_flow_scope = []\n",
    "    train_flow_scope = []\n",
    "    outputs = {}\n",
    "\n",
    "    for i, step in enumerate(steps):\n",
    "        return_vars = get_return_var_names(step)\n",
    "        step_vars = extract_step_vars(\n",
    "            step, param_names, proc_flow_scope, train_flow_scope\n",
    "        )\n",
    "\n",
    "        flow_file.write(f\"{single_indent}def {step.name}(self):\\n\")\n",
    "        if step.docstring:\n",
    "            flow_file.write(f\"{indent_multiline(step.docstring, 2)}\\n\")\n",
    "        # Processing step\n",
    "        if is_processing_step(step):\n",
    "            # print(f\"Step: {step.name}\")\n",
    "            # print(step_vars)\n",
    "            write_script_processor(flow_file)\n",
    "\n",
    "            flow_file.write(\"\\n\")\n",
    "            flow_file.write(\n",
    "                f\"{single_indent}{single_indent}{step.name}_step = ProcessingStep(\\n\"\n",
    "            )\n",
    "            flow_file.write(\n",
    "                f'{single_indent}{single_indent}{single_indent}name = \"{step.name}\",\\n'\n",
    "            )\n",
    "            flow_file.write(\n",
    "                f\"{single_indent}{single_indent}{single_indent}processor = script_processor,\\n\"\n",
    "            )\n",
    "            flow_file.write(\n",
    "                f\"{single_indent}{single_indent}{single_indent}code = \\\"{fq_module_name.split('.')[-1]}_{step.name}.py\\\",\\n\"\n",
    "            )\n",
    "            if len(step_vars) > 0:\n",
    "                step_param_meta = {k: param_meta[k] for k in step_vars[\"step_input\"]}\n",
    "                if len(step_param_meta) > 0:\n",
    "                    # Job Args\n",
    "                    job_args = format_job_arguments(step_param_meta)\n",
    "                    flow_file.write(\n",
    "                        f\"{single_indent}{single_indent}{single_indent}job_arguments=[\\n\"\n",
    "                    )\n",
    "                    job_arg_pairs = zip(job_args[::2], job_args[1::2])\n",
    "                    for job_arg_pair in job_arg_pairs:\n",
    "                        flow_file.write(\n",
    "                            f'{single_indent}{single_indent}{single_indent}{single_indent}\"{job_arg_pair[0]}\", {job_arg_pair[1]},\\n'\n",
    "                        )\n",
    "                    flow_file.write(\n",
    "                        f\"{single_indent}{single_indent}{single_indent}],\\n\"\n",
    "                    )\n",
    "\n",
    "                # ProcInputs\n",
    "                if (\n",
    "                    len(step_vars[\"step_proc_vars\"]) > 0\n",
    "                    or len(step_vars[\"step_train_vars\"]) > 0\n",
    "                ):\n",
    "                    flow_file.write(\n",
    "                        f\"{single_indent}{single_indent}{single_indent}inputs = [\\n\"\n",
    "                    )\n",
    "                    flow_file.write(\n",
    "                        \"\\n\".join(\n",
    "                            [\n",
    "                                f'{single_indent}{single_indent}{single_indent}{single_indent}ProcessingInput(source=self.{outputs[cv]}, destination=\"/opt/ml/processing/{cv}\"),\\n'\n",
    "                                for cv in step_vars[\"step_train_vars\"]\n",
    "                                + step_vars[\"step_proc_vars\"]\n",
    "                            ]\n",
    "                        )\n",
    "                    )\n",
    "                    flow_file.write(\n",
    "                        f\"{single_indent}{single_indent}{single_indent}],\\n\"\n",
    "                    )\n",
    "\n",
    "                # ProcOutputs\n",
    "                proc_outs = {\n",
    "                    (\n",
    "                        v,\n",
    "                        f'{step.name}_step.properties.ProcessingOutputConfig.Outputs[\"{v}\"].S3Output.S3Uri',\n",
    "                    )\n",
    "                    for v in return_vars\n",
    "                }\n",
    "                outputs.update(proc_outs)\n",
    "                if len(proc_outs) > 0:\n",
    "                    flow_file.write(\n",
    "                        f\"{single_indent}{single_indent}{single_indent}outputs = [\\n\"\n",
    "                    )\n",
    "                    flow_file.write(\n",
    "                        \"\\n\".join(\n",
    "                            [\n",
    "                                f'{single_indent}{single_indent}{single_indent}{single_indent}ProcessingOutput(output_name=\"{v}\", source=\"/opt/ml/processing/{v}\"),'\n",
    "                                for v in [x[0] for x in proc_outs]\n",
    "                            ]\n",
    "                        )\n",
    "                        + \"\\n\"\n",
    "                    )\n",
    "                    flow_file.write(f\"{single_indent}{single_indent}{single_indent}]\\n\")\n",
    "\n",
    "            flow_file.write(f\"{single_indent}{single_indent})\\n\")\n",
    "            # print(f'Processing Flow scope extended by: {return_vars}')\n",
    "            proc_flow_scope.extend(return_vars)\n",
    "        elif is_train_step(step):\n",
    "            if len(step_vars) > 0:\n",
    "                # if len(step.args) > 0:\n",
    "                #    print(f\"Step Args: {step.args}\")\n",
    "                # if len(step_vars['step_input']) > 0:\n",
    "                #    print(f\"Hyperparameters: {step_vars['step_input']}\")\n",
    "                # print(f\"Args that are params: {input_vars}\")\n",
    "                # print(f\"Args that are in flow scope: {created_vars}\")\n",
    "                #\n",
    "                # if len(step_vars['step_proc_vars']) > 0:\n",
    "                #    print(f\"TrainingInputs: {[outputs[cv] for cv in step_vars['step_proc_vars']]}\")\n",
    "                train_outs = {\n",
    "                    (v, f\"{step.name}_step.properties.ModelArtifacts.S3ModelArtifacts\")\n",
    "                    for v in return_vars\n",
    "                }\n",
    "                outputs.update(train_outs)\n",
    "                train_flow_scope.extend(return_vars)\n",
    "\n",
    "                flow_file.write(f\"{single_indent}metrics_regex = None\\n\")\n",
    "                flow_file.write(f\"{single_indent}if 'metric_names' in self.__dict__:\\n\")\n",
    "                flow_file.write(\n",
    "                    f\"{single_indent}{single_indent}metrics = self.metric_names.split(\",\n",
    "                    \")\\n\",\n",
    "                )\n",
    "                flow_file.write(\n",
    "                    f'{single_indent}{single_indent}metrics_regex = [{{\"Name\": m, \"Regex\": f\"{m}=(.*?);\"}} for m in metrics]\\n'\n",
    "                )\n",
    "                flow_file.write(f\"\\n\")\n",
    "                flow_file.write(\n",
    "                    f\"{single_indent}{single_indent}estimator = Estimator(\\n\"\n",
    "                )\n",
    "                flow_file.write(\n",
    "                    f'{single_indent}{single_indent}{single_indent}image_uri = \"141502667606.dkr.ecr.eu-west-1.amazonaws.com/sagemaker-scikit-learn:0.23-1-cpu-py3\",'\n",
    "                )\n",
    "                flow_file.write(\n",
    "                    f'{single_indent}{single_indent}{single_indent}entry_point=\"{test_clustering_fit}\",\\n'\n",
    "                )\n",
    "                flow_file.write(\n",
    "                    f'{single_indent}{single_indent}{single_indent}hyperparameters={{\"workers\": str(self.workers.__int__())}},\\n'\n",
    "                )\n",
    "                flow_file.write(\n",
    "                    f'{single_indent}{single_indent}{single_indent}instance_type=\"ml.m5.xlarge\",\\n'\n",
    "                )\n",
    "                flow_file.write(\n",
    "                    f\"{single_indent}{single_indent}{single_indent}instance_count=1,\\n\"\n",
    "                )\n",
    "                flow_file.write(\n",
    "                    f\"{single_indent}{single_indent}{single_indent}output_path=\\\"s3://{{os.environ['SCIFLOW_BUCKET']}}/test_clustering/training-job-output\\\",\\n\"\n",
    "                )\n",
    "                flow_file.write(\n",
    "                    f\"{single_indent}{single_indent}{single_indent}base_job_name=f'processing-job/{__file__}',\\n\"\n",
    "                )\n",
    "                flow_file.write(\n",
    "                    f\"{single_indent}{single_indent}{single_indent}sagemaker_session = self.sagemaker_session,\\n\"\n",
    "                )\n",
    "                flow_file.write(\n",
    "                    f\"{single_indent}{single_indent}{single_indent}role = self.role,\\n\"\n",
    "                )\n",
    "                flow_file.write(\n",
    "                    f\"{single_indent}{single_indent}{single_indent}metric_definitions=metrics_regex,\\n\"\n",
    "                )\n",
    "                flow_file.write(\n",
    "                    f\"{single_indent}{single_indent}{single_indent}enable_sagemaker_metrics=True,\\n\"\n",
    "                )\n",
    "                flow_file.write(f\"{single_indent}{single_indent})\\n\")\n",
    "                flow_file.write(\"\\n\")\n",
    "\n",
    "                flow_file.write(\"\\n\")\n",
    "                flow_file.write(\n",
    "                    f\"{single_indent}{single_indent}{step.name}_step = TrainingStep(\\n\"\n",
    "                )\n",
    "                flow_file.write(\n",
    "                    f'{single_indent}{single_indent}{single_indent}name=\"{step.name}\",\\n'\n",
    "                )\n",
    "                flow_file.write(\n",
    "                    f\"{single_indent}{single_indent}{single_indent}estimator=estimator,\\n\"\n",
    "                )\n",
    "                flow_file.write(\n",
    "                    f\"{single_indent}{single_indent}{single_indent}inputs={{\\n\"\n",
    "                )\n",
    "                flow_file.write(\n",
    "                    f'{single_indent}{single_indent}{single_indent}{single_indent}\"documents\": TrainingInput(\\n'\n",
    "                )\n",
    "                flow_file.write(\n",
    "                    f\"{single_indent}{single_indent}{single_indent}{single_indent}{single_indent}s3_data=self.preprocess_step.properties.ProcessingOutputConfig.Outputs[\\n\"\n",
    "                )\n",
    "                flow_file.write(\n",
    "                    f'{single_indent}{single_indent}{single_indent}{single_indent}{single_indent}{single_indent}\"documents\"\\n'\n",
    "                )\n",
    "                flow_file.write(\n",
    "                    f\"{single_indent}{single_indent}{single_indent}{single_indent}{single_indent}].S3Output.S3Uri,\\n\"\n",
    "                )\n",
    "                flow_file.write(\n",
    "                    f'{single_indent}{single_indent}{single_indent}{single_indent}{single_indent}content_type=\"text/csv\",\\n'\n",
    "                )\n",
    "                flow_file.write(\n",
    "                    f\"{single_indent}{single_indent}{single_indent}{single_indent})\\n\"\n",
    "                )\n",
    "                flow_file.write(f\"{single_indent}{single_indent}{single_indent}}}\\n\")\n",
    "                flow_file.write(f\"{single_indent}{single_indent})\\n\")\n",
    "\n",
    "        flow_file.write(\n",
    "            f\"{single_indent}{single_indent}self.{step.name}_step = {step.name}_step\\n\"\n",
    "        )\n",
    "        flow_file.write(f\"{single_indent}{single_indent}return {step.name}_step\\n\")\n",
    "        flow_file.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "def create_sm_dag(steps, param_meta):\n",
    "    param_names = list(param_meta.keys())\n",
    "    proc_flow_scope = []\n",
    "    train_flow_scope = []\n",
    "    outputs = {}\n",
    "    for step in steps:\n",
    "        return_vars = get_return_var_names(step)\n",
    "        step_vars = extract_step_vars(\n",
    "            step, param_names, proc_flow_scope, train_flow_scope\n",
    "        )\n",
    "        if is_processing_step(step):\n",
    "            print(f\"Step: {step.name}\")\n",
    "            # if len(step.args) > 0:\n",
    "            #    print(f\"Step Args: {step.args}\")\n",
    "            if len(step_vars) > 0:\n",
    "                if len(step_vars[\"step_input\"]) > 0:\n",
    "                    step_param_meta = {\n",
    "                        k: param_meta[k] for k in step_vars[\"step_input\"]\n",
    "                    }\n",
    "                    print(f\"job_arguments={format_job_arguments(step_param_meta)}\")\n",
    "                # print(f\"Args that are in flow scope: {created_vars}\")\n",
    "                if (\n",
    "                    len(step_vars[\"step_proc_vars\"]) > 0\n",
    "                    or len(step_vars[\"step_train_vars\"]) > 0\n",
    "                ):\n",
    "                    print(\n",
    "                        f\"ProcessingInputs: {[outputs[cv] for cv in step_vars['step_train_vars'] + step_vars['step_proc_vars']]}\"\n",
    "                    )\n",
    "                proc_outs = {\n",
    "                    (\n",
    "                        v,\n",
    "                        f'step_{step.name}.properties.ProcessingOutputConfig.Outputs[\"{v}\"].S3Output.S3Uri',\n",
    "                    )\n",
    "                    for v in return_vars\n",
    "                }\n",
    "                outputs.update(proc_outs)\n",
    "                if len(proc_outs) > 0:\n",
    "                    print(f\"ProcessingOutputs: {proc_outs}\")\n",
    "                print(f\"Processing Flow scope extended by: {return_vars}\")\n",
    "                proc_flow_scope.extend(return_vars)\n",
    "        elif is_train_step(step):\n",
    "            print(f\"Step: {step.name}\")\n",
    "            if len(step_vars) > 0:\n",
    "                # if len(step.args) > 0:\n",
    "                #    print(f\"Step Args: {step.args}\")\n",
    "                if len(step_vars[\"step_input\"]) > 0:\n",
    "                    print(f\"Hyperparameters: {step_vars['step_input']}\")\n",
    "                # print(f\"Args that are params: {input_vars}\")\n",
    "                # print(f\"Args that are in flow scope: {created_vars}\")\n",
    "                if len(step_vars[\"step_proc_vars\"]) > 0:\n",
    "                    print(\n",
    "                        f\"TrainingInputs: {[outputs[cv] for cv in step_vars['step_proc_vars']]}\"\n",
    "                    )\n",
    "                train_outs = {\n",
    "                    (v, f\"step_{step.name}.properties.ModelArtifacts.S3ModelArtifacts\")\n",
    "                    for v in return_vars\n",
    "                }\n",
    "                outputs.update(train_outs)\n",
    "                print(f\"Training Flow scope extended by: {return_vars}\")\n",
    "                train_flow_scope.extend(return_vars)\n",
    "        print(\"\\n\")\n",
    "\n",
    "    # ProcessingInput comes from params or from preceding ste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "def write_track_capture(flow_file):\n",
    "    flow_file.write(\n",
    "        f\"\"\"\n",
    "        for key in results.keys():\n",
    "            if key in self.__dict__:\n",
    "                self.__dict__[key] = self.__dict__[key] + results[key]\n",
    "            else:\n",
    "                self.__dict__[key] = results[key]\n",
    "\n",
    "\"\"\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "def get_return_var_names(step):\n",
    "    results_index = step.code.find(\"results =\")\n",
    "    if results_index == -1:\n",
    "        return []\n",
    "    return [\n",
    "        l.split(\":\")[1].strip(\", \\}\")\n",
    "        for l in step.code[results_index:].split(\"\\n\")\n",
    "        if l.strip().find(\":\") > -1\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = params_as_dict(nb_path)\n",
    "param_meta = extract_param_meta(fq_module_name, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_names = list(param_meta.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create_sm_dag(steps, param_meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted test_clustering.ipynb to TestClusteringPipeline in: test_clustering.py\n"
     ]
    }
   ],
   "source": [
    "# nb_to_sagemaker_pipeline(nb_path, flow_path, silent=False, track_experiment=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add Additional Code to Processing Jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Path('/home/sagemaker-user/git/sciflow')"
      ]
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from sciflow.utils import lib_path\n",
    "# lib_path()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate if it doesn't exist\n",
    "# Copy to flow directory"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.m5.xlarge",
  "kernelspec": {
   "display_name": "sciflow (sciflow/3)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:eu-west-1:368653567616:image-version/sciflow/3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
