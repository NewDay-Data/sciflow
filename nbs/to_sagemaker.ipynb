{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp to_sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Iterable\n",
    "\n",
    "from nbdev.export import find_default_export, get_config, read_nb\n",
    "\n",
    "from sciflow.data_handler import extract_param_meta\n",
    "from sciflow.params import params_as_dict\n",
    "from sciflow.parse_module import FuncDetails, extract_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sciflow.to_metaflow import (\n",
    "    extract_module_only,\n",
    "    get_flow_path,\n",
    "    titleize,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sciflow Notebook to Sagemaker Pipeline\n",
    "\n",
    "> Converts from a `sciflow` format notebook to a `sagemaker` pipeline. \n",
    "\n",
    "Currently Supported features:\n",
    "\n",
    "* Linear/sequential DAGs\n",
    "* Simple `Parameters`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_path = Path(os.path.join(\"test\", \"test_clustering.ipynb\"))\n",
    "nb = read_nb(nb_path)\n",
    "module_name = find_default_export(nb[\"cells\"]).replace(\".\", \"/\")\n",
    "test_module = os.path.join(get_config().path(\"lib_path\"), f\"{module_name}.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "flow_path = get_flow_path(nb_path, flow_provider=\"sagemaker\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Path('/home/sagemaker-user/git/sciflow/nbs/test/flows/sagemaker/test_clustering.py')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flow_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = extract_steps(test_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "def nb_to_sagemaker_pipeline(\n",
    "    nb_path: Path, flow_path: Path, silent=True, track_experiment=True\n",
    "):\n",
    "    nb = read_nb(nb_path)\n",
    "    lib_name = get_config().get(\"lib_name\")\n",
    "    module_name = find_default_export(nb[\"cells\"])\n",
    "    if not module_name:\n",
    "        return\n",
    "    module_name = module_name\n",
    "    path_sep_module_name = module_name.replace(\".\", \"/\")\n",
    "    nb_name = os.path.basename(nb_path)\n",
    "    exported_module = os.path.join(\n",
    "        get_config().path(\"lib_path\"), f\"{path_sep_module_name}.py\"\n",
    "    )\n",
    "    steps = extract_steps(exported_module)\n",
    "    if len(steps) == 0:\n",
    "        print(\"Skipping sagemaker conversion - not steps found\")\n",
    "        return\n",
    "    params = params_as_dict(nb_path)\n",
    "    if len(params) == 0:\n",
    "        print(f\"No params cell found for: {os.path.basename(nb_path)}\")\n",
    "    pipeline_class_name = f\"{titleize(extract_module_only(module_name))}Pipeline\"\n",
    "    write_pipeline_to_files(\n",
    "        flow_path,\n",
    "        pipeline_class_name,\n",
    "        lib_name,\n",
    "        module_name,\n",
    "        steps,\n",
    "        params,\n",
    "        track_experiment,\n",
    "    )\n",
    "    if not silent:\n",
    "        print(\n",
    "            f\"Converted {nb_name} to {pipeline_class_name} in: {os.path.basename(flow_path)}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "def is_train_step(step):\n",
    "    return any(step.name.startswith(prefix) for prefix in ('fit', 'train'))\n",
    "\n",
    "def is_processing_step(step):\n",
    "    return not is_train_step(step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_step = FuncDetails('fit_something', docstring=None, args=None, \n",
    "                            has_return=False, return_stmt=None, code='')\n",
    "proc_step = FuncDetails('blabla', docstring=None, args=None, \n",
    "                            has_return=False, return_stmt=None, code='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(not is_processing_step(training_step))\n",
    "assert(is_train_step(training_step))\n",
    "assert(is_processing_step(proc_step))\n",
    "assert(not is_train_step(proc_step))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "lib_name = get_config().get(\"lib_name\")\n",
    "module_name = find_default_export(nb[\"cells\"])\n",
    "fq_module_name = f\"{lib_name}.{module_name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  export\n",
    "\n",
    "\n",
    "def write_pipeline_to_files(\n",
    "    flow_path: Path,\n",
    "    pipeline_class_name: str,\n",
    "    lib_name: str,\n",
    "    module_name: str,\n",
    "    steps: Iterable[FuncDetails],\n",
    "    params: dict,\n",
    "    track_experiment: bool,\n",
    "):\n",
    "    if not os.path.exists(flow_path.parent):\n",
    "        os.mkdir(flow_path.parent)\n",
    "    fq_module_name = f\"{lib_name}.{module_name}\"\n",
    "    param_meta = extract_param_meta(fq_module_name, params)\n",
    "    with open(flow_path, \"w\") as flow_file:\n",
    "        flow_file.write(\"#!/usr/bin/env python\\n\")\n",
    "        flow_file.write(\"# coding=utf-8\\n\")\n",
    "        flow_file.write(\"# SCIFLOW GENERATED FILE - EDIT COMPANION NOTEBOOK\\n\")\n",
    "\n",
    "        flow_file.write(\"import os\\n\")\n",
    "\n",
    "        flow_file.write(\"import sagemaker\\n\")\n",
    "        flow_file.write(\"from sagemaker.session import Session\\n\")\n",
    "        flow_file.write(\"from sagemaker.workflow.pipeline import Pipeline\\n\")\n",
    "\n",
    "        has_train_step = any(\n",
    "            [is_train_step(s) for s in steps]\n",
    "        )\n",
    "        has_processing_step = sum(\n",
    "            [is_processing_step(s) for s in steps]\n",
    "        ) != len(steps)\n",
    "\n",
    "        if has_train_step and has_processing_step:\n",
    "            flow_file.write(\n",
    "                \"from sagemaker.workflow.steps import ProcessingStep, TrainingStep\\n\"\n",
    "            )\n",
    "        if has_processing_step:\n",
    "            flow_file.write(\n",
    "                \"from sagemaker.processing import ScriptProcessor, ProcessingInput, ProcessingOutput\\n\"\n",
    "            )\n",
    "            flow_file.write(\"from sagemaker.workflow.pipeline import Pipeline\\n\")\n",
    "        if has_train_step:\n",
    "            flow_file.write(\"from sagemaker.inputs import TrainingInput\\n\")\n",
    "            flow_file.write(\"from sagemaker.estimator import Estimator\\n\")\n",
    "\n",
    "        has_sm_param = any((p.has_sagemaker_param for p in param_meta.values()))\n",
    "        if has_sm_param:\n",
    "            instance_types = [p.instance_type for p in param_meta.values()]\n",
    "            sm_params_import = \"from sagemaker.workflow.parameters import \"\n",
    "            if int in instance_types:\n",
    "                sm_params_import += \"ParameterInteger\"\n",
    "                if float in instance_types or str in instance_types:\n",
    "                    sm_params_import += \", \"\n",
    "            if float in instance_types:\n",
    "                sm_params_import += \"ParameterFloat\"\n",
    "                if str in instance_types:\n",
    "                    sm_params_import += \", \"\n",
    "            if str in instance_types:\n",
    "                sm_params_import += \"ParameterString\"\n",
    "\n",
    "            flow_file.write(sm_params_import + \"\\n\")\n",
    "\n",
    "        flow_file.write(\"\\n\")\n",
    "        flow_file.write(\n",
    "            f\"from {fq_module_name} import {', '.join([s.name for s in steps])}\\n\"\n",
    "        )\n",
    "        if len(params) > 0:\n",
    "            flow_file.write(\n",
    "                f\"from {fq_module_name} import {', '.join(params.keys())}\\n\"\n",
    "            )\n",
    "\n",
    "        flow_file.write(f\"\\n\\nclass {pipeline_class_name}():\\n\")\n",
    "        single_indent = \"    \"\n",
    "        write_params(flow_file, param_meta, single_indent)\n",
    "        flow_file.write(\"\\n\")\n",
    "        write_steps(\n",
    "            fq_module_name,\n",
    "            flow_file,\n",
    "            steps,\n",
    "            param_meta,\n",
    "            single_indent,\n",
    "            track_experiment,\n",
    "        )\n",
    "        flow_file.write(\"\\n\")\n",
    "\n",
    "        flow_file.write('if __name__ == \"__main__\":\\n')\n",
    "        flow_file.write(f\"{single_indent}{pipeline_class_name}()\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "def write_observers(lib_name, flow_file, module_name, bucket_name, project):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "def write_track_flow(flow_file, track_experiment):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  export\n",
    "\n",
    "\n",
    "def write_params(flow_file, param_meta, single_indent):\n",
    "    for param in param_meta.keys():\n",
    "        if param_meta[param].instance_type == int:\n",
    "            flow_file.write(\n",
    "                f\"{single_indent}{param} = ParameterInteger(name='{param}', default_value={param})\\n\"\n",
    "            )\n",
    "        elif param_meta[param].instance_type == float:\n",
    "            flow_file.write(\n",
    "                f\"{single_indent}{param} = ParameterFloat(name='{param}', default_value={param})\\n\"\n",
    "            )\n",
    "        elif param_meta[param].instance_type == str:\n",
    "            flow_file.write(\n",
    "                f\"{single_indent}{param} = ParameterString(name='{param}', default_value={param})\\n\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_script_processor(flow_file):\n",
    "    flow_file.write(\n",
    "        \"\"\"\n",
    "        script_processor = ScriptProcessor(\n",
    "                command=['python3'],\n",
    "                image_uri=\"141502667606.dkr.ecr.eu-west-1.amazonaws.com/sagemaker-scikit-learn:0.23-1-cpu-py3\",\n",
    "                role=self.role,\n",
    "                instance_count=1,\n",
    "                instance_type=\"ml.m5.xlarge\",\n",
    "                sagemaker_session=self.sagemaker_session,\n",
    "                env={'AWS_DEFAULT_REGION': self.region},\n",
    "                base_job_name=f'processing-job/{__file__}'\n",
    "        )\n",
    "        \"\"\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "def extract_step_vars(step, param_names, processing_flow_scope, train_flow_scope):\n",
    "    if len(step.args) == 0:\n",
    "        result = {}\n",
    "    else:\n",
    "        args = [x.strip() for x in step.args.split(\",\")]\n",
    "        step_input = [a for a in args if a in param_names]\n",
    "        step_proc_vars = [a for a in args if a in processing_flow_scope]\n",
    "        step_train_vars = [a for a in args if a in train_flow_scope]\n",
    "        unscoped_vars = set(args).difference(set(step_input + step_proc_vars + step_train_vars))\n",
    "        if len(unscoped_vars) > 0:\n",
    "            raise ValueError(f'Step: {step.name} depends on variable(s), \"{unscoped_vars}\", which are not in the flow scope')\n",
    "        result = {'step_input': step_input, \n",
    "                'step_proc_vars': step_proc_vars, \n",
    "                'step_train_vars': step_train_vars\n",
    "               }\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_arg_step = FuncDetails('no_arg_step', docstring=None, args='', \n",
    "                            has_return=False, return_stmt=None, code='')\n",
    "single_arg_step = FuncDetails('fit_single_arg_step', docstring=None, args=\"one_param\", \n",
    "                            has_return=False, return_stmt=None, code='')\n",
    "multi_arg_step = FuncDetails('multi_arg_step', docstring=None, args=\"one_param,proc_param\", \n",
    "                            has_return=False, return_stmt=None, code='')\n",
    "post_train_step = FuncDetails('post_train_step', docstring=None, args=\"one_param,proc_param,train_param\", \n",
    "                            has_return=False, return_stmt=None, code='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert extract_step_vars(no_arg_step, ['one_param'], ['two_param'], []) == {}\n",
    "assert extract_step_vars(no_arg_step, [], [], []) == {}\n",
    "raised = False\n",
    "try:\n",
    "    extract_step_vars(single_arg_step, [], [], []) == {}\n",
    "except:\n",
    "    raised = True\n",
    "assert raised\n",
    "assert extract_step_vars(single_arg_step, ['one_param'], ['proc_param'], []) == {'step_input': ['one_param'], \n",
    "                                                                         'step_proc_vars': [], \n",
    "                                                                         'step_train_vars': []}\n",
    "assert extract_step_vars(multi_arg_step, ['one_param'], ['proc_param'], []) == {'step_input': ['one_param'], \n",
    "                                                                         'step_proc_vars': ['proc_param'], \n",
    "                                                                         'step_train_vars': []}\n",
    "assert extract_step_vars(post_train_step, ['one_param'], ['proc_param'], ['train_param']) == {'step_input': ['one_param'], \n",
    "                                                                         'step_proc_vars': ['proc_param'], \n",
    "                                                                         'step_train_vars': ['train_param']}\n",
    "raised = False\n",
    "try:\n",
    "    assert extract_step_vars(multi_arg_step, ['one_param'], [], [])\n",
    "except:\n",
    "    raised = True\n",
    "assert raised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "def format_job_arguments(param_meta):\n",
    "    job_arg_values = [\n",
    "        f\"str(self.{p}.__int__())\" if param_meta[p].instance_type == int else \n",
    "        f\"str(self.{p}.__float__())\" if param_meta[p].instance_type == float else\n",
    "        f\"self.{p}.__str__()\" if param_meta[p].instance_type == str else\n",
    "        f\"str(self.{p})\" for p in param_meta.keys()\n",
    "    ]\n",
    "    stitched_args = list(zip([f\"--{p}\" for p in param_meta.keys()], job_arg_values))\n",
    "    flattened = [item for sublist in stitched_args for item in sublist]\n",
    "    return flattened"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sciflow.data_handler import ParamMeta\n",
    "from dataclasses import dataclass\n",
    "@dataclass\n",
    "class MockParamMeta:\n",
    "    instance_type: type\n",
    "\n",
    "assert (\n",
    "    ['--str_param', 'self.str_param.__str__()'] ==\n",
    "    format_job_arguments({\"str_param\": MockParamMeta(str)})\n",
    ")\n",
    "assert (\n",
    "    ['--int_param', \"str(self.int_param.__int__())\",\n",
    "    '--float_param', \"str(self.float_param.__float__())\"] ==\n",
    "    format_job_arguments(\n",
    "        {\n",
    "            \"int_param\": MockParamMeta(int),\n",
    "            \"float_param\": MockParamMeta(float) \n",
    "        }\n",
    "    )\n",
    ")\n",
    "assert (\n",
    "    ['--int_param', \"str(self.int_param.__int__())\"] ==\n",
    "    format_job_arguments({\"int_param\": MockParamMeta(int)})\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "def format_arg(arg, param_meta):\n",
    "    if arg in param_meta and not param_meta[arg].has_metaflow_param:\n",
    "        result = arg\n",
    "    else:\n",
    "        result = \"self.\" + arg\n",
    "    return result\n",
    "\n",
    "\n",
    "def write_steps(\n",
    "    fq_module_name, flow_file, steps, param_meta, single_indent, track_experiment\n",
    "):\n",
    "    param_names = list(param_meta.keys())\n",
    "    proc_flow_scope = []\n",
    "    train_flow_scope = []\n",
    "    outputs = {}\n",
    "    \n",
    "    for i, step in enumerate(steps):\n",
    "        return_vars = get_return_var_names(step)\n",
    "        step_vars = extract_step_vars(step, param_names, proc_flow_scope, train_flow_scope)\n",
    "            \n",
    "        flow_file.write(f\"{single_indent}def {step.name}(self):\\n\")\n",
    "        if step.docstring:\n",
    "            flow_file.write(f\"{indent_multiline(step.docstring, 2)}\\n\")\n",
    "        # Processing step\n",
    "        if is_processing_step(step):\n",
    "            write_script_processor(flow_file)\n",
    "\n",
    "            flow_file.write(\"\\n\")\n",
    "            flow_file.write(\n",
    "                f\"{single_indent}{single_indent}{step.name}_step = ProcessingStep(\\n\"\n",
    "            )\n",
    "            flow_file.write(\n",
    "                f\"{single_indent}{single_indent}{single_indent}name={step.name}\\n\"\n",
    "            )\n",
    "            flow_file.write(\n",
    "                f\"{single_indent}{single_indent}{single_indent}processor = script_processor\\n\"\n",
    "            )\n",
    "            print(f\"job_arguments={format_job_arguments(param_meta)}\")\n",
    "            flow_file.write(\n",
    "                f\"{single_indent}{single_indent}{single_indent}code = {fq_module_name}_{step.name}.py\\n\"\n",
    "            )\n",
    "            if len(step_vars['step_input']) > 0:\n",
    "                print(f\"Job arguments: {input_vars}\")\n",
    "            print(f\"Args that are in flow scope: {step_vars['step_proc_vars']}\")\n",
    "            if len(step_vars['step_proc_vars']) > 0:\n",
    "                print(f\"ProcessingInputs: {[outputs[cv] for cv in created_vars]}\")\n",
    "            proc_outs = {(v, f'step_{step.name}.properties.ProcessingOutputConfig.Outputs[\"{v}\"].S3Output.S3Uri') \n",
    "                               for v in return_vars}\n",
    "            outputs.update(proc_outs)\n",
    "            if len(proc_outs) > 0:\n",
    "                print(f\"ProcessingOutputs: {proc_outs}\")\n",
    "            flow_file.write(f\"{single_indent}{single_indent})\\n\")\n",
    "            proc_flow_scope.extend(return_vars)\n",
    "        else:\n",
    "            # Training step\n",
    "            pass\n",
    "        flow_file.write(f\"self.{step.name}_step = {step.name}_step\\n\")\n",
    "        flow_file.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "def create_sm_dag(steps, param_meta):\n",
    "    param_names = list(param_meta.keys())\n",
    "    proc_flow_scope = []\n",
    "    train_flow_scope = []\n",
    "    outputs = {}\n",
    "    for step in steps:\n",
    "        return_vars = get_return_var_names(step)\n",
    "        step_vars = extract_step_vars(step, param_names, proc_flow_scope, train_flow_scope)\n",
    "        print(step_vars)\n",
    "        if is_processing_step(step):\n",
    "            print(f\"Step: {step.name}\")\n",
    "            #if len(step.args) > 0:\n",
    "            #    print(f\"Step Args: {step.args}\")\n",
    "            if len(step_vars) > 0:\n",
    "                if len(step_vars['step_input']) > 0:\n",
    "                    print(f\"Job arguments: {step_vars['step_input']}\")\n",
    "                    step_param_meta = { k: param_meta[k] for k in step_vars['step_input'] }\n",
    "                    print(f\"job_arguments={format_job_arguments(step_param_meta)}\")\n",
    "                #print(f\"Args that are in flow scope: {created_vars}\")\n",
    "                if len(step_vars['step_proc_vars']) > 0:\n",
    "                    print(f\"ProcessingInputs: {[outputs[cv] for cv in step_vars['step_proc_vars']]}\")\n",
    "                proc_outs = {(v, f'step_{step.name}.properties.ProcessingOutputConfig.Outputs[\"{v}\"].S3Output.S3Uri') \n",
    "                                   for v in return_vars}\n",
    "                outputs.update(proc_outs)\n",
    "                if len(proc_outs) > 0:\n",
    "                    print(f\"ProcessingOutputs: {proc_outs}\")\n",
    "                print(f'Processing Flow scope extended by: {return_vars}')\n",
    "                proc_flow_scope.extend(return_vars)\n",
    "        elif is_train_step(step):\n",
    "            print(f\"Step: {step.name}\")\n",
    "            if len(step_vars) > 0:\n",
    "                #if len(step.args) > 0:\n",
    "                #    print(f\"Step Args: {step.args}\")\n",
    "                if len(step_vars['step_input']) > 0:\n",
    "                    print(f\"Hyperparameters: {step_vars['step_input']}\")\n",
    "                #print(f\"Args that are params: {input_vars}\")\n",
    "                #print(f\"Args that are in flow scope: {created_vars}\")\n",
    "                if len(step_vars['step_proc_vars']) > 0:\n",
    "                    print(f\"TrainingInputs: {[outputs[cv] for cv in step_vars['step_proc_vars']]}\")\n",
    "                train_outs = {(v, f'step_{step.name}.properties.ModelArtifacts.S3ModelArtifacts') \n",
    "                                   for v in return_vars}\n",
    "                outputs.update(train_outs)\n",
    "                print(f'Training Flow scope extended by: {return_vars}')\n",
    "                train_flow_scope.extend(return_vars)\n",
    "        print(\"\\n\")\n",
    "        \n",
    "    # ProcessingInput comes from params or from preceding ste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "def write_track_capture(flow_file):\n",
    "    flow_file.write(\n",
    "        f\"\"\"\n",
    "        for key in results.keys():\n",
    "            if key in self.__dict__:\n",
    "                self.__dict__[key] = self.__dict__[key] + results[key]\n",
    "            else:\n",
    "                self.__dict__[key] = results[key]\n",
    "\n",
    "\"\"\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "def get_return_var_names(step):\n",
    "    results_index = step.code.find(\"results =\")\n",
    "    if results_index == -1:\n",
    "        return []\n",
    "    return [\n",
    "        l.split(\":\")[1].strip(\", \\}\")\n",
    "        for l in step.code[results_index:].split(\"\\n\")\n",
    "        if l.strip().find(\":\") > -1\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = params_as_dict(nb_path)\n",
    "param_meta = extract_param_meta(fq_module_name, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_names = list(param_meta.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n",
      "Step: something\n",
      "\n",
      "\n",
      "{'step_input': ['model_level', 'min_date', 'traffic_percent'], 'step_proc_vars': [], 'step_train_vars': []}\n",
      "Step: preprocess\n",
      "Job arguments: ['model_level', 'min_date', 'traffic_percent']\n",
      "job_arguments=['--model_level', 'self.model_level.__str__()', '--min_date', 'self.min_date.__str__()', '--traffic_percent', 'str(self.traffic_percent.__int__())']\n",
      "ProcessingOutputs: {('documents', 'step_preprocess.properties.ProcessingOutputConfig.Outputs[\"documents\"].S3Output.S3Uri')}\n",
      "Processing Flow scope extended by: ['documents']\n",
      "\n",
      "\n",
      "{'step_input': ['workers'], 'step_proc_vars': ['documents'], 'step_train_vars': []}\n",
      "Step: fit\n",
      "Hyperparameters: ['workers']\n",
      "TrainingInputs: ['step_preprocess.properties.ProcessingOutputConfig.Outputs[\"documents\"].S3Output.S3Uri']\n",
      "Training Flow scope extended by: ['model']\n",
      "\n",
      "\n",
      "{'step_input': [], 'step_proc_vars': [], 'step_train_vars': ['model']}\n",
      "Step: evaluate\n",
      "ProcessingOutputs: {('artifacts', 'step_evaluate.properties.ProcessingOutputConfig.Outputs[\"artifacts\"].S3Output.S3Uri'), ('metrics', 'step_evaluate.properties.ProcessingOutputConfig.Outputs[\"metrics\"].S3Output.S3Uri'), ('word_summaries', 'step_evaluate.properties.ProcessingOutputConfig.Outputs[\"word_summaries\"].S3Output.S3Uri')}\n",
      "Processing Flow scope extended by: ['word_summaries', 'artifacts', 'metrics']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "create_sm_dag(steps, param_meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "job_arguments=['--traffic_percent', 'str(self.traffic_percent.__int__())', '--workers', 'str(self.workers.__int__())', '--model_level', 'self.model_level.__str__()', '--min_date', 'self.min_date.__str__()']\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'step_input'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[0;32mIn [258]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mnb_to_sagemaker_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnb_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflow_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msilent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrack_experiment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36mnb_to_sagemaker_pipeline\u001b[0;34m(nb_path, flow_path, silent, track_experiment)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo params cell found for: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mos\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(nb_path)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     25\u001b[0m pipeline_class_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtitleize(extract_module_only(module_name))\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mPipeline\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 26\u001b[0m \u001b[43mwrite_pipeline_to_files\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflow_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpipeline_class_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlib_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrack_experiment\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m silent:\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m     37\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConverted \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnb_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpipeline_class_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m in: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mos\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(flow_path)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     38\u001b[0m     )\n",
      "Input \u001b[0;32mIn [14]\u001b[0m, in \u001b[0;36mwrite_pipeline_to_files\u001b[0;34m(flow_path, pipeline_class_name, lib_name, module_name, steps, params, track_experiment)\u001b[0m\n\u001b[1;32m     76\u001b[0m write_params(flow_file, param_meta, single_indent)\n\u001b[1;32m     77\u001b[0m flow_file\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 78\u001b[0m \u001b[43mwrite_steps\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfq_module_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflow_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparam_meta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[43m    \u001b[49m\u001b[43msingle_indent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrack_experiment\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     86\u001b[0m flow_file\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     88\u001b[0m flow_file\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mif __name__ == \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Input \u001b[0;32mIn [238]\u001b[0m, in \u001b[0;36mwrite_steps\u001b[0;34m(fq_module_name, flow_file, steps, param_meta, single_indent, track_experiment)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjob_arguments=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mformat_job_arguments(param_meta)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     42\u001b[0m flow_file\u001b[38;5;241m.\u001b[39mwrite(\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msingle_indent\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msingle_indent\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msingle_indent\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mcode = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfq_module_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstep\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.py\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     44\u001b[0m )\n\u001b[0;32m---> 45\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[43mstep_vars\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mstep_input\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJob arguments: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_vars\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArgs that are in flow scope: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstep_vars[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstep_proc_vars\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'step_input'"
     ]
    }
   ],
   "source": [
    "nb_to_sagemaker_pipeline(nb_path, flow_path, silent=False, track_experiment=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.m5.xlarge",
  "kernelspec": {
   "display_name": "sciflow (sciflow/3)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:eu-west-1:368653567616:image-version/sciflow/3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
