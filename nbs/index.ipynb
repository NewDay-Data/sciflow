{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "description: 'This library bridges the gap between research and production for Data\n",
    "  Science.** This library takes a different approach to achieve this than many others\n",
    "  do.. the interactive notebook is the primary driver of exploration. Exploration\n",
    "  driven development is a different paradigm to most software engineering exercises.\n",
    "  `sciflow` mixes the strengths of the notebook environment: flexibility, access to\n",
    "  data and constant feedback with the strengths of production-like workflows: resilience\n",
    "  and dedicated compute.'\n",
    "output-file: index.html\n",
    "title: \"Sciflow \\U0001F52C\"\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install\n",
    "\n",
    "* `pip install sciflow`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to annotate your workflow\n",
    "\n",
    "See `examples/hello_sciflow.ipynb` for details of how to mark your notebook based workflows for conversion. Once your notebooks have been annotated correctly you can start using `SciFlow`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Initialise your project using `sciflow_init`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\> `sciflow_init`\n",
    "\n",
    "\\> `source \"~/.sciflow/env\"` # Sourcing the environment file adds the variables to the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Edit your settings.ini file\n",
    "\n",
    "You can use the settings.ini fle from `sciflow` as a base to edit and make changes to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Test your setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Converting your notebooks to Python modules then test all is working\n",
    "\n",
    "> the order may seem unusual to build the modules then test the notebooks but this is because your project notebooks will likely import from other modules in your project so you want to test on the latest versin of these modules. \n",
    "\n",
    "```console\n",
    "\n",
    "~/codedir/project: sciflow_build_lib\n",
    "~/codedir/project: nbdev_test_nbs --pause=3\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Ensure your notebooks have a consistent style\n",
    "\n",
    "```console\n",
    "~/codedir/project: sciflow_tidy\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Inspect your notebooks for any potential quality issues [Experimental]\n",
    "\n",
    "```console\n",
    "~/codedir/project: sciflow_lint\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Convert your Python Moules to Workflows\n",
    "\n",
    "```console\n",
    "~/codedir/project: sciflow_metaflow\n",
    "~/codedir/project: sciflow_sagemaker\n",
    "~/codedir/project: sciflow_check_metaflows\n",
    "~/codedir/project: sciflow_check_sagemaker_flows\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Running your workflows\n",
    "\n",
    "```console\n",
    "~/codedir/project: sciflow_run_metaflows\n",
    "~/codedir/project: sciflow_run_sagemaker_flows\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RESTRUCTURE BELOW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment Constraints\n",
    "\n",
    "`SciFlow` aim is to provide a repeatable path from idea to production without you having to deploy and manage a complex toolsuite. This is achieved by being constrained to the toolset that covers many industry Data Science users today. The assumptions we make are that you are running a notebook environment on public cloud provider and have access to lake storage files (e.g parquet) and an ability to query over these files with an odbc interface."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Motivation\n",
    "\n",
    "A popular approach to productionising research code in notebooks is to lift the workflow from notebooks to a python modules via a manual rewrite process. Sometimes one person will have the skills and ability to perform this task but mostly it will be a collaborative effort to understand and rewrite the code for more robust production ready purpose. \n",
    "\n",
    "As a team you can get to be good at this process but there is always a lag between the exploration being performed now and the ability to test that out in live environments. If the person writing the experimental approach was able, without switching hats to \"development mode\" to see their exploration validated safely by production users then this should be transformative for the impact of Data Science in your organisation. \n",
    "\n",
    "The discipline of Software Development offers many approaches to ensuring code is of high quality with consistent style and testable assertions on known data. Data Science and Machine Learning introduce new Software Development challenges that are not as well explored by the community. In Data Science behaviour is a more fundamental unit for testing than logic for instance so tests on real user data will show your model works as expected syntethic data is unlikely to achieve the same.\n",
    "\n",
    "Modern notebook environment have a common advantage in practice over many local development environments and that is that Data Scientists can write code against real data with all the quirks and anomolaies and underlying behaviour that they are actually trying to develop a solution against. We start by assuming only that there are high levels of uncertainty and that the code needs not only to work but to be well conditioned to real operating environment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Progressive Consolidation\n",
    "\n",
    "Progressive consolidation is a development idealogy for exploratory programming. You start by writing code which optimises for speed of exploration. You will mostly have scripted code cells with little or no functions and minimal code re-use. As time goes by you will know which parts of your notebook are important and you consolidate those to a higher level of quality. \n",
    "\n",
    "In `sciflow` we simplify quality to mean functions and tests for those functions. `nbdev` lets us mark functions and code that is important for the production element of our work using the `# export` directive. \n",
    "\n",
    "See https://nbdev.fast.ai/tutorial.html to get started using nbdev."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to use\n",
    "\n",
    "If any functions are important and you want to bring them with you to your production experiment then export them using nbdev."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Components\n",
    "\n",
    "* Ensure notebooks meet style standards (`nbqa`)\n",
    "* Create workflow from notebook steps (`ndbev`)\n",
    "* Experiment tracking (`sacred/incense`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps\n",
    "\n",
    "Steps are functions which can be executed independently. Structuring your code into steps brings many benefits:\n",
    "\n",
    "* Save Time:\n",
    "    * Checkpointing: can skip having to run expensive steps again\n",
    "    * Re-use: write a step once and use in many different workflows\n",
    "    \n",
    "* Easier to debug\n",
    "    * You can narrow down where the problem is happening quicker and can use print statements or a debugger within the fewlines of a functino rather than a longer script.\n",
    "    \n",
    "* Portability\n",
    "    * Steps can be run on different machines; potentially in parallel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flows\n",
    "\n",
    "A flow is short for workflow; they help you structure your work into something can be executed from start to finish. Structuring your work into flows has the following benefits:\n",
    "\n",
    "* Ordered execution: anyone can run your workflow because the order is defined.\n",
    "* Portability: writing your research as a flow helps to draw out dependencies on libraries or anything that can run in your environment but not elsewhere."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Commands\n",
    "       \n",
    "* nbdev_diff_nbs                   \n",
    "* nbdev_fix_merge          \n",
    "* nbdev_test_nbs  \n",
    "* nbdev_clean_nbs                                  \n",
    "* nbdev_new     \n",
    "* sciflow_tidy\n",
    "* sciflow_build_lib\n",
    "* sciflow_prepare\n",
    "* sciflow_build\n",
    "* sciflow_generate\n",
    "* sciflow_check_flows\n",
    "* sciflow_release"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:eu-west-1:819792524951:image/sagemaker-distribution-cpu-v0",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:eu-west-1:819792524951:image/sagemaker-distribution-cpu-v0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
