{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp experiment.lake_observer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sacred Data Lake Observer\n",
    "\n",
    "`sacred` is an excellent library for tracking machine learning experiments. It has an observer model for experiments and there are many different types of observer, which accomodate many destinations. When you combined with some community provided tooling like incense and omniboard this as complete an experimentation management capability as Data Scientists need. \n",
    "\n",
    "An issue that prevents greater adoption of the SIO stack sacred/incense/omniboard is dependence on an external service, namely MongoDB. It is not easy for Data Scientists to deploy a MongoDB instance within a production environment. However most Data Science notebook environments now permit access to data lake storage such as S3.\n",
    "\n",
    "> This `sacred` observer adds support for a data lake observer. This observer stores all data in block storage under a root experiment directory. Each experiment component, e.g artifacts, metrics, runs is stored in it's own directory. Components like runs and metrics can be queried using a lake compatible query engine with a client ODBC driver. Files and other nested/unstructured entities can be accessed from the block storage client directly. The goal is to provide the same capability as the MongoDBObserver and hence to be compatible with key downstream libraries like: `incense` and `omniboard`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import json\n",
    "import os\n",
    "import os.path\n",
    "import uuid\n",
    "\n",
    "import boto3\n",
    "import pandas as pd\n",
    "from sacred.dependencies import get_digest\n",
    "from sacred.observers.base import RunObserver\n",
    "from sacred.serializer import flatten\n",
    "from sciflow.s3_utils import (\n",
    "    delete_dir,\n",
    "    is_valid_bucket,\n",
    "    list_s3_subdirs,\n",
    "    objects_exist_in_dir,\n",
    "    s3_join,\n",
    ")\n",
    "from sciflow.utils import prepare_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "DEFAULT_S3_PRIORITY = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class AWSLakeObserver(RunObserver):\n",
    "    VERSION = \"AWSLakeObserver-0.1.0\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        project,\n",
    "        experiment_name,\n",
    "        bucket_name=None,\n",
    "        experiments_key_prefix=None,\n",
    "        priority=DEFAULT_S3_PRIORITY,\n",
    "        region=\"eu-west-1\",\n",
    "    ):\n",
    "        \"\"\"Constructor for a AWSLakeObserver object.\n",
    "\n",
    "        Run when the object is first created,\n",
    "        before it's used within an experiment.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        experiment_name\n",
    "            The nme of this experiment\n",
    "        bucket_name\n",
    "            The name of the bucket you want to store results in.\n",
    "            Doesn't need to contain `s3://`, but needs to be a valid bucket name\n",
    "        experiments_key_prefix\n",
    "            The relative path inside your bucket where you want this experiment to store results\n",
    "        priority\n",
    "            The priority to assign to this observer if\n",
    "            multiple observers are present\n",
    "        region\n",
    "            The AWS region in which you want to create and access\n",
    "            buckets. Needs to be either set here or configured in your AWS\n",
    "        \"\"\"\n",
    "        self.experiment_name = experiment_name\n",
    "        if bucket_name is None:\n",
    "            try:\n",
    "                bucket_name = os.environ[\"SCIFLOW_BUCKET\"]\n",
    "            except KeyError:\n",
    "                raise ValueError(\n",
    "                    \"Bucket name must be provided or set using SCIFLOW_BUCKET env\"\n",
    "                )\n",
    "        self.bucket_name = (\n",
    "            os.environ[\"SCIFLOW_BUCKET\"] if bucket_name is None else bucket_name\n",
    "        )\n",
    "        if not is_valid_bucket(self.bucket_name):\n",
    "            raise ValueError(\n",
    "                \"Your chosen bucket name doesn't follow AWS bucket naming rules\"\n",
    "            )\n",
    "        self.experiments_key_prefix = (\n",
    "            f\"{project}/experiments\"\n",
    "            if experiments_key_prefix is None\n",
    "            else experiments_key_prefix\n",
    "        )\n",
    "        self.experiments_key = s3_join(\n",
    "            self.experiments_key_prefix, self.experiment_name\n",
    "        )\n",
    "        self.experiment_dir = s3_join(self.bucket_name, self.experiments_key)\n",
    "        self.bucket_name = bucket_name\n",
    "        self.priority = priority\n",
    "        self.resource_dir = None\n",
    "        self.source_dir = None\n",
    "        self.runs_dir = None\n",
    "        self.metrics_dir = None\n",
    "        self.artifacts_dir = None\n",
    "        self.run_entry = None\n",
    "        self.config = None\n",
    "        self.info = None\n",
    "        self.experiment_id = None\n",
    "        self.cout = \"\"\n",
    "        self.cout_write_cursor = 0\n",
    "        self.saved_metrics = {}\n",
    "        if region is not None:\n",
    "            self.region = region\n",
    "            self.s3 = boto3.resource(\"s3\", region_name=region)\n",
    "        else:\n",
    "            session = boto3.session.Session()\n",
    "            if session.region_name is not None:\n",
    "                self.region = session.region_name\n",
    "                self.s3 = boto3.resource(\"s3\")\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                    \"You must either pass in an AWS region name, or have a \"\n",
    "                    \"region name specified in your AWS config file\"\n",
    "                )\n",
    "\n",
    "    def put_data(self, key, binary_data):\n",
    "        self.s3.Object(self.bucket_name, key).put(Body=binary_data)\n",
    "\n",
    "    def save_json(self, table_dir, obj, filename):\n",
    "        key = s3_join(table_dir, filename)\n",
    "        self.put_data(key, json.dumps(flatten(obj), sort_keys=True, indent=2))\n",
    "\n",
    "    def save_file(self, file_save_dir, filename, target_name=None):\n",
    "        target_name = target_name or os.path.basename(filename)\n",
    "        key = s3_join(file_save_dir, target_name)\n",
    "        self.put_data(key, open(filename, \"rb\"))\n",
    "\n",
    "    def save_sources(self, ex_info):\n",
    "        base_dir = ex_info[\"base_dir\"]\n",
    "        source_info = []\n",
    "        for s, m in ex_info[\"sources\"]:\n",
    "            abspath = os.path.join(base_dir, s)\n",
    "            store_path, md5sum = self.find_or_save(abspath, self.source_dir)\n",
    "            source_info.append(\n",
    "                [s, os.path.relpath(store_path, self.experiments_key_prefix)]\n",
    "            )\n",
    "        return source_info\n",
    "\n",
    "    def find_or_save(self, filename, store_dir):\n",
    "        source_name, ext = os.path.splitext(os.path.basename(filename))\n",
    "        md5sum = get_digest(filename)\n",
    "        store_name = source_name + \"_\" + md5sum + ext\n",
    "        store_path = s3_join(store_dir, store_name)\n",
    "        if len(list_s3_subdirs(self.s3, self.bucket_name, prefix=store_path)) == 0:\n",
    "            self.save_file(self.source_dir, filename, store_path)\n",
    "        return store_path, md5sum\n",
    "\n",
    "    def _determine_run_dir(self, run_id):\n",
    "        self.runs_dir = s3_join(self.experiments_key, \"runs\", str(run_id))\n",
    "        self.metrics_dir = s3_join(self.experiments_key, \"metrics\", str(run_id))\n",
    "        self.artifacts_dir = s3_join(self.experiments_key, \"artifacts\", str(run_id))\n",
    "        self.resource_dir = s3_join(self.experiments_key, \"resources\", str(run_id))\n",
    "        self.source_dir = s3_join(self.experiments_key, \"sources\", str(run_id))\n",
    "\n",
    "        self.dirs = (\n",
    "            self.runs_dir,\n",
    "            self.metrics_dir,\n",
    "            self.artifacts_dir,\n",
    "            self.resource_dir,\n",
    "            self.source_dir,\n",
    "        )\n",
    "        for dir_to_check in self.dirs:\n",
    "            if objects_exist_in_dir(self.s3, self.bucket_name, dir_to_check):\n",
    "                raise FileExistsError(\n",
    "                    \"S3 dir at {}/{} already exists; check your run_id is unique\".format(\n",
    "                        self.bucket_name, dir_to_check\n",
    "                    )\n",
    "                )\n",
    "\n",
    "    def queued_event(\n",
    "        self, ex_info, command, host_info, queue_time, config, meta_info, _id\n",
    "    ):\n",
    "        self._determine_run_dir(meta_info[\"run_id\"])\n",
    "\n",
    "        self.run_entry = {\n",
    "            \"experiment\": dict(ex_info),\n",
    "            \"command\": command,\n",
    "            \"host\": dict(host_info),\n",
    "            \"config\": flatten(config),\n",
    "            \"meta\": meta_info,\n",
    "            \"status\": \"QUEUED\",\n",
    "        }\n",
    "        self.config = config\n",
    "        self.info = {}\n",
    "\n",
    "        self.save_json(self.run_entry, \"run.json\")\n",
    "\n",
    "        return _id\n",
    "\n",
    "    def started_event(\n",
    "        self, ex_info, command, host_info, start_time, config, meta_info, _id\n",
    "    ):\n",
    "        self._determine_run_dir(meta_info[\"run_id\"])\n",
    "        self.experiment_id = meta_info[\"run_id\"]\n",
    "\n",
    "        ex_info[\"sources\"] = self.save_sources(ex_info)\n",
    "\n",
    "        self.run_entry = {\n",
    "            \"experiment_id\": self.experiment_id,\n",
    "            \"experiment\": dict(ex_info),\n",
    "            \"format\": self.VERSION,\n",
    "            \"command\": command,\n",
    "            \"host\": dict(host_info),\n",
    "            \"start_time\": start_time.isoformat(),\n",
    "            \"config\": flatten(config),\n",
    "            \"meta\": meta_info,\n",
    "            \"status\": \"RUNNING\",\n",
    "            \"resources\": [],\n",
    "            \"artifacts\": [],\n",
    "            \"captured_out\": \"\",\n",
    "            \"info\": {},\n",
    "            \"heartbeat\": None,\n",
    "        }\n",
    "        self.config = config\n",
    "        self.info = {}\n",
    "        self.cout = \"\"\n",
    "        self.cout_write_cursor = 0\n",
    "\n",
    "        self.save_json(self.runs_dir, self.run_entry, \"run.json\")\n",
    "\n",
    "        return _id\n",
    "\n",
    "    def heartbeat_event(self, info, captured_out, beat_time, result):\n",
    "        self.info = info\n",
    "        self.run_entry[\"heartbeat\"] = beat_time.isoformat()\n",
    "        self.run_entry[\"captured_out\"] = captured_out\n",
    "        self.run_entry[\"result\"] = result\n",
    "        self.save_json(self.runs_dir, self.run_entry, \"run.json\")\n",
    "\n",
    "    def completed_event(self, stop_time, result):\n",
    "        self.run_entry[\"stop_time\"] = stop_time.isoformat()\n",
    "        self.run_entry[\"result\"] = result\n",
    "        self.run_entry[\"status\"] = \"COMPLETED\"\n",
    "\n",
    "        self.save_json(self.runs_dir, self.run_entry, \"run.json\")\n",
    "\n",
    "    def interrupted_event(self, interrupt_time, status):\n",
    "        self.run_entry[\"stop_time\"] = interrupt_time.isoformat()\n",
    "        self.run_entry[\"status\"] = status\n",
    "        self.save_json(self.runs_dir, self.run_entry, \"run.json\")\n",
    "\n",
    "    def failed_event(self, fail_time, fail_trace):\n",
    "        self.run_entry[\"stop_time\"] = fail_time.isoformat()\n",
    "        self.run_entry[\"status\"] = \"FAILED\"\n",
    "        self.run_entry[\"fail_trace\"] = fail_trace\n",
    "        self.save_json(self.runs_dir, self.run_entry, \"run.json\")\n",
    "\n",
    "    def resource_event(self, filename):\n",
    "        store_path, md5sum = self.find_or_save(filename, self.resource_dir)\n",
    "        self.run_entry[\"resources\"].append([filename, store_path])\n",
    "        self.save_json(self.runs_dir, self.run_entry, \"run.json\")\n",
    "\n",
    "    def artifact_event(self, name, filename, metadata=None, content_type=None):\n",
    "        self.save_file(self.artifacts_dir, filename, name)\n",
    "        self.run_entry[\"artifacts\"].append(name)\n",
    "        self.save_json(self.runs_dir, self.run_entry, \"run.json\")\n",
    "\n",
    "    def log_metrics(self, metrics_by_name, info):\n",
    "        \"\"\"Store new measurements into metrics.csv\"\"\"\n",
    "        if len(metrics_by_name.values()) > 0:\n",
    "            metric_frames = [pd.DataFrame(v) for v in metrics_by_name.values()]\n",
    "            metrics = pd.concat(metric_frames).reset_index(drop=True)\n",
    "            metrics[\"experiment_id\"] = self.experiment_id\n",
    "            metrics_path = f\"s3://{self.bucket_name}/{self.metrics_dir}/metrics.csv\"\n",
    "            metrics.to_csv(metrics_path, index=False)\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        if isinstance(other, AWSLakeObserver):\n",
    "            return (\n",
    "                self.experiment_name == other.experiment_name\n",
    "                and self.bucket_name == other.bucket_name\n",
    "                and self.experiments_key_prefix == other.experiments_key_prefix\n",
    "            )\n",
    "        else:\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "project = \"sciflow\"\n",
    "experiment_name = \"lake_observer\"\n",
    "experiments_key_prefix = \"sciflow/experiments\"\n",
    "missing_bucket = \"s3awsmissing\"\n",
    "invalid_bucket = \"some bucket\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sciflow.utils import prepare_env\n",
    "\n",
    "prepare_env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "observer = AWSLakeObserver(project, experiment_name)\n",
    "observer = AWSLakeObserver(\n",
    "    project, experiment_name, experiments_key_prefix=experiments_key_prefix\n",
    ")\n",
    "observer = AWSLakeObserver(\n",
    "    project,\n",
    "    experiment_name,\n",
    "    experiments_key_prefix=experiments_key_prefix,\n",
    "    region=\"eu-west-1\",\n",
    ")\n",
    "assert observer.region == \"eu-west-1\"\n",
    "# Do not check for missing bucket yet\n",
    "observer = AWSLakeObserver(\n",
    "    project, experiment_name, missing_bucket, experiments_key_prefix, region=\"eu-west-1\"\n",
    ")\n",
    "try:\n",
    "    observer = AWSLakeObserver(\n",
    "        project,\n",
    "        experiment_name,\n",
    "        invalid_bucket,\n",
    "        experiments_key_prefix,\n",
    "        region=\"eu-west-1\",\n",
    "    )\n",
    "except ValueError as ve:\n",
    "    assert \"naming\" in str(ve).lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Run Expeirments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sacred import Experiment\n",
    "from sacred.run import Run\n",
    "\n",
    "ex_name = \"test_lake_obs\"\n",
    "ex = Experiment(\"test_lake_obs\", interactive=True)\n",
    "\n",
    "obs = AWSLakeObserver(\n",
    "    project,\n",
    "    experiment_name=ex_name,\n",
    "    bucket_name=os.environ[\"SCIFLOW_BUCKET\"],\n",
    "    experiments_key_prefix=experiments_key_prefix,\n",
    ")\n",
    "\n",
    "ex.observers.append(obs)\n",
    "\n",
    "\n",
    "@ex.config\n",
    "def my_config():\n",
    "    recipient = \"test\"\n",
    "    message = f\"Hello {recipient}!\"\n",
    "    f\"{message}\"\n",
    "\n",
    "\n",
    "@ex.main\n",
    "def my_main(message, _run: Run):\n",
    "    _run.add_artifact(\"test/requirements-generated.txt\")\n",
    "    _run.add_artifact(\"test/dataframe_artifact.csv\")\n",
    "    _run.log_scalar(\"another one\", 9.12, 0)\n",
    "    print(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_id = str(uuid.uuid4()).replace(\"-\", \"_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - test_lake_obs - Running command 'my_main'\n",
      "INFO - test_lake_obs - Started\n",
      "INFO - test_lake_obs - Completed after 0:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello test!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<sacred.run.Run at 0x7fa60c9b5190>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex.run(meta_info={\"run_id\": sample_id})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check they were created correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert objects_exist_in_dir(\n",
    "    boto3.resource(\"s3\"),\n",
    "    os.environ[\"SCIFLOW_BUCKET\"],\n",
    "    f\"{experiments_key_prefix}/test_lake_obs/artifacts/{sample_id}/requirements-generated.txt\",\n",
    ")\n",
    "assert objects_exist_in_dir(\n",
    "    boto3.resource(\"s3\"),\n",
    "    os.environ[\"SCIFLOW_BUCKET\"],\n",
    "    f\"{experiments_key_prefix}/test_lake_obs/artifacts/{sample_id}/dataframe_artifact.csv\",\n",
    ")\n",
    "assert objects_exist_in_dir(\n",
    "    boto3.resource(\"s3\"),\n",
    "    os.environ[\"SCIFLOW_BUCKET\"],\n",
    "    f\"{experiments_key_prefix}/test_lake_obs/metrics/{sample_id}/metrics.csv\",\n",
    ")\n",
    "assert objects_exist_in_dir(\n",
    "    boto3.resource(\"s3\"),\n",
    "    os.environ[\"SCIFLOW_BUCKET\"],\n",
    "    f\"{experiments_key_prefix}/test_lake_obs/runs/{sample_id}/run.json\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean up test resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.resource(\"s3\")\n",
    "delete_dir(s3, os.environ[\"SCIFLOW_BUCKET\"], f\"{experiments_key_prefix}/test_lake_obs\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:sciflow]",
   "language": "python",
   "name": "conda-env-sciflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
