{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp experiment.tracking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flow Experiment Tracking\n",
    "\n",
    "`sacred` ...\n",
    "\n",
    "An issue that prevents greater adoption of the SIO stack sacred/incense/omniboard is dependence on an external service, namely MongoDB. ..\n",
    "\n",
    "> This `sacred` observer adds support for a data lake observer. This observer stores all data in block storage under a root experiment directory. Each experiment component, e.g artifacts, metrics, runs is stored in it's own directory. Components like runs and metrics can be queried using a lake compatible query engine with a client ODBC driver. Files and other nested/unstructured entities can be accessed from the block storage client directly. The goal is to provide the same capability as the MongoDBObserver and hence to be compatible with key downstream libraries like: `incense` and `omniboard`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "import datetime\n",
    "import json\n",
    "import socket\n",
    "import time\n",
    "import uuid\n",
    "from pathlib import Path\n",
    "\n",
    "import boto3\n",
    "import pandas as pd\n",
    "from sacred import metrics_logger\n",
    "from sacred.host_info import get_host_info\n",
    "from sacred.serializer import flatten\n",
    "from sacred.stdout_capturing import get_stdcapturer\n",
    "from sacred.utils import IntervalTimer\n",
    "\n",
    "from sciflow.s3_utils import (\n",
    "    delete_dir,\n",
    "    list_bucket,\n",
    "    objects_exist_in_dir,\n",
    "    put_data,\n",
    "    s3_join,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step-level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "def save_json(s3_res, bucket_name, key, filename, obj):\n",
    "    key = s3_join(key, filename)\n",
    "    put_data(\n",
    "        s3_res, bucket_name, key, json.dumps(flatten(obj), sort_keys=True, indent=2)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "class StepTracker:\n",
    "    def __init__(\n",
    "        self,\n",
    "        bucket_name,\n",
    "        flow_base_key,\n",
    "        flow_run_id,\n",
    "        step_name,\n",
    "        capture_mode=\"sys\",\n",
    "        region=\"eu-west-1\",\n",
    "    ):\n",
    "        self.bucket_name = bucket_name\n",
    "        self.flow_base_key = flow_base_key\n",
    "        self.flow_run_id = flow_run_id\n",
    "        self.exp_base_key = s3_join(flow_base_key, flow_run_id, \"experiment\")\n",
    "        self.step_name = step_name\n",
    "        self.capture_mode = capture_mode\n",
    "        self._stop_heartbeat_event = None\n",
    "        self._heartbeat = None\n",
    "        self._output_file = None\n",
    "        self._metrics = metrics_logger.MetricsLogger()\n",
    "        self.captured_out = None\n",
    "        self.saved_metrics = {}\n",
    "        self.info = {}\n",
    "        self.result = None\n",
    "\n",
    "        # TODO read run_entry from run.json...\n",
    "        self.run_entry = {\n",
    "            \"experiment_id\": self.flow_run_id,\n",
    "            \"experiment\": {},\n",
    "            \"format\": None,\n",
    "            \"command\": None,\n",
    "            \"host\": get_host_info(),\n",
    "            \"all_hosts\": {socket.gethostname(): get_host_info()},\n",
    "            \"start_time\": datetime.datetime.utcnow().isoformat(),\n",
    "            \"config\": {},\n",
    "            \"meta\": {},\n",
    "            \"status\": \"RUNNING\",\n",
    "            \"resources\": [],\n",
    "            \"artifacts\": [],\n",
    "            \"captured_out\": \"\",\n",
    "            \"info\": self.info,\n",
    "            \"heartbeat\": None,\n",
    "        }\n",
    "\n",
    "        if region is not None:\n",
    "            self.region = region\n",
    "            self.s3_res = boto3.resource(\"s3\", region_name=region)\n",
    "        else:\n",
    "            session = boto3.session.Session()\n",
    "            if session.region_name is not None:\n",
    "                self.region = session.region_name\n",
    "                self.s3_res = boto3.resource(\"s3\")\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                    \"You must either pass in an AWS region name, or have a \"\n",
    "                    \"region name specified in your AWS config file\"\n",
    "                )\n",
    "\n",
    "        self.init_keys()\n",
    "\n",
    "    def start_heartbeat(self, beat_interval=10.0):\n",
    "        print(\"Starting Heartbeat\")\n",
    "        self._stop_heartbeat_event, self._heartbeat = IntervalTimer.create(\n",
    "            self._emit_heartbeat, beat_interval\n",
    "        )\n",
    "        self._heartbeat.start()\n",
    "\n",
    "    def stop_heartbeat(self):\n",
    "        print(\"Stopping Heartbeat\")\n",
    "        if self._heartbeat is not None:\n",
    "            self._stop_heartbeat_event.set()\n",
    "            self._heartbeat.join(timeout=2)\n",
    "\n",
    "    def capture_out(self):\n",
    "        # TODO figure out why only \"sys\" seems to work in Sagemaker? - tee is installed\n",
    "        _, capture_stdout = get_stdcapturer(self.capture_mode)\n",
    "        return capture_stdout()\n",
    "\n",
    "    def get_captured_out(self):\n",
    "        if self._output_file.closed:\n",
    "            return\n",
    "        text = self._output_file.get()\n",
    "        if isinstance(text, bytes):\n",
    "            text = text.decode(\"utf-8\", \"replace\")\n",
    "        if self.captured_out:\n",
    "            text = self.captured_out + text\n",
    "        self.captured_out = text\n",
    "\n",
    "    def log_metric(self, metric_name, metric_value, metric_step):\n",
    "        if metric_name not in self.saved_metrics:\n",
    "            self.saved_metrics[metric_name] = {\n",
    "                \"values\": [],\n",
    "                \"steps\": [],\n",
    "                \"timestamps\": [],\n",
    "            }\n",
    "\n",
    "        self.saved_metrics[metric_name][\"values\"].append(metric_value)\n",
    "        self.saved_metrics[metric_name][\"steps\"].append(metric_step)\n",
    "        self.saved_metrics[metric_name][\"timestamps\"].append(\n",
    "            datetime.datetime.utcnow().isoformat()\n",
    "        )\n",
    "        save_json(\n",
    "            self.s3_res,\n",
    "            self.bucket_name,\n",
    "            self.metrics_key,\n",
    "            \"metrics.json\",\n",
    "            self.saved_metrics,\n",
    "        )\n",
    "\n",
    "    def add_artifact(self, artifact_path):\n",
    "        name = Path(artifact_path).name\n",
    "        self.save_file(self.artifacts_key, artifact_path, name)\n",
    "        self.run_entry[\"artifacts\"].append(name)\n",
    "        save_json(\n",
    "            self.s3_res, self.bucket_name, self.runs_key, \"run.json\", self.run_entry\n",
    "        )\n",
    "\n",
    "    def _emit_heartbeat(self):\n",
    "        beat_time = datetime.datetime.utcnow().isoformat()\n",
    "        self.run_entry[\"heartbeat\"] = beat_time\n",
    "        print(f\"Emitted heartbeat at: {beat_time}\")\n",
    "        self.run_entry[\"captured_out\"] = self.get_captured_out()\n",
    "        self.run_entry[\"result\"] = self.result\n",
    "        save_json(\n",
    "            self.s3_res, self.bucket_name, self.runs_key, \"run.json\", self.run_entry\n",
    "        )\n",
    "\n",
    "    def save_file(self, file_save_dir, filename, target_name=None):\n",
    "        target_name = target_name or os.path.basename(filename)\n",
    "        key = s3_join(file_save_dir, target_name)\n",
    "        put_data(self.s3_res, self.bucket_name, key, open(filename, \"rb\"))\n",
    "\n",
    "    def init_keys(self):\n",
    "        self.runs_key = s3_join(self.exp_base_key, \"runs\")\n",
    "        self.metrics_key = s3_join(self.exp_base_key, \"metrics\")\n",
    "        self.artifacts_key = s3_join(self.exp_base_key, \"artifacts\")\n",
    "        self.resource_key = s3_join(self.exp_base_key, \"resources\")\n",
    "        self.source_key = s3_join(self.exp_base_key, \"sources\")\n",
    "\n",
    "        self.keys = (\n",
    "            self.runs_key,\n",
    "            self.metrics_key,\n",
    "            self.artifacts_key,\n",
    "            self.resource_key,\n",
    "            self.source_key,\n",
    "        )\n",
    "        for key_to_check in self.keys:\n",
    "            if objects_exist_in_dir(self.s3_res, self.bucket_name, key_to_check):\n",
    "                raise FileExistsError(\n",
    "                    f\"S3 dir at {self.bucket_name}/{key_to_check} already exists; check your run_id is unique\"\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name = \"pprsandboxpdlras3\"\n",
    "flow_base_key = \"flow-\" + str(uuid.uuid4())\n",
    "flow_run_id = \"sample_flow_instance_123\"\n",
    "flow_run_key = s3_join(flow_base_key, flow_run_id)\n",
    "s3_res = boto3.resource(\"s3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flow_base_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracker = StepTracker(bucket_name, flow_base_key, flow_run_id, \"experiment-test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracker.log_metric(\"auc\", 0.37, 0)\n",
    "tracker.log_metric(\"auc\", 0.45, 1)\n",
    "tracker.log_metric(\"auc\", 0.63, 2)\n",
    "tracker.log_metric(\"auc\", 0.89, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_by_name = json.loads(\n",
    "    pd.read_json(\n",
    "        f\"s3://{bucket_name}/{flow_base_key}/{flow_run_id}/experiment/metrics/metrics.json\"\n",
    "    ).to_json()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics_to_df(metrics_by_name):\n",
    "    metric_frames = []\n",
    "    for metric_name, metric_ptr in metrics_by_name.items():\n",
    "        metric_frame = pd.DataFrame(metric_ptr)\n",
    "        metric_frame[\"metric\"] = metric_name\n",
    "        metric_frames.append(metric_frame)\n",
    "    metrics = pd.concat(metric_frames).reset_index(drop=True)\n",
    "    metrics[\"flow_run_id\"] = flow_run_id\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = metrics_to_df(metrics_by_name)\n",
    "assert metrics[\"steps\"].tolist() == [0, 1, 2, 3]\n",
    "assert metrics[\"values\"].tolist() == [0.37, 0.45, 0.63, 0.89]\n",
    "assert metrics[\"metric\"].tolist() == [\"auc\", \"auc\", \"auc\", \"auc\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contents = list_bucket(bucket_name, flow_run_key)\n",
    "assert len(contents) == 1\n",
    "assert contents[0].split(\"/\")[-1] == \"metrics.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Out Capture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert tracker.captured_out is None\n",
    "with tracker.capture_out() as tracker._output_file:\n",
    "    print(\"Some text\")\n",
    "    print(\"Some text\")\n",
    "    tracker.get_captured_out()\n",
    "assert tracker.captured_out == \"Some text\\nSome text\\n\"\n",
    "tracker.captured_out = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artifacts\n",
    "\n",
    "> Support is provided for the same artifact types as found in `sacred`; however we will not be testing the creation, saving or loading of mp4s here as this would require external dependencies for video creation such as ffmpeg. \n",
    "\n",
    "Supported artifact types:\n",
    "\n",
    "* `.txt`: `text/csv`,\n",
    "* `.csv`: `text/csv`,\n",
    "* `.png`: `image/png`,\n",
    "* `.jpg`: `image/jpeg`,\n",
    "* `.mp4`: `video/mp4`,\n",
    "* `.pickle`: `application/octet-stream`,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\"a\": [1, 2, 3], \"b\": [\"a\", \"b\", \"c\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib auto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tempfile.TemporaryDirectory() as temp_dir:\n",
    "    csv_path = f\"{temp_dir}/testfile.csv\"\n",
    "    df.to_csv(csv_path)\n",
    "    txt_path = f\"{temp_dir}/testfile.txt\"\n",
    "    df.to_csv(txt_path)\n",
    "    fig = df.a.plot.hist().figure\n",
    "    png_path = f\"{temp_dir}/testfile.png\"\n",
    "    fig.savefig(png_path)\n",
    "    pdf_path = f\"{temp_dir}/testfile.pdf\"\n",
    "    fig.savefig(pdf_path)\n",
    "    pickle_path = f\"{temp_dir}/testfile.pkl\"\n",
    "    df.to_pickle(pickle_path)\n",
    "    artifacts = [csv_path, txt_path, png_path, pdf_path, pickle_path]\n",
    "    for artifact_path in artifacts:\n",
    "        tracker.add_artifact(artifact_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mock tracker in user mode\n",
    "# running in a flow??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Heartbeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tracker.capture_out() as tracker._output_file:\n",
    "    tracker.start_heartbeat(1.0)\n",
    "    print(\"Some text\")\n",
    "    time.sleep(4)\n",
    "    print(\"Some text\")\n",
    "    tracker.stop_heartbeat()\n",
    "    tracker.get_captured_out()\n",
    "assert len([t for t in tracker.captured_out.split(\"\\n\") if t == \"Some text\"]) == 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracker.run_entry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tear-down - delete created remote objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delete_dir(s3_res, bucket_name, flow_base_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flow-level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "def tracking_started(s3_res, bucket_name, flow_base_key, flow_run_id):\n",
    "    # Create run-entry\n",
    "    # Write run.json\n",
    "    # experiment = experiment_info - sacred.ingredient.experiment_info\n",
    "    # Command = run_flow?\n",
    "    # Config = params?\n",
    "    # meta = startup metadata put in FlowTracker\n",
    "\n",
    "    host_info = get_host_info()\n",
    "    self.run_entry = {\n",
    "        \"experiment_id\": self.flow_run_id,\n",
    "        \"experiment\": {},\n",
    "        \"format\": None,\n",
    "        \"command\": None,\n",
    "        \"host\": host_info,\n",
    "        \"all_hosts\": {socket.gethostname(): host_info},\n",
    "        \"start_time\": datetime.datetime.utcnow().isoformat(),\n",
    "        \"config\": {},\n",
    "        \"meta\": {},\n",
    "        \"status\": \"RUNNING\",\n",
    "        \"resources\": [],\n",
    "        \"artifacts\": [],\n",
    "        \"captured_out\": \"\",\n",
    "        \"info\": self.info,\n",
    "        \"heartbeat\": None,\n",
    "    }\n",
    "\n",
    "    runs_key = s3_join(flow_base_key, flow_run_id, \"experiment\", \"runs\")\n",
    "\n",
    "    save_json(s3_res, bucket_name, runs_key, run_entry, \"run.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "def tracking_interrupted(flow_run_id):\n",
    "    # Read run-entry from run.json\n",
    "    # Add interrupt\n",
    "    # Write run.json\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "def tracking_failed(flow_run_id):\n",
    "    # Read run-entry from run.json\n",
    "    # Add failure\n",
    "    # Write run.json\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "def tracking_completed(flow_run_id):\n",
    "    # Read run-entry from run.json\n",
    "    # Add failure\n",
    "    # Write run.json\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sciflow (sciflow/3)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:eu-west-1:368653567616:image-version/sciflow/3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
