{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp experiment.tracking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flow Experiment Tracking\n",
    "\n",
    "`sacred` ...\n",
    "\n",
    "An issue that prevents greater adoption of the SIO stack sacred/incense/omniboard is dependence on an external service, namely MongoDB. ..\n",
    "\n",
    "> This `sacred` observer adds support for a data lake observer. This observer stores all data in block storage under a root experiment directory. Each experiment component, e.g artifacts, metrics, runs is stored in it's own directory. Components like runs and metrics can be queried using a lake compatible query engine with a client ODBC driver. Files and other nested/unstructured entities can be accessed from the block storage client directly. The goal is to provide the same capability as the MongoDBObserver and hence to be compatible with key downstream libraries like: `incense` and `omniboard`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "import datetime\n",
    "import json\n",
    "import sys\n",
    "import tempfile\n",
    "import time\n",
    "import traceback as tb\n",
    "import uuid\n",
    "from pathlib import Path\n",
    "\n",
    "import boto3\n",
    "import pandas as pd\n",
    "from botocore.exceptions import ClientError\n",
    "from sacred import metrics_logger\n",
    "from sacred.host_info import get_host_info\n",
    "from sacred.serializer import flatten\n",
    "from sacred.stdout_capturing import get_stdcapturer\n",
    "from sacred.utils import IntervalTimer\n",
    "\n",
    "from sciflow.s3_utils import delete_dir, list_bucket, load_json, put_data, s3_join\n",
    "from sciflow.utils import prepare_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare_env()\n",
    "_bucket_name = os.environ[\"SCIFLOW_BUCKET\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "today = datetime.datetime.utcnow().strftime(\"%Y%m%d\")\n",
    "_flow_base_key = f\"sciflow-experiment-testing-{today}\"\n",
    "_flow_run_id = f\"sample_flow_instance_{str(uuid.uuid4())[-6:]}\"\n",
    "_flow_run_key = s3_join(_flow_base_key, _flow_run_id)\n",
    "_s3_res = boto3.resource(\"s3\")\n",
    "_steps = [\"experiment-test-1\", \"experiment-test-2\"]\n",
    "_step_name = _steps[0]\n",
    "_flow_base_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "# TODO replace the sacred flatten function and mvoe to s3_utils - needs a jsonpickle serialiser\n",
    "\n",
    "\n",
    "def save_json(s3_res, bucket_name, key, filename, obj):\n",
    "    key = s3_join(key, filename)\n",
    "    put_data(\n",
    "        s3_res, bucket_name, key, json.dumps(flatten(obj), sort_keys=True, indent=2)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "class FlowTracker:\n",
    "    def __init__(\n",
    "        self,\n",
    "        bucket_name,\n",
    "        flow_base_key,\n",
    "        flow_run_id,\n",
    "        steps,\n",
    "        params=None,\n",
    "        run_name=None,\n",
    "        region=\"eu-west-1\",\n",
    "    ):\n",
    "        self.bucket_name = bucket_name\n",
    "        self.flow_base_key = flow_base_key\n",
    "        self.flow_run_id = flow_run_id\n",
    "        self.steps = steps\n",
    "        self.params = params\n",
    "        self.run_name = run_name\n",
    "\n",
    "        if region is not None:\n",
    "            self.region = region\n",
    "            self.s3_res = boto3.resource(\"s3\", region_name=region)\n",
    "        else:\n",
    "            session = boto3.session.Session()\n",
    "            if session.region_name is not None:\n",
    "                self.region = session.region_name\n",
    "                self.s3 = boto3.resource(\"s3\")\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                    \"You must either pass in an AWS region name, or have a \"\n",
    "                    \"region name specified in your AWS config file\"\n",
    "                )\n",
    "\n",
    "        self.run_entry_key = s3_join(flow_base_key, flow_run_id, \"experiment\", \"runs\")\n",
    "        self.runs_table_key = s3_join(flow_base_key, \"experiments\", \"runs\", flow_run_id)\n",
    "\n",
    "    def start(self, params=None):\n",
    "        host_info = get_host_info()\n",
    "        run_entry = {\n",
    "            \"experiment_id\": self.flow_run_id,\n",
    "            \"experiment_name\": self.run_name,\n",
    "            \"experiment\": {\"name\": self.run_name},\n",
    "            \"format\": None,\n",
    "            \"command\": None,\n",
    "            \"host\": host_info,\n",
    "            \"start_time\": round(time.time()),\n",
    "            \"config\": params if params is not None else {},\n",
    "            \"meta\": {},\n",
    "            \"status\": \"RUNNING\",\n",
    "            \"resources\": [],\n",
    "            \"artifacts\": [],\n",
    "            \"captured_out\": \"\",\n",
    "            \"info\": {},\n",
    "            \"heartbeat\": None,\n",
    "            \"steps\": self.steps,\n",
    "        }\n",
    "\n",
    "        save_json(\n",
    "            self.s3_res, self.bucket_name, self.run_entry_key, \"run.json\", run_entry\n",
    "        )\n",
    "        save_json(\n",
    "            self.s3_res,\n",
    "            self.bucket_name,\n",
    "            self.run_entry_key,\n",
    "            \"flow_start_run.json\",\n",
    "            run_entry,\n",
    "        )\n",
    "        save_json(\n",
    "            self.s3_res, self.bucket_name, self.runs_table_key, \"run.json\", run_entry\n",
    "        )\n",
    "        # Create each step entry at flow start - in case of step failure\n",
    "        for step in self.steps:\n",
    "            save_json(\n",
    "                self.s3_res,\n",
    "                self.bucket_name,\n",
    "                self.run_entry_key,\n",
    "                f\"step_{step}.json\",\n",
    "                run_entry,\n",
    "            )\n",
    "        print(f\"Started tracking flow: {self.flow_run_id}\")\n",
    "\n",
    "    def interrupted(self):\n",
    "        self._tracking_event(\"INTERRUPTED\")\n",
    "        print(f\"Flow tracking interrupted: {self.flow_run_id}\")\n",
    "\n",
    "    def failed(self, except_info):\n",
    "        self._tracking_event(\"FAILED\", except_info)\n",
    "        print(f\"Flow tracking failed: {self.flow_run_id}\")\n",
    "\n",
    "    def completed(self):\n",
    "        self._tracking_event(\"COMPLETED\")\n",
    "        print(f\"Flow tracking completed: {self.flow_run_id}\")\n",
    "\n",
    "    def _tracking_event(self, event_status, except_info=None):\n",
    "        run_entry = load_json(\n",
    "            self.s3_res, self.bucket_name, s3_join(self.run_entry_key, \"run.json\")\n",
    "        )\n",
    "        run_entry[\"status\"] = event_status\n",
    "        run_entry[\"stop_time\"] = round(time.time())\n",
    "        run_entry[\"elapsed_time\"] = round(\n",
    "            run_entry[\"stop_time\"] - run_entry[\"start_time\"], 2\n",
    "        )\n",
    "        if except_info is not None:\n",
    "            run_entry[\"fail_trace\"] = tb.format_exception(\n",
    "                except_info[\"exc_type\"], except_info[\"exc_value\"], except_info[\"trace\"]\n",
    "            )\n",
    "\n",
    "        run_entry = self._merge_step_entries(run_entry)\n",
    "        save_json(\n",
    "            self.s3_res, self.bucket_name, self.run_entry_key, \"run.json\", run_entry\n",
    "        )\n",
    "        save_json(\n",
    "            self.s3_res, self.bucket_name, self.runs_table_key, \"run.json\", run_entry\n",
    "        )\n",
    "\n",
    "    def _merge_step_entries(self, run_entry):\n",
    "        all_hosts = {}\n",
    "        captured_out = \"\"\n",
    "        step_entries = {}\n",
    "        for step in self.steps:\n",
    "            s3_join(self.run_entry_key, f\"step_{step}.json\")\n",
    "            step_entry = load_json(\n",
    "                self.s3_res,\n",
    "                self.bucket_name,\n",
    "                s3_join(self.run_entry_key, f\"step_{step}.json\"),\n",
    "            )\n",
    "            all_hosts[step] = step_entry[\"host\"]\n",
    "            step_out = (\n",
    "                \"\" if step_entry[\"captured_out\"] is None else step_entry[\"captured_out\"]\n",
    "            )\n",
    "            captured_out += f\"******BEGIN step: {step}******\\n\"\n",
    "            captured_out += step_out\n",
    "            captured_out += f\"******END step: {step}******\\n\"\n",
    "            step_entries[step] = step_entry\n",
    "        run_entry[\"all_hosts\"] = all_hosts\n",
    "        run_entry[\"captured_out\"] = captured_out\n",
    "        run_entry[\"steps\"] = self.steps\n",
    "        run_entry[\"step_entries\"] = step_entries\n",
    "        return run_entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flow_tracker = FlowTracker(_bucket_name, _flow_base_key, _flow_run_id, _steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flow_tracker.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step-level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "class SciFlowTracker:\n",
    "    def log_metric(self, metric_name, metric_value, metric_step):\n",
    "        pass\n",
    "\n",
    "    def add_artifact(self, artifact_path):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "class TempFileTracker(SciFlowTracker):\n",
    "    def __init__(metrics_path=None, artifacts_dir=None):\n",
    "        self.metrics_path = metrics_path\n",
    "        self.artifacts_dir = artifacts_dir\n",
    "\n",
    "    def log_metric(self, metric_name, metric_value, metric_step):\n",
    "        # Append to csv dataframe\n",
    "        pass\n",
    "\n",
    "    def add_artifact(self, artifact_path):\n",
    "        # Save file to tmp directory\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "class MockTracker(SciFlowTracker):\n",
    "    def log_metric(self, metric_name, metric_value, metric_step):\n",
    "        pass\n",
    "\n",
    "    def add_artifact(self, artifact_path):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "class StepTracker(SciFlowTracker):\n",
    "    def __init__(\n",
    "        self,\n",
    "        bucket_name,\n",
    "        flow_base_key,\n",
    "        flow_run_id,\n",
    "        step_name,\n",
    "        capture_mode=\"sys\",\n",
    "        region=\"eu-west-1\",\n",
    "    ):\n",
    "        self.bucket_name = bucket_name\n",
    "        self.flow_base_key = flow_base_key\n",
    "        self.flow_run_id = flow_run_id\n",
    "        self.exp_base_key = s3_join(flow_base_key, flow_run_id, \"experiment\")\n",
    "        self.step_name = step_name\n",
    "        self.capture_mode = capture_mode\n",
    "        self._stop_heartbeat_event = None\n",
    "        self._heartbeat = None\n",
    "        self._output_file = None\n",
    "        self._metrics = metrics_logger.MetricsLogger()\n",
    "        self.captured_out = None\n",
    "        self.info = {}\n",
    "        self.result = None\n",
    "        self.start_time = round(time.time())\n",
    "\n",
    "        if region is not None:\n",
    "            self.region = region\n",
    "            self.s3_res = boto3.resource(\"s3\", region_name=region)\n",
    "        else:\n",
    "            session = boto3.session.Session()\n",
    "            if session.region_name is not None:\n",
    "                self.region = session.region_name\n",
    "                self.s3_res = boto3.resource(\"s3\")\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                    \"You must either pass in an AWS region name, or have a \"\n",
    "                    \"region name specified in your AWS config file\"\n",
    "                )\n",
    "\n",
    "        try:\n",
    "            self.saved_metrics = load_json(\n",
    "                self.s3_res,\n",
    "                bucket_name,\n",
    "                s3_join(self.exp_base_key, \"metrics\", \"metrics.json\"),\n",
    "            )\n",
    "        except ClientError as ex:\n",
    "            if ex.response[\"Error\"][\"Code\"] == \"NoSuchKey\":\n",
    "                self.saved_metrics = {}\n",
    "            else:\n",
    "                raise ex\n",
    "\n",
    "        self.flow_start_run_entry = load_json(\n",
    "            self.s3_res,\n",
    "            bucket_name,\n",
    "            s3_join(self.exp_base_key, \"runs\", \"flow_start_run.json\"),\n",
    "        )\n",
    "        self.run_entry = self.flow_start_run_entry\n",
    "        self.init_keys()\n",
    "\n",
    "    def start_heartbeat(self, beat_interval=10.0):\n",
    "        print(\"Starting Heartbeat\")\n",
    "        self._stop_heartbeat_event, self._heartbeat = IntervalTimer.create(\n",
    "            self._emit_heartbeat, beat_interval\n",
    "        )\n",
    "        self._heartbeat.start()\n",
    "\n",
    "    def stop_heartbeat(self):\n",
    "        print(\"Stopping Heartbeat\")\n",
    "        if self._heartbeat is not None:\n",
    "            self._stop_heartbeat_event.set()\n",
    "            self._heartbeat.join(timeout=2)\n",
    "\n",
    "    def capture_out(self):\n",
    "        # TODO figure out why only \"sys\" seems to work in Sagemaker? - tee is installed\n",
    "        _, capture_stdout = get_stdcapturer(self.capture_mode)\n",
    "        return capture_stdout()\n",
    "\n",
    "    def get_captured_out(self):\n",
    "        if self._output_file is None:\n",
    "            raise IOError(\n",
    "                \"Attempting to get captured out when capturing has not been started. Remember to wrap tracked statements in 'with tracker.capture_out() as tracker._output_file:'\"\n",
    "            )\n",
    "        if self._output_file.closed:\n",
    "            return\n",
    "        text = self._output_file.get()\n",
    "        if isinstance(text, bytes):\n",
    "            text = text.decode(\"utf-8\", \"replace\")\n",
    "        if self.captured_out:\n",
    "            text = self.captured_out + text\n",
    "        self.captured_out = text\n",
    "\n",
    "    def log_metric(self, metric_name, metric_value, metric_step):\n",
    "        if metric_name not in self.saved_metrics:\n",
    "            self.saved_metrics[metric_name] = {\n",
    "                \"values\": [],\n",
    "                \"steps\": [],\n",
    "                \"timestamps\": [],\n",
    "            }\n",
    "\n",
    "        self.saved_metrics[metric_name][\"values\"].append(metric_value)\n",
    "        self.saved_metrics[metric_name][\"steps\"].append(metric_step)\n",
    "        self.saved_metrics[metric_name][\"timestamps\"].append(\n",
    "            datetime.datetime.utcnow().isoformat()\n",
    "        )\n",
    "        save_json(\n",
    "            self.s3_res,\n",
    "            self.bucket_name,\n",
    "            self.metrics_key,\n",
    "            \"metrics.json\",\n",
    "            self.saved_metrics,\n",
    "        )\n",
    "        # TODO: handle parallel metric producing steps - requires merge of step entries\n",
    "        save_json(\n",
    "            self.s3_res,\n",
    "            self.bucket_name,\n",
    "            self.metrics_key,\n",
    "            f\"step_{self.step_name}_metrics.json\",\n",
    "            self.saved_metrics,\n",
    "        )\n",
    "        save_json(\n",
    "            self.s3_res,\n",
    "            self.bucket_name,\n",
    "            self.metrics_table_key,\n",
    "            \"metrics.json\",\n",
    "            self.saved_metrics,\n",
    "        )\n",
    "\n",
    "    def add_artifact(self, artifact_path):\n",
    "        name = Path(artifact_path).name\n",
    "        self.save_file(self.artifacts_key, artifact_path, name)\n",
    "        self.save_file(self.artifacts_table_key, artifact_path, name)\n",
    "        self.run_entry[\"artifacts\"].append(name)\n",
    "        save_json(\n",
    "            self.s3_res, self.bucket_name, self.runs_key, \"run.json\", self.run_entry\n",
    "        )\n",
    "        save_json(\n",
    "            self.s3_res,\n",
    "            self.bucket_name,\n",
    "            self.runs_key,\n",
    "            f\"step_{self.step_name}.json\",\n",
    "            self.run_entry,\n",
    "        )\n",
    "\n",
    "    def _emit_heartbeat(self):\n",
    "        beat_time = datetime.datetime.utcnow().isoformat()\n",
    "        self.run_entry[\"heartbeat\"] = beat_time\n",
    "        self.run_entry[\"captured_out\"] = self.get_captured_out()\n",
    "        self.run_entry[\"result\"] = self.result\n",
    "        save_json(\n",
    "            self.s3_res, self.bucket_name, self.runs_key, \"run.json\", self.run_entry\n",
    "        )\n",
    "        save_json(\n",
    "            self.s3_res,\n",
    "            self.bucket_name,\n",
    "            self.runs_key,\n",
    "            f\"step_{self.step_name}.json\",\n",
    "            self.run_entry,\n",
    "        )\n",
    "\n",
    "    def completed(self, status=\"COMPLETED\", except_info=None):\n",
    "        self.stop_heartbeat()\n",
    "        self.get_captured_out()\n",
    "        self.run_entry[\"captured_out\"] = self.captured_out\n",
    "        self.run_entry[\"result\"] = self.result\n",
    "        save_json(\n",
    "            self.s3_res, self.bucket_name, self.runs_key, \"run.json\", self.run_entry\n",
    "        )\n",
    "        self.run_entry[\"status\"] = status\n",
    "        self.run_entry[\"stop_time\"] = round(time.time())\n",
    "        self.run_entry[\"elapsed_time\"] = round(\n",
    "            self.run_entry[\"stop_time\"] - self.start_time, 2\n",
    "        )\n",
    "        self.run_entry[\"host\"] = get_host_info()\n",
    "        if except_info is not None:\n",
    "            self.run_entry[\"fail_trace\"] = tb.format_exception(\n",
    "                except_info[\"exc_type\"], except_info[\"exc_value\"], except_info[\"trace\"]\n",
    "            )\n",
    "        save_json(\n",
    "            self.s3_res,\n",
    "            self.bucket_name,\n",
    "            self.runs_key,\n",
    "            f\"step_{self.step_name}.json\",\n",
    "            self.run_entry,\n",
    "        )\n",
    "\n",
    "    def save_file(self, file_save_dir, filename, target_name=None):\n",
    "        target_name = target_name or os.path.basename(filename)\n",
    "        key = s3_join(file_save_dir, target_name)\n",
    "        put_data(self.s3_res, self.bucket_name, key, open(filename, \"rb\"))\n",
    "\n",
    "    def init_keys(self):\n",
    "        self.runs_key = s3_join(self.exp_base_key, \"runs\")\n",
    "        self.metrics_key = s3_join(self.exp_base_key, \"metrics\")\n",
    "        self.artifacts_key = s3_join(self.exp_base_key, \"artifacts\")\n",
    "        self.resource_key = s3_join(self.exp_base_key, \"resources\")\n",
    "        self.source_key = s3_join(self.exp_base_key, \"sources\")\n",
    "        self.metrics_table_key = s3_join(\n",
    "            self.flow_base_key, \"experiments\", \"metrics\", self.flow_run_id\n",
    "        )\n",
    "        self.artifacts_table_key = s3_join(\n",
    "            self.flow_base_key, \"experiments\", \"artifacts\", self.flow_run_id\n",
    "        )\n",
    "\n",
    "        self.keys = (\n",
    "            self.runs_key,\n",
    "            self.metrics_key,\n",
    "            self.artifacts_key,\n",
    "            self.resource_key,\n",
    "            self.source_key,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracker = StepTracker(_bucket_name, _flow_base_key, _flow_run_id, _steps[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracker.log_metric(\"auc\", 0.37, 0)\n",
    "tracker.log_metric(\"auc\", 0.45, 1)\n",
    "tracker.log_metric(\"auc\", 0.63, 2)\n",
    "tracker.log_metric(\"auc\", 0.89, 3)\n",
    "tracker.log_metric(\"r2\", 0.66, 0)\n",
    "tracker.log_metric(\"r2\", 0.67, 1)\n",
    "tracker.log_metric(\"rmse\", 0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_by_name = json.loads(\n",
    "    pd.read_json(\n",
    "        f\"s3://{_bucket_name}/{_flow_base_key}/{_flow_run_id}/experiment/metrics/metrics.json\"\n",
    "    ).to_json()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics_to_df(metrics_by_name, flow_run_id):\n",
    "    metric_frames = []\n",
    "    for metric_name, metric_ptr in metrics_by_name.items():\n",
    "        metric_frame = pd.DataFrame(metric_ptr)\n",
    "        metric_frame[\"metric\"] = metric_name\n",
    "        metric_frames.append(metric_frame)\n",
    "    metrics = pd.concat(metric_frames).reset_index(drop=True)\n",
    "    metrics[\"flow_run_id\"] = flow_run_id\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = metrics_to_df(metrics_by_name, _flow_run_id)\n",
    "assert metrics[\"steps\"].tolist() == [0, 1, 2, 3, 0, 1, 0]\n",
    "assert metrics[\"values\"].tolist() == [0.37, 0.45, 0.63, 0.89, 0.66, 0.67, 0]\n",
    "assert metrics[\"metric\"].tolist() == [\"auc\", \"auc\", \"auc\", \"auc\", \"r2\", \"r2\", \"rmse\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contents = list_bucket(_bucket_name, _flow_run_key)\n",
    "assert len(contents) >= 1\n",
    "assert (\n",
    "    len(\n",
    "        [\n",
    "            Path(c).name\n",
    "            for c in contents\n",
    "            if Path(c).name == \"metrics.json\" or Path(c).name == \"run.json\"\n",
    "        ]\n",
    "    )\n",
    "    == 2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Out Capture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raised = False\n",
    "try:\n",
    "    tracker.get_captured_out()\n",
    "except IOError:\n",
    "    raised = True\n",
    "assert raised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert tracker.captured_out is None\n",
    "with tracker.capture_out() as tracker._output_file:\n",
    "    print(\"Some text\")\n",
    "    print(\"Some text\")\n",
    "    tracker.get_captured_out()\n",
    "assert tracker.captured_out == \"Some text\\nSome text\\n\"\n",
    "tracker.captured_out = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artifacts\n",
    "\n",
    "> Support is provided for the same artifact types as found in `sacred`; however we will not be testing the creation, saving or loading of mp4s here as this would require external dependencies for video creation such as ffmpeg. \n",
    "\n",
    "Supported artifact types:\n",
    "\n",
    "* `.txt`: `text/csv`,\n",
    "* `.csv`: `text/csv`,\n",
    "* `.png`: `image/png`,\n",
    "* `.jpg`: `image/jpeg`,\n",
    "* `.mp4`: `video/mp4`,\n",
    "* `.pickle`: `application/octet-stream`,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\"a\": [1, 2, 3], \"b\": [\"a\", \"b\", \"c\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib auto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tempfile.TemporaryDirectory() as temp_dir:\n",
    "    csv_path = f\"{temp_dir}/testfile.csv\"\n",
    "    df.to_csv(csv_path)\n",
    "    txt_path = f\"{temp_dir}/testfile.txt\"\n",
    "    df.to_csv(txt_path)\n",
    "    fig = df.a.plot.hist().figure\n",
    "    png_path = f\"{temp_dir}/testfile.png\"\n",
    "    fig.savefig(png_path)\n",
    "    pdf_path = f\"{temp_dir}/testfile.pdf\"\n",
    "    fig.savefig(pdf_path)\n",
    "    pickle_path = f\"{temp_dir}/testfile.pkl\"\n",
    "    df.to_pickle(pickle_path)\n",
    "    artifacts = [csv_path, txt_path, png_path, pdf_path, pickle_path]\n",
    "    for artifact_path in artifacts:\n",
    "        tracker.add_artifact(artifact_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Heartbeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slow\n",
    "\n",
    "with tracker.capture_out() as tracker._output_file:\n",
    "    tracker.start_heartbeat(1.0)\n",
    "    print(\"Some text\")\n",
    "    time.sleep(3)\n",
    "    print(\"Some text\")\n",
    "    tracker.stop_heartbeat()\n",
    "    tracker.get_captured_out()\n",
    "assert len([t for t in tracker.captured_out.split(\"\\n\") if t == \"Some text\"]) == 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finish Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_entry_key = s3_join(\n",
    "    _flow_base_key, _flow_run_id, \"experiment\", \"runs\", f\"step_{_step_name}.json\"\n",
    ")\n",
    "step_entry = load_json(_s3_res, _bucket_name, step_entry_key)\n",
    "assert step_entry[\"status\"] == \"RUNNING\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracker.completed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_entry_key = s3_join(\n",
    "    _flow_base_key, _flow_run_id, \"experiment\", \"runs\", f\"step_{_step_name}.json\"\n",
    ")\n",
    "step_entry = load_json(_s3_res, _bucket_name, step_entry_key)\n",
    "assert step_entry[\"status\"] == \"COMPLETED\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add a second step.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracker = StepTracker(_bucket_name, _flow_base_key, _flow_run_id, _steps[1])\n",
    "with tempfile.TemporaryDirectory() as temp_dir:\n",
    "    with tracker.capture_out() as tracker._output_file:\n",
    "        tracker.start_heartbeat(1.0)\n",
    "        tracker.log_metric(\"qini_auc\", 0.5, 0)\n",
    "        tracker.log_metric(\"qini_auc\", 0.6, 1)\n",
    "        csv_path = f\"{temp_dir}/{_steps[1]}_file.csv\"\n",
    "        df.to_csv(csv_path)\n",
    "        tracker.add_artifact(csv_path)\n",
    "    tracker.completed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flow_tracker.completed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_entry_key = s3_join(_flow_base_key, _flow_run_id, \"experiment\", \"runs\", \"run.json\")\n",
    "run_entry = load_json(_s3_res, _bucket_name, run_entry_key)\n",
    "assert run_entry[\"status\"] == \"COMPLETED\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_by_name = json.loads(\n",
    "    pd.read_json(\n",
    "        f\"s3://{_bucket_name}/{_flow_base_key}/{_flow_run_id}/experiment/metrics/metrics.json\"\n",
    "    ).to_json()\n",
    ")\n",
    "metrics = metrics_to_df(metrics_by_name, _flow_run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert sorted(metrics.metric.unique()) == [\"auc\", \"qini_auc\", \"r2\", \"rmse\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "artifacts = [\n",
    "    \"testfile.csv\",\n",
    "    \"testfile.txt\",\n",
    "    \"testfile.png\",\n",
    "    \"testfile.pdf\",\n",
    "    \"testfile.pkl\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check artifacts exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check metrics logged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "captured_out = \"\"\"******BEGIN step: experiment-test******\\n\n",
    "Starting Heartbeat\\n\n",
    "Some text\\n\n",
    "Emitted heartbeat at: 2022-06-09T11:40:25.112986\\n\n",
    "Emitted heartbeat at: 2022-06-09T11:40:26.223391\\n\n",
    "Some text\\n\n",
    "Stopping Heartbeat\\n\n",
    "Emitted heartbeat at: 2022-06-09T11:40:27.116552\\n\n",
    "******END step: experiment-test******\\n\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# has a positive long stop time\n",
    "# has a positive float elapsed_time\n",
    "# has an all_hosts entry with len 1\n",
    "# caputed out has beginning & end text and 2 Some texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete_dir(s3_res, bucket_name, flow_base_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Full Flow Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_step(tracker):\n",
    "    print(\"Some text\")\n",
    "    time.sleep(2)\n",
    "    tracker.log_metric(\"auroc\", 0.5, 0)\n",
    "    csv_path = f\"{temp_dir}/testfile.csv\"\n",
    "    df.to_csv(csv_path)\n",
    "    time.sleep(2)\n",
    "    tracker.add_artifact(csv_path)\n",
    "    time.sleep(2)\n",
    "    fig = df.a.plot.hist().figure\n",
    "    png_path = f\"{temp_dir}/testfile.png\"\n",
    "    fig.savefig(png_path)\n",
    "    tracker.add_artifact(png_path)\n",
    "    print(\"Some text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def second_step(tracker):\n",
    "    print(\"More text\")\n",
    "    time.sleep(2)\n",
    "    tracker.log_metric(\"auroc\", 0.8, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = [\"first_step\", \"second_step\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_flow_run_id = f\"sample_flow_instance_{str(uuid.uuid4())[-6:]}\"\n",
    "_flow_run_key = s3_join(_flow_base_key, _flow_run_id)\n",
    "_steps = [\"first_step\", \"second_step\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slow\n",
    "\n",
    "flow_tracker = FlowTracker(_bucket_name, _flow_base_key, _flow_run_id, _steps)\n",
    "\n",
    "try:\n",
    "    flow_tracker.start()\n",
    "\n",
    "    try:\n",
    "        tracker = StepTracker(_bucket_name, _flow_base_key, _flow_run_id, \"first_step\")\n",
    "        with tempfile.TemporaryDirectory() as temp_dir:\n",
    "            with tracker.capture_out() as tracker._output_file:\n",
    "                tracker.start_heartbeat(1.0)\n",
    "                first_step(tracker)\n",
    "                tracker.completed()\n",
    "    except BaseException:\n",
    "        exc_type, exc_value, trace = sys.exc_info()\n",
    "        except_info = {\"exc_type\": exc_type, \"exc_value\": exc_value, \"trace\": trace}\n",
    "        tracker.completed(status=\"FAILED\", except_info=except_info)\n",
    "\n",
    "    try:\n",
    "        tracker = StepTracker(_bucket_name, _flow_base_key, _flow_run_id, \"second_step\")\n",
    "        with tempfile.TemporaryDirectory() as temp_dir:\n",
    "            with tracker.capture_out() as tracker._output_file:\n",
    "                tracker.start_heartbeat(1.0)\n",
    "                second_step(tracker)\n",
    "                tracker.completed()\n",
    "    except BaseException:\n",
    "        exc_type, exc_value, trace = sys.exc_info()\n",
    "        except_info = {\"exc_type\": exc_type, \"exc_value\": exc_value, \"trace\": trace}\n",
    "        tracker.completed(status=\"FAILED\", except_info=except_info)\n",
    "\n",
    "    flow_tracker.completed()\n",
    "except (KeyboardInterrupt):\n",
    "    flow_tracker.interrupted()\n",
    "    print(f\"Flow interrupted by user: {_flow_run_id}\")\n",
    "except BaseException:\n",
    "    exc_type, exc_value, trace = sys.exc_info()\n",
    "    except_info = {\"exc_type\": exc_type, \"exc_value\": exc_value, \"trace\": trace}\n",
    "    flow_tracker.failed(except_info)\n",
    "    print(f\"Flow failed: {_flow_run_id}\")\n",
    "# Exception interrupt handling\n",
    "# Failure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_flow_run_id = f\"sample_flow_instance_{str(uuid.uuid4())[-6:]}\"\n",
    "_flow_run_key = s3_join(_flow_base_key, _flow_run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slow\n",
    "\n",
    "flow_tracker = FlowTracker(_bucket_name, _flow_base_key, _flow_run_id, _steps)\n",
    "\n",
    "try:\n",
    "    flow_tracker.start()\n",
    "    flow_tracker.completed()\n",
    "except (KeyboardInterrupt):\n",
    "    flow_tracker.interrupted()\n",
    "    print(f\"Flow interrupted by user: {_flow_run_id}\")\n",
    "except BaseException:\n",
    "    exc_type, exc_value, trace = sys.exc_info()\n",
    "    except_info = {\"exc_type\": exc_type, \"exc_value\": exc_value, \"trace\": trace}\n",
    "    flow_tracker.failed(except_info)\n",
    "    print(f\"Flow failed: {_flow_run_id}\")\n",
    "# Exception interrupt handling\n",
    "# Failure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_flow_run_id = f\"sample_flow_instance_{str(uuid.uuid4())[-6:]}\"\n",
    "_flow_run_key = s3_join(_flow_base_key, _flow_run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slow\n",
    "\n",
    "flow_tracker = FlowTracker(_bucket_name, _flow_base_key, _flow_run_id, _steps)\n",
    "\n",
    "try:\n",
    "    flow_tracker.start()\n",
    "\n",
    "    try:\n",
    "        tracker = StepTracker(_bucket_name, _flow_base_key, _flow_run_id, \"first_step\")\n",
    "        with tempfile.TemporaryDirectory() as temp_dir:\n",
    "            with tracker.capture_out() as tracker._output_file:\n",
    "                raise ValueError()\n",
    "                tracker.start_heartbeat(1.0)\n",
    "                tracker.completed()\n",
    "    except BaseException:\n",
    "        exc_type, exc_value, trace = sys.exc_info()\n",
    "        except_info = {\"exc_type\": exc_type, \"exc_value\": exc_value, \"trace\": trace}\n",
    "        tracker.completed(status=\"FAILED\", except_info=except_info)\n",
    "\n",
    "    raise ValueError()\n",
    "except (KeyboardInterrupt):\n",
    "    flow_tracker.interrupted()\n",
    "    print(f\"Flow interrupted by user: {_flow_run_id}\")\n",
    "except BaseException:\n",
    "    exc_type, exc_value, trace = sys.exc_info()\n",
    "    except_info = {\"exc_type\": exc_type, \"exc_value\": exc_value, \"trace\": trace}\n",
    "    flow_tracker.failed(except_info)\n",
    "    print(f\"Flow failed: {_flow_run_id}\")\n",
    "# Exception interrupt handling\n",
    "# Failure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_flow_run_id = f\"sample_flow_instance_{str(uuid.uuid4())[-6:]}\"\n",
    "_flow_run_key = s3_join(_flow_base_key, _flow_run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slow\n",
    "\n",
    "flow_tracker = FlowTracker(_bucket_name, _flow_base_key, _flow_run_id, _steps)\n",
    "\n",
    "try:\n",
    "    flow_tracker.start()\n",
    "\n",
    "    raise ValueError()\n",
    "\n",
    "    flow_tracker.completed()\n",
    "except (KeyboardInterrupt):\n",
    "    flow_tracker.interrupted()\n",
    "    print(f\"Flow interrupted by user: {_flow_run_id}\")\n",
    "except BaseException:\n",
    "    exc_type, exc_value, trace = sys.exc_info()\n",
    "    except_info = {\"exc_type\": exc_type, \"exc_value\": exc_value, \"trace\": trace}\n",
    "    flow_tracker.failed(except_info)\n",
    "    print(f\"Flow failed: {_flow_run_id}\")\n",
    "# Exception interrupt handling\n",
    "# Failure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slow\n",
    "\n",
    "run_entry_key = s3_join(_flow_base_key, _flow_run_id, \"experiment\", \"runs\")\n",
    "run_entry = load_json(_s3_res, _bucket_name, s3_join(run_entry_key, \"run.json\"))\n",
    "first_step_entry = load_json(\n",
    "    _s3_res, _bucket_name, s3_join(run_entry_key, \"step_first_step.json\")\n",
    ")\n",
    "second_step_entry = load_json(\n",
    "    _s3_res, _bucket_name, s3_join(run_entry_key, \"step_second_step.json\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slow\n",
    "delete_dir(_s3_res, _bucket_name, _flow_base_key)"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.m5.xlarge",
  "kernelspec": {
   "display_name": "sciflow (sciflow/3)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:eu-west-1:368653567616:image-version/sciflow/3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
