{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp experiment.tracking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flow Experiment Tracking\n",
    "\n",
    "`sacred` ...\n",
    "\n",
    "An issue that prevents greater adoption of the SIO stack sacred/incense/omniboard is dependence on an external service, namely MongoDB. ..\n",
    "\n",
    "> This `sacred` observer adds support for a data lake observer. This observer stores all data in block storage under a root experiment directory. Each experiment component, e.g artifacts, metrics, runs is stored in it's own directory. Components like runs and metrics can be queried using a lake compatible query engine with a client ODBC driver. Files and other nested/unstructured entities can be accessed from the block storage client directly. The goal is to provide the same capability as the MongoDBObserver and hence to be compatible with key downstream libraries like: `incense` and `omniboard`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "import datetime\n",
    "import json\n",
    "import uuid\n",
    "\n",
    "import boto3\n",
    "from sacred.serializer import flatten\n",
    "from sacred.stdout_capturing import get_stdcapturer\n",
    "from sacred.utils import IntervalTimer\n",
    "\n",
    "from sciflow.s3_utils import (\n",
    "    delete_dir,\n",
    "    list_bucket,\n",
    "    list_s3_subdirs,\n",
    "    objects_exist_in_dir,\n",
    "    s3_join,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step-level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "class StepTracker:\n",
    "    def __init__(\n",
    "        self,\n",
    "        bucket_name,\n",
    "        flow_base_key,\n",
    "        flow_run_id,\n",
    "        step_name,\n",
    "        capture_mode=\"sys\",\n",
    "        region=\"eu-west-1\",\n",
    "    ):\n",
    "        self.bucket_name = bucket_name\n",
    "        self.flow_base_key = flow_base_key\n",
    "        self.flow_run_id = flow_run_id\n",
    "        self.exp_base_key = s3_join(flow_base_key, flow_run_id, \"experiment\")\n",
    "        self.step_name = step_name\n",
    "        self.capture_mode = capture_mode\n",
    "        self._stop_heartbeat_event = None\n",
    "        self._heartbeat = None\n",
    "        self._output_file = None\n",
    "        self.captured_out = None\n",
    "        self.saved_metrics = {}\n",
    "\n",
    "        if region is not None:\n",
    "            self.region = region\n",
    "            self.s3 = boto3.resource(\"s3\", region_name=region)\n",
    "        else:\n",
    "            session = boto3.session.Session()\n",
    "            if session.region_name is not None:\n",
    "                self.region = session.region_name\n",
    "                self.s3 = boto3.resource(\"s3\")\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                    \"You must either pass in an AWS region name, or have a \"\n",
    "                    \"region name specified in your AWS config file\"\n",
    "                )\n",
    "\n",
    "        self.init_keys()\n",
    "\n",
    "    def start_heartbeat(self):\n",
    "        print(\"Starting Heartbeat\")\n",
    "        self._stop_heartbeat_event, self._heartbeat = IntervalTimer.create(\n",
    "            _emit_heartbeat\n",
    "        )\n",
    "        self._heartbeat.start()\n",
    "\n",
    "    def stop_heartbeat(self):\n",
    "        print(\"Stopping Heartbeat\")\n",
    "        if self._heartbeat is not None:\n",
    "            self._stop_heartbeat_event.set()\n",
    "            self._heartbeat.join(timeout=2)\n",
    "\n",
    "    def capture_out(self):\n",
    "        # TODO figure out why only \"sys\" seems to work in Sagemaker? - tee is installed\n",
    "        _, capture_stdout = get_stdcapturer(self.capture_mode)\n",
    "        return capture_stdout()\n",
    "\n",
    "    def get_captured_out(self):\n",
    "        if self._output_file.closed:\n",
    "            return\n",
    "        text = self._output_file.get()\n",
    "        if isinstance(text, bytes):\n",
    "            text = text.decode(\"utf-8\", \"replace\")\n",
    "        if self.captured_out:\n",
    "            text = self.captured_out + text\n",
    "        self.captured_out = text\n",
    "\n",
    "    def log_metrics(self, metrics_by_name):\n",
    "        \"\"\"Store new measurements into metrics.json.\"\"\"\n",
    "        for metric_name, metric_ptr in metrics_by_name.items():\n",
    "\n",
    "            if metric_name not in self.saved_metrics:\n",
    "                self.saved_metrics[metric_name] = {\n",
    "                    \"values\": [],\n",
    "                    \"steps\": [],\n",
    "                    \"timestamps\": [],\n",
    "                }\n",
    "\n",
    "            self.saved_metrics[metric_name][\"values\"] += metric_ptr[\"values\"]\n",
    "            self.saved_metrics[metric_name][\"steps\"] += metric_ptr[\"steps\"]\n",
    "            timestamps_norm = [ts.isoformat() for ts in metric_ptr[\"timestamps\"]]\n",
    "            self.saved_metrics[metric_name][\"timestamps\"] += timestamps_norm\n",
    "        self.save_json(self.metrics_key, self.saved_metrics, \"metrics.json\")\n",
    "\n",
    "    def add_artifacts(self, artifacts):\n",
    "        for name, filepath in artifacts:\n",
    "            self.save_file(self.artifacts_key, filepath, name)\n",
    "            self.run_entry[\"artifacts\"].append(name)\n",
    "        self.save_json(self.runs_key, self.run_entry, \"run.json\")\n",
    "\n",
    "    def _emit_heartbeat(self):\n",
    "        self.get_captured_out()\n",
    "        # Read all measured metrics since last heartbeat\n",
    "        logged_metrics = self._metrics.get_last_metrics()\n",
    "        metrics_by_name = linearize_metrics(logged_metrics)\n",
    "        self.log_metrics(metrics_by_name, self.info)\n",
    "        beat_time = datetime.datetime.utcnow()\n",
    "        self.run_entry[\"heartbeat\"] = beat_time.isoformat()\n",
    "        self.run_entry[\"captured_out\"] = self.get_captured_out()\n",
    "        self.run_entry[\"result\"] = self.result\n",
    "        self.save_json(self.runs_key, self.run_entry, \"run.json\")\n",
    "\n",
    "    def put_data(self, key, binary_data):\n",
    "        self.s3.Object(self.bucket_name, key).put(Body=binary_data)\n",
    "\n",
    "    def save_json(self, table_dir, obj, filename):\n",
    "        key = s3_join(table_dir, filename)\n",
    "        self.put_data(key, json.dumps(flatten(obj), sort_keys=True, indent=2))\n",
    "\n",
    "    def save_file(self, file_save_dir, filename, target_name=None):\n",
    "        target_name = target_name or os.path.basename(filename)\n",
    "        key = s3_join(file_save_dir, target_name)\n",
    "        self.put_data(key, open(filename, \"rb\"))\n",
    "\n",
    "    def save_sources(self, ex_info):\n",
    "        base_dir = ex_info[\"base_dir\"]\n",
    "        source_info = []\n",
    "        for s, m in ex_info[\"sources\"]:\n",
    "            abspath = os.path.join(base_dir, s)\n",
    "            store_path, md5sum = self.find_or_save(abspath, self.source_key)\n",
    "            source_info.append(\n",
    "                [s, os.path.relpath(store_path, self.experiments_key_prefix)]\n",
    "            )\n",
    "        return source_info\n",
    "\n",
    "    def find_or_save(self, filename, store_dir):\n",
    "        source_name, ext = os.path.splitext(os.path.basename(filename))\n",
    "        md5sum = get_digest(filename)\n",
    "        store_name = source_name + \"_\" + md5sum + ext\n",
    "        store_path = s3_join(store_dir, store_name)\n",
    "        if len(list_s3_subdirs(self.s3, self.bucket_name, prefix=store_path)) == 0:\n",
    "            self.save_file(self.source_key, filename, store_path)\n",
    "        return store_path, md5sum\n",
    "\n",
    "    def init_keys(self):\n",
    "        self.runs_key = s3_join(self.exp_base_key, \"runs\")\n",
    "        self.metrics_key = s3_join(self.exp_base_key, \"metrics\")\n",
    "        self.artifacts_key = s3_join(self.exp_base_key, \"artifacts\")\n",
    "        self.resource_key = s3_join(self.exp_base_key, \"resources\")\n",
    "        self.source_key = s3_join(self.exp_base_key, \"sources\")\n",
    "\n",
    "        self.keys = (\n",
    "            self.runs_key,\n",
    "            self.metrics_key,\n",
    "            self.artifacts_key,\n",
    "            self.resource_key,\n",
    "            self.source_key,\n",
    "        )\n",
    "        for key_to_check in self.keys:\n",
    "            if objects_exist_in_dir(self.s3, self.bucket_name, key_to_check):\n",
    "                raise FileExistsError(\n",
    "                    f\"S3 dir at {self.bucket_name}/{key_to_check} already exists; check your run_id is unique\"\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name = \"pprsandboxpdlras3\"\n",
    "flow_base_key = \"flow-\" + str(uuid.uuid4())\n",
    "flow_run_id = \"sample_flow_instance_123\"\n",
    "flow_run_key = s3_join(flow_base_key, flow_run_id)\n",
    "s3_res = boto3.resource(\"s3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flow_base_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracker = StepTracker(bucket_name, flow_base_key, flow_run_id, \"experiment-test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_by_name = {\n",
    "    \"auc\": {\"values\": [0.9], \"steps\": [1], \"timestamps\": [datetime.datetime.utcnow()]}\n",
    "}\n",
    "metrics_by_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracker.log_metrics(metrics_by_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_bucket(bucket_name, flow_run_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Out Capture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert tracker.captured_out is None\n",
    "with tracker.capture_out() as tracker._output_file:\n",
    "    print(\"Some text\")\n",
    "    print(\"Some text\")\n",
    "    tracker.get_captured_out()\n",
    "assert tracker.captured_out == \"Some text\\nSome text\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artifacts\n",
    "\n",
    "> Support is provided for the same artifact types as found in `sacred`; however we will not be testing the creation, saving or loading of mp4s here as this would require external dependencies for video creation such as ffmpeg. \n",
    "\n",
    "Supported artifact types:\n",
    "\n",
    "* `.txt`: `text/csv`,\n",
    "* `.csv`: `text/csv`,\n",
    "* `.png`: `image/png`,\n",
    "* `.jpg`: `image/jpeg`,\n",
    "* `.mp4`: `video/mp4`,\n",
    "* `.pickle`: `application/octet-stream`,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\"a\": [1, 2, 3], \"b\": [\"a\", \"b\", \"c\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.a.plot.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.DataFrame({'a': [1,2,3], 'b': ['a', 'b', 'c']})\n",
    "\n",
    "# with tempfile.TemporaryDirectory() as temp_dir:\n",
    "#     csv_path = f\"{temp_dir}/testfile.csv\"\n",
    "#     df.to_csv(csv_path)\n",
    "#     txt_path = f\"{temp_dir}/testfile.txt\"\n",
    "#     df.to_csv(txt_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Host info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tear-down - delete created remote objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delete_dir(s3_res, bucket_name, flow_base_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flow-level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "def _emit_started(flow_run_id):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "def _emit_interrupted(flow_run_id):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "def _emit_failed(flow_run_id):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "def _emit_completed(flow_run_id):\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sciflow (sciflow/3)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:eu-west-1:368653567616:image-version/sciflow/3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
