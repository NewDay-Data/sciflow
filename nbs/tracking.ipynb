{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp experiment.tracking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flow Experiment Tracking\n",
    "\n",
    "`sacred` ...\n",
    "\n",
    "An issue that prevents greater adoption of the SIO stack sacred/incense/omniboard is dependence on an external service, namely MongoDB. ..\n",
    "\n",
    "> This `sacred` observer adds support for a data lake observer. This observer stores all data in block storage under a root experiment directory. Each experiment component, e.g artifacts, metrics, runs is stored in it's own directory. Components like runs and metrics can be queried using a lake compatible query engine with a client ODBC driver. Files and other nested/unstructured entities can be accessed from the block storage client directly. The goal is to provide the same capability as the MongoDBObserver and hence to be compatible with key downstream libraries like: `incense` and `omniboard`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "import datetime\n",
    "\n",
    "from sacred.stdout_capturing import get_stdcapturer\n",
    "from sacred.utils import IntervalTimer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step-level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "class StepTracker:\n",
    "    def __init__(self, flow_base_uri, flow_run_id, step_name, capture_mode=\"sys\"):\n",
    "        self.flow_base_uri = flow_base_uri\n",
    "        self.flow_run_id = flow_run_id\n",
    "        self.step_name = step_name\n",
    "        self.capture_mode = capture_mode\n",
    "        self._stop_heartbeat_event = None\n",
    "        self._heartbeat = None\n",
    "        self._output_file = None\n",
    "        self.captured_out = None\n",
    "        self.saved_metrics = {}\n",
    "\n",
    "    def start_heartbeat(self):\n",
    "        print(\"Starting Heartbeat\")\n",
    "        self._stop_heartbeat_event, self._heartbeat = IntervalTimer.create(\n",
    "            _emit_heartbeat\n",
    "        )\n",
    "        self._heartbeat.start()\n",
    "\n",
    "    def stop_heartbeat(self):\n",
    "        print(\"Stopping Heartbeat\")\n",
    "        if self._heartbeat is not None:\n",
    "            self._stop_heartbeat_event.set()\n",
    "            self._heartbeat.join(timeout=2)\n",
    "\n",
    "    def capture_out(self):\n",
    "        #\n",
    "        # TODO figure out why only sys seems to work in Sagemaker\n",
    "        _, capture_stdout = get_stdcapturer(self.capture_mode)\n",
    "        return capture_stdout()\n",
    "\n",
    "    def get_captured_out(self):\n",
    "        if self._output_file.closed:\n",
    "            return\n",
    "        text = self._output_file.get()\n",
    "        if isinstance(text, bytes):\n",
    "            text = text.decode(\"utf-8\", \"replace\")\n",
    "        if self.captured_out:\n",
    "            text = self.captured_out + text\n",
    "        self.captured_out = text\n",
    "\n",
    "    def log_metrics(self, metrics_by_name, info):\n",
    "        \"\"\"Store new measurements into metrics.json.\"\"\"\n",
    "        for metric_name, metric_ptr in metrics_by_name.items():\n",
    "\n",
    "            if metric_name not in self.saved_metrics:\n",
    "                self.saved_metrics[metric_name] = {\n",
    "                    \"values\": [],\n",
    "                    \"steps\": [],\n",
    "                    \"timestamps\": [],\n",
    "                }\n",
    "\n",
    "            self.saved_metrics[metric_name][\"values\"] += metric_ptr[\"values\"]\n",
    "            self.saved_metrics[metric_name][\"steps\"] += metric_ptr[\"steps\"]\n",
    "\n",
    "            timestamps_norm = [ts.isoformat() for ts in metric_ptr[\"timestamps\"]]\n",
    "            self.saved_metrics[metric_name][\"timestamps\"] += timestamps_norm\n",
    "\n",
    "        self.save_json(self.saved_metrics, \"metrics.json\")\n",
    "\n",
    "    def add_artifact(self, name, filename, metadata=None, content_type=None):\n",
    "        self.save_file(self.artifacts_dir, filename, name)\n",
    "        self.run_entry[\"artifacts\"].append(name)\n",
    "        self.save_json(self.runs_dir, self.run_entry, \"run.json\")\n",
    "\n",
    "    def _emit_heartbeat(self):\n",
    "        self.get_captured_out()\n",
    "        # Read all measured metrics since last heartbeat\n",
    "        logged_metrics = self._metrics.get_last_metrics()\n",
    "        metrics_by_name = linearize_metrics(logged_metrics)\n",
    "        self.log_metrics(metrics_by_name, self.info)\n",
    "        self.heartbeat_event()\n",
    "\n",
    "    def heartbeat_event(self):\n",
    "        beat_time = datetime.datetime.utcnow()\n",
    "        self.run_entry[\"heartbeat\"] = beat_time.isoformat()\n",
    "        self.run_entry[\"captured_out\"] = self.get_captured_out()\n",
    "        self.run_entry[\"result\"] = self.result\n",
    "        self.save_json(self.runs_dir, self.run_entry, \"run.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # export\n",
    "# class AWSLakeObserver(RunObserver):\n",
    "#     VERSION = \"AWSLakeObserver-0.1.0\"\n",
    "\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         flow_base_uri,\n",
    "#         flow_run_id,\n",
    "#         bucket_name=None,\n",
    "#         experiments_key_prefix=None,\n",
    "#         priority=DEFAULT_S3_PRIORITY,\n",
    "#         region=\"eu-west-1\",\n",
    "#     ):\n",
    "#         \"\"\"Constructor for a AWSLakeObserver object.\n",
    "\n",
    "#         Run when the object is first created,\n",
    "#         before it's used within an experiment.\n",
    "\n",
    "#         Parameters\n",
    "#         ----------\n",
    "#         experiment_name\n",
    "#             The nme of this experiment\n",
    "#         bucket_name\n",
    "#             The name of the bucket you want to store results in.\n",
    "#             Doesn't need to contain `s3://`, but needs to be a valid bucket name\n",
    "#         experiments_key_prefix\n",
    "#             The relative path inside your bucket where you want this experiment to store results\n",
    "#         priority\n",
    "#             The priority to assign to this observer if\n",
    "#             multiple observers are present\n",
    "#         region\n",
    "#             The AWS region in which you want to create and access\n",
    "#             buckets. Needs to be either set here or configured in your AWS\n",
    "#         \"\"\"\n",
    "#         self.experiment_name = experiment_name\n",
    "#         if bucket_name is None:\n",
    "#             try:\n",
    "#                 bucket_name = os.environ[\"SCIFLOW_BUCKET\"]\n",
    "#             except KeyError:\n",
    "#                 raise ValueError(\n",
    "#                     \"Bucket name must be provided or set using SCIFLOW_BUCKET env\"\n",
    "#                 )\n",
    "#         self.bucket_name = (\n",
    "#             os.environ[\"SCIFLOW_BUCKET\"] if bucket_name is None else bucket_name\n",
    "#         )\n",
    "#         if not is_valid_bucket(self.bucket_name):\n",
    "#             raise ValueError(\n",
    "#                 \"Your chosen bucket name doesn't follow AWS bucket naming rules\"\n",
    "#             )\n",
    "#         self.experiments_key_prefix = (\n",
    "#             f\"{project}/experiments\"\n",
    "#             if experiments_key_prefix is None\n",
    "#             else experiments_key_prefix\n",
    "#         )\n",
    "#         self.experiments_key = s3_join(\n",
    "#             self.experiments_key_prefix, self.experiment_name\n",
    "#         )\n",
    "#         self.experiment_dir = s3_join(self.bucket_name, self.experiments_key)\n",
    "#         self.bucket_name = bucket_name\n",
    "#         self.priority = priority\n",
    "#         self.resource_dir = None\n",
    "#         self.source_dir = None\n",
    "#         self.runs_dir = None\n",
    "#         self.metrics_dir = None\n",
    "#         self.artifacts_dir = None\n",
    "#         self.run_entry = None\n",
    "#         self.config = None\n",
    "#         self.info = None\n",
    "#         self.experiment_id = None\n",
    "#         self.cout = \"\"\n",
    "#         self.cout_write_cursor = 0\n",
    "#         self.saved_metrics = {}\n",
    "#         if region is not None:\n",
    "#             self.region = region\n",
    "#             self.s3 = boto3.resource(\"s3\", region_name=region)\n",
    "#         else:\n",
    "#             session = boto3.session.Session()\n",
    "#             if session.region_name is not None:\n",
    "#                 self.region = session.region_name\n",
    "#                 self.s3 = boto3.resource(\"s3\")\n",
    "#             else:\n",
    "#                 raise ValueError(\n",
    "#                     \"You must either pass in an AWS region name, or have a \"\n",
    "#                     \"region name specified in your AWS config file\"\n",
    "#                 )\n",
    "\n",
    "#     def put_data(self, key, binary_data):\n",
    "#         self.s3.Object(self.bucket_name, key).put(Body=binary_data)\n",
    "\n",
    "#     def save_json(self, table_dir, obj, filename):\n",
    "#         key = s3_join(table_dir, filename)\n",
    "#         self.put_data(key, json.dumps(flatten(obj), sort_keys=True, indent=2))\n",
    "\n",
    "#     def save_file(self, file_save_dir, filename, target_name=None):\n",
    "#         target_name = target_name or os.path.basename(filename)\n",
    "#         key = s3_join(file_save_dir, target_name)\n",
    "#         self.put_data(key, open(filename, \"rb\"))\n",
    "\n",
    "#     def save_sources(self, ex_info):\n",
    "#         base_dir = ex_info[\"base_dir\"]\n",
    "#         source_info = []\n",
    "#         for s, m in ex_info[\"sources\"]:\n",
    "#             abspath = os.path.join(base_dir, s)\n",
    "#             store_path, md5sum = self.find_or_save(abspath, self.source_dir)\n",
    "#             source_info.append(\n",
    "#                 [s, os.path.relpath(store_path, self.experiments_key_prefix)]\n",
    "#             )\n",
    "#         return source_info\n",
    "\n",
    "#     def find_or_save(self, filename, store_dir):\n",
    "#         source_name, ext = os.path.splitext(os.path.basename(filename))\n",
    "#         md5sum = get_digest(filename)\n",
    "#         store_name = source_name + \"_\" + md5sum + ext\n",
    "#         store_path = s3_join(store_dir, store_name)\n",
    "#         if len(list_s3_subdirs(self.s3, self.bucket_name, prefix=store_path)) == 0:\n",
    "#             self.save_file(self.source_dir, filename, store_path)\n",
    "#         return store_path, md5sum\n",
    "\n",
    "#     def _determine_run_dir(self, run_id):\n",
    "#         self.runs_dir = s3_join(self.experiments_key, \"runs\", str(run_id))\n",
    "#         self.metrics_dir = s3_join(self.experiments_key, \"metrics\", str(run_id))\n",
    "#         self.artifacts_dir = s3_join(self.experiments_key, \"artifacts\", str(run_id))\n",
    "#         self.resource_dir = s3_join(self.experiments_key, \"resources\", str(run_id))\n",
    "#         self.source_dir = s3_join(self.experiments_key, \"sources\", str(run_id))\n",
    "\n",
    "#         self.dirs = (\n",
    "#             self.runs_dir,\n",
    "#             self.metrics_dir,\n",
    "#             self.artifacts_dir,\n",
    "#             self.resource_dir,\n",
    "#             self.source_dir,\n",
    "#         )\n",
    "#         for dir_to_check in self.dirs:\n",
    "#             if objects_exist_in_dir(self.s3, self.bucket_name, dir_to_check):\n",
    "#                 raise FileExistsError(\n",
    "#                     \"S3 dir at {}/{} already exists; check your run_id is unique\".format(\n",
    "#                         self.bucket_name, dir_to_check\n",
    "#                     )\n",
    "#                 )\n",
    "\n",
    "#     def started_event(\n",
    "#         self, ex_info, command, host_info, start_time, config, meta_info, _id\n",
    "#     ):\n",
    "#         self._determine_run_dir(meta_info[\"run_id\"])\n",
    "#         self.experiment_id = meta_info[\"run_id\"]\n",
    "\n",
    "#         ex_info[\"sources\"] = self.save_sources(ex_info)\n",
    "\n",
    "#         self.run_entry = {\n",
    "#             \"experiment_id\": self.experiment_id,\n",
    "#             \"experiment\": dict(ex_info),\n",
    "#             \"format\": self.VERSION,\n",
    "#             \"command\": command,\n",
    "#             \"host\": dict(host_info),\n",
    "#             \"start_time\": start_time.isoformat(),\n",
    "#             \"config\": flatten(config),\n",
    "#             \"meta\": meta_info,\n",
    "#             \"status\": \"RUNNING\",\n",
    "#             \"resources\": [],\n",
    "#             \"artifacts\": [],\n",
    "#             \"captured_out\": \"\",\n",
    "#             \"info\": {},\n",
    "#             \"heartbeat\": None,\n",
    "#         }\n",
    "#         self.config = config\n",
    "#         self.info = {}\n",
    "#         self.cout = \"\"\n",
    "#         self.cout_write_cursor = 0\n",
    "\n",
    "#         self.save_json(self.runs_dir, self.run_entry, \"run.json\")\n",
    "\n",
    "#         return _id\n",
    "\n",
    "\n",
    "#     def completed_event(self, stop_time, result):\n",
    "#         self.run_entry[\"stop_time\"] = stop_time.isoformat()\n",
    "#         self.run_entry[\"result\"] = result\n",
    "#         self.run_entry[\"status\"] = \"COMPLETED\"\n",
    "\n",
    "#         self.save_json(self.runs_dir, self.run_entry, \"run.json\")\n",
    "\n",
    "#     def interrupted_event(self, interrupt_time, status):\n",
    "#         self.run_entry[\"stop_time\"] = interrupt_time.isoformat()\n",
    "#         self.run_entry[\"status\"] = status\n",
    "#         self.save_json(self.runs_dir, self.run_entry, \"run.json\")\n",
    "\n",
    "#     def failed_event(self, fail_time, fail_trace):\n",
    "#         self.run_entry[\"stop_time\"] = fail_time.isoformat()\n",
    "#         self.run_entry[\"status\"] = \"FAILED\"\n",
    "#         self.run_entry[\"fail_trace\"] = fail_trace\n",
    "#         self.save_json(self.runs_dir, self.run_entry, \"run.json\")\n",
    "\n",
    "#     def resource_event(self, filename):\n",
    "#         store_path, md5sum = self.find_or_save(filename, self.resource_dir)\n",
    "#         self.run_entry[\"resources\"].append([filename, store_path])\n",
    "#         self.save_json(self.runs_dir, self.run_entry, \"run.json\")\n",
    "\n",
    "\n",
    "#     def __eq__(self, other):\n",
    "#         if isinstance(other, AWSLakeObserver):\n",
    "#             return (\n",
    "#                 self.experiment_name == other.experiment_name\n",
    "#                 and self.bucket_name == other.bucket_name\n",
    "#                 and self.experiments_key_prefix == other.experiments_key_prefix\n",
    "#             )\n",
    "#         else:\n",
    "#             return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracker = StepTracker(\"file://tmp/test-flow/\", \"flow-123\", \"experiment-test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert tracker.captured_out is None\n",
    "with tracker.capture_out() as tracker._output_file:\n",
    "    print(\"Some text\")\n",
    "    print(\"Some text\")\n",
    "    tracker.get_captured_out()\n",
    "assert tracker.captured_out == \"Some text\\nSome text\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flow-level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "def _emit_started(flow_run_id):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "def _emit_interrupted(flow_run_id):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "def _emit_failed(flow_run_id):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "def _emit_completed(flow_run_id):\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sciflow (sciflow/3)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:eu-west-1:368653567616:image-version/sciflow/3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
