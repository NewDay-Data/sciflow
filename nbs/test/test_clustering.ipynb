{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "# default_exp test.test_clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exportn_step:first\n",
    "\n",
    "\n",
    "def something():\n",
    "    print(\"The first step\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example Usage: Mock Unsupervised Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Auto-reloading modules is very useful when using `nbdev` as changes to underlying modules are picked up without having to restart the kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sciflow.utils import lib_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Params\n",
    "\n",
    "> `sciflow` uses the papermill format for paramaeterising notebooks. See here for how to specify papermill params: https://papermill.readthedocs.io/en/latest/usage-parameterize.html. These parameters will be available to use in your flows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "traffic_percent = 1\n",
    "workers = 8\n",
    "model_level = \"dispatcher\"\n",
    "min_date = \"2021-01-01\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "def get_traffic_text(percent):\n",
    "    return str(percent) if int(percent) >= 10 else \"0\" + str(percent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`nbdev` tests are any cells which are not exporting code and do not have flags that say they should be ignored from testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert get_traffic_text(\"3\") == \"03\"\n",
    "assert get_traffic_text(\"13\") == \"13\"\n",
    "assert get_traffic_text(\"78\") == \"78\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "def get_experiment_segment(traffic_percent):\n",
    "    return tuple(get_traffic_text(tp) for tp in range(traffic_percent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert get_experiment_segment(1) == (\"00\",)\n",
    "assert get_experiment_segment(3) == (\"00\", \"01\", \"02\")\n",
    "assert \"' '\".join(get_experiment_segment(1)) == \"00\"\n",
    "assert f\"\"\"IN ('{\"','\".join(get_experiment_segment(3))}')\"\"\" == \"IN ('00','01','02')\"\n",
    "assert len(get_experiment_segment(50)) == 50\n",
    "assert max([int(x) for x in get_experiment_segment(100)]) == 99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "def get_utterances(model_level=None, min_date=None, traffic_percent=100):\n",
    "    \"\"\"\n",
    "    You will probably call data preparation code here. To simplify dependencies we are just creating synthetic data instead.\n",
    "    \"\"\"\n",
    "    get_experiment_segment(traffic_percent)\n",
    "    dummy_data = pd.Series(\n",
    "        np.random.choice(\n",
    "            [\n",
    "                \"Hello\",\n",
    "                \"Goodbye\",\n",
    "                \"Hi\",\n",
    "                \"Can you help?\",\n",
    "                \"I have an issue, can you help me?\",\n",
    "            ],\n",
    "            100,\n",
    "        ),\n",
    "        name=\"utterance\",\n",
    "    )\n",
    "    return dummy_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exportn_step:preprocess\n",
    "\n",
    "\n",
    "def preprocess(model_level=None, min_date=None, traffic_percent=100):\n",
    "    data = get_utterances(model_level, min_date, traffic_percent)\n",
    "    documents = data.tolist()\n",
    "    results = {\"documents\": documents}\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = preprocess(traffic_percent)[\"documents\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(documents) > 0\n",
    "assert (\n",
    "    pd.Series([\"Some other text\", \"Which should not be in the utterances\"])\n",
    "    .isin(pd.Series(documents))\n",
    "    .sum()\n",
    "    == 0\n",
    ")  # no button response texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "class Topics:\n",
    "    def __init__(self, documents, workers):\n",
    "        pass\n",
    "\n",
    "    def get_num_topics(self):\n",
    "        return 6\n",
    "\n",
    "    def get_topic_sizes(self):\n",
    "        return [1, 2, 3, 4, 5, 6], [1, 2, 3, 4, 5, 6]\n",
    "\n",
    "    def get_topics(self, num_topics):\n",
    "        return (\n",
    "            [\"cat\", \"sat\", \"mat\", \"mouse\", \"house\", \"grouse\"],\n",
    "            np.asarray([1, 1, 1, 1, 1, 1]),\n",
    "            [1, 2, 3, 4, 5, 6],\n",
    "        )\n",
    "\n",
    "    def plot_wordcloud(self):\n",
    "        print(\"you may want to remove plotting code from testing to speed things up\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exportn_step:fit\n",
    "\n",
    "\n",
    "def fit(documents, workers=workers):\n",
    "    model = Topics(documents, workers=workers)\n",
    "    results = {\"model\": model}\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Tests which are long running can be ignored from test execution. You can use the tst flags in settings.ini or create your own in the same file. See https://nbdev.fast.ai/test for more info. In this example we use `#slow` to indicate this should be skipped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slow\n",
    "import time\n",
    "\n",
    "time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = fit(documents, workers=workers)[\"model\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Number of Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_num_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Size of Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_sizes, topic_nums = model.get_topic_sizes()\n",
    "assert all([s > 0 for s in topic_sizes])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Topic Words & Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_words, word_scores, topic_nums = model.get_topics(model.get_num_topics())\n",
    "assert len(topic_words) == model.get_num_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you may want to remove plotting code from testing to speed things up\n"
     ]
    }
   ],
   "source": [
    "# vis\n",
    "# time.sleep(120)\n",
    "model.plot_wordcloud()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exportn_step:evaluate\n",
    "\n",
    "\n",
    "def evaluate(model):\n",
    "    topic_words, word_scores, topic_nums = model.get_topics(model.get_num_topics())\n",
    "\n",
    "    topic_contains_non_empty_words = all([len(tw) > 0 for tw in topic_words])\n",
    "    word_scores_in_range = word_scores.min() >= 0.0 and word_scores.max() <= 1.0\n",
    "    as_many_items_as_topics = (\n",
    "        model.get_num_topics() == len(topic_words) == word_scores.shape[0]\n",
    "    )\n",
    "    word_summaries = (\n",
    "        topic_contains_non_empty_words\n",
    "        and word_scores_in_range\n",
    "        and as_many_items_as_topics\n",
    "    )\n",
    "    # You can add artifacts in a step that will be saved to block storage. Add the paths to the file on the local filesystem\n",
    "    # and the artifact will be uploaded to remote storage.\n",
    "    artifacts = [lib_path(\"nbs\", \"test\", \"dataframe_artifact.csv\")]\n",
    "    # You can add step metrics too this time just add a list of 3-tuples where tuple order = (name, value, step)\n",
    "    metrics = [(\"mae\", 100, 0), (\"mae\", 67, 1), (\"mae\", 32, 2)]\n",
    "    results = {\n",
    "        \"word_summaries\": word_summaries,\n",
    "        \"artifacts\": artifacts,\n",
    "        \"metrics\": metrics,\n",
    "    }\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = evaluate(model)\n",
    "assert results[\"word_summaries\"]\n",
    "assert results[\"metrics\"] == [(\"mae\", 100, 0), (\"mae\", 67, 1), (\"mae\", 32, 2)]\n",
    "assert results[\"artifacts\"] == [lib_path(\"nbs\", \"test\", \"dataframe_artifact.csv\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Serve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def serve_num_topics(model):\n",
    "    return model.get_num_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert serve_num_topics(model) > 0"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "sciflow (sciflow/3)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:eu-west-1:368653567616:image-version/sciflow/3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
