{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp lake_experiment_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sacred Data Lake Experiment Loader\n",
    "\n",
    "> This class extends the `incense` project to allow you to load `sacred` experiments from a data lake store such as S3. It is assumed that there exists a ODBC SQL driver for this lake source.\n",
    "\n",
    "> NOTE: initially this class supports S3 & turbodbc only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from functools import lru_cache\n",
    "from typing import Tuple\n",
    "\n",
    "import pandas as pd\n",
    "from incense.artifact import CSVArtifact\n",
    "from incense.experiment import Experiment\n",
    "from sciflow.utils import load_dremio_access\n",
    "from text_discovery.lake_experiment import LakeExperiment\n",
    "from tinydb import Query, TinyDB\n",
    "from tinydb.storages import MemoryStorage\n",
    "from turbodbc.exceptions import DatabaseError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_CACHE_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dremio_access = load_dremio_access()\n",
    "bucket_name = \"s3bawspprwe1chatbotunpub01\"\n",
    "project_dir = \"discovery/experiments/test\"\n",
    "experiments_dir = \"discovery/experiments/test\"\n",
    "experiment_name = \"lake_observer\"\n",
    "table_context = '\"chatbot_unpublish_s3\".discovery.experiments.test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class LakeExperimentLoader:\n",
    "    def __init__(\n",
    "        self,\n",
    "        dremio_access,\n",
    "        table_context,\n",
    "        bucket_name,\n",
    "        experiments_dir,\n",
    "        experiment_name,\n",
    "    ):\n",
    "        self.dremio_access = dremio_access\n",
    "        self.table_context = table_context\n",
    "        self.bucket_name = bucket_name\n",
    "        self.experiments_dir = experiments_dir\n",
    "        self.experiment_name = experiment_name\n",
    "\n",
    "    @lru_cache(maxsize=MAX_CACHE_SIZE)\n",
    "    def _find(\n",
    "        self,\n",
    "        experiment_name=None,\n",
    "        experiment_ids=None,\n",
    "        experiment_id: int = None,\n",
    "        order_by: str = None,\n",
    "        limit: int = None,\n",
    "    ) -> Experiment:\n",
    "        if experiment_name is None:\n",
    "            experiment_name = self.experiment_name\n",
    "        query = f\"select * from {self.table_context}.{experiment_name}.runs\"\n",
    "        if experiment_ids:\n",
    "            ids = \", \".join([str(i) for i in experiment_ids])\n",
    "            query += f\" where dir0 IN ({ids})\"\n",
    "        if experiment_id:\n",
    "            query += f\" where dir0 = {experiment_id}\"\n",
    "        if order_by:\n",
    "            query += f\" order by {order_by} desc\"\n",
    "        if limit:\n",
    "            query += f\" limit {limit}\"\n",
    "        data = dremio_access.read_sql_to_dataframe(query)\n",
    "        experiments = [\n",
    "            LakeExperiment(\n",
    "                self.bucket_name,\n",
    "                self.experiments_dir,\n",
    "                experiment_name,\n",
    "                ex_id,\n",
    "                data.iloc[i, :].to_dict(),\n",
    "            )\n",
    "            for i, ex_id in enumerate(data.dir0.tolist())\n",
    "        ]\n",
    "        return experiments\n",
    "\n",
    "    @lru_cache(maxsize=MAX_CACHE_SIZE)\n",
    "    def find_by_id(self, experiment_id):\n",
    "        experiments = self._find(experiment_id=experiment_id)\n",
    "        return None if len(experiments) == 0 else experiments[0]\n",
    "\n",
    "    @lru_cache(maxsize=MAX_CACHE_SIZE)\n",
    "    def find_by_ids(self, experiment_ids: Tuple[int]):\n",
    "        if len(experiment_ids) == 1:\n",
    "            raise ValueError(\"Use find_by_id for a single experiment\")\n",
    "        return self._find(experiment_ids=experiment_ids)\n",
    "\n",
    "    @lru_cache(maxsize=MAX_CACHE_SIZE)\n",
    "    def find_latest(self, n=5):\n",
    "        return self._find(order_by=\"dir0\", limit=n)\n",
    "\n",
    "    @lru_cache(maxsize=MAX_CACHE_SIZE)\n",
    "    def find_all(self):\n",
    "        return self._find()\n",
    "\n",
    "    @lru_cache(maxsize=MAX_CACHE_SIZE)\n",
    "    def find_by_name(self, experiment_name):\n",
    "        result = None\n",
    "        try:\n",
    "            result = self._find(experiment_name=experiment_name)\n",
    "        except PermissionError:\n",
    "            print(f\"File not found or access not granted; check path information\")\n",
    "        return result\n",
    "\n",
    "    def insert_docs(self, db, prop_name):\n",
    "        experiments = self.find_all()\n",
    "        for ex in experiments:\n",
    "            document = json.loads(ex._data[prop_name])\n",
    "            document[\"experiment_id\"] = ex.experiment_id\n",
    "            db.insert(document)\n",
    "\n",
    "    def find_by_key(self, prop_name, key, value):\n",
    "        db = TinyDB(storage=MemoryStorage)\n",
    "        self.insert_docs(db, prop_name)\n",
    "        Experiment = Query()\n",
    "        docs = list(db.search(Experiment[key] == value))\n",
    "        if len(docs) == 0:\n",
    "            return None\n",
    "        if len(docs) == 1:\n",
    "            return self.find_by_id(docs[0][\"experiment_id\"])\n",
    "        return self.find_by_ids(tuple(d[\"experiment_id\"] for d in docs))\n",
    "\n",
    "    def find_by_config_key(self, key, value):\n",
    "        return self.find_by_key(\"config\", key, value)\n",
    "\n",
    "    def cache_clear(self):\n",
    "        \"\"\"Clear all caches of all find functions.\n",
    "        Useful when you want to see the updates to your database.\"\"\"\n",
    "        self._find.cache_clear()\n",
    "        self.find_all.cache_clear()\n",
    "        self.find_by_id.cache_clear()\n",
    "        self.find_by_ids.cache_clear()\n",
    "        self.find_by_name.cache_clear()\n",
    "        self.find_latest.cache_clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = LakeExperimentLoader(\n",
    "    dremio_access, table_context, bucket_name, experiments_dir, experiment_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex1 = loader.find_by_id(1)\n",
    "assert len(ex1.metrics) == 2\n",
    "assert type(ex1.metrics) == pd.DataFrame\n",
    "assert len(ex1.artifacts.values()) == 2\n",
    "assert all([type(art) == CSVArtifact for art in ex1.artifacts.values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    ex_ids = (1,)\n",
    "    exs = loader.find_by_ids(ex_ids)\n",
    "except ValueError:\n",
    "    pass\n",
    "ex_ids = (1, 3)\n",
    "assert len(loader.find_by_ids(ex_ids)) == 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert [int(ex.experiment_id) for ex in loader.find_latest()] == [5, 4, 3, 2, 1]\n",
    "assert [int(ex.experiment_id) for ex in loader.find_latest(n=2)] == [5, 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(loader.find_all()) == 5\n",
    "assert sorted([int(ex.experiment_id) for ex in loader.find_all()]) == [1, 2, 3, 4, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 8.34 Âµs\n"
     ]
    }
   ],
   "source": [
    "%time assert len(loader.find_all()) == 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader.cache_clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 248 ms, sys: 24 ms, total: 272 ms\n",
      "Wall time: 1.44 s\n"
     ]
    }
   ],
   "source": [
    "%time assert len(loader.find_all()) == 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR IS: `ODBC error\n",
      "state: HY000\n",
      "native error code: 1040\n",
      "message: [Dremio][Connector] (1040) Dremio failed to execute the query: select * from \"chatbot_unpublish_s3\".discovery.experiments.test.laketest.runs\n",
      "[30038]Query execution error. Details:[ \n",
      "VALIDATION ERROR: Table 'chatbot_unpublish_s3.discovery.experiments.test.laketest.runs' not found\n",
      "\n",
      "SQL Query select * from \"chatbot_unpublish_s3\".discovery.experiments.test.laketest.runs\n",
      "startLine 1\n",
      "startColumn 15\n",
      "endLine 1\n",
      "endColumn 36\n",
      "\n",
      "[Error Id: f5f5...[see log]`\n",
      "Table not found\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    assert len(loader.find_by_name(\"laketest\")) is None\n",
    "except DatabaseError:\n",
    "    print(\"Table not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(loader.find_by_name(\"lake_observer\")) == 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(loader.find_by_config_key(\"recipient\", \"test\")) == 5\n",
    "assert loader.find_by_config_key(\"recipient\", \"hello\") is None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(loader.find_by_key(\"experiment\", \"name\", \"test-lake-obs\")) == 5\n",
    "assert loader.find_by_key(\"experiment\", \"mainfile\", \"extest.py\") is None"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:jovyan-discovery]",
   "language": "python",
   "name": "conda-env-jovyan-discovery-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
