{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp experiment.lake_experiment_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sacred Data Lake Experiment Loader\n",
    "\n",
    "> This class extends the `incense` project to allow you to load `sacred` experiments from a data lake store such as S3. It is assumed that there exists a ODBC SQL driver for this lake source.\n",
    "\n",
    "> NOTE: initially this class supports S3 & turbodbc only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "import json\n",
    "import os\n",
    "import uuid\n",
    "from functools import lru_cache\n",
    "from typing import Tuple\n",
    "\n",
    "import boto3\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.io.sql import DatabaseError\n",
    "from sciflow.experiment.lake_experiment import CSVArtifact, LakeExperiment\n",
    "from sciflow.s3_utils import delete_dir\n",
    "from sciflow.utils import odbc_connect, prepare_env, query\n",
    "from tinydb import Query, TinyDB\n",
    "from tinydb.storages import MemoryStorage\n",
    "\n",
    "MAX_CACHE_SIZE = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = \"lake_experiment_loader\"\n",
    "project = \"sciflow\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare_env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.resource(\"s3\")\n",
    "delete_dir(s3, os.environ[\"SCIFLOW_BUCKET\"], f\"sciflow/experiments/{experiment_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class LakeExpLoader:\n",
    "    def __init__(\n",
    "        self,\n",
    "        project,\n",
    "        experiment_name,\n",
    "        experiments_key_prefix=None,\n",
    "        connection=None,\n",
    "        bucket_name=None,\n",
    "        bucket_table_alias=None,\n",
    "    ):\n",
    "        self.project = project\n",
    "        self.experiment_name = experiment_name\n",
    "        self.connection = odbc_connect() if connection is None else connection\n",
    "        self.bucket_name = (\n",
    "            os.environ[\"SCIFLOW_BUCKET\"] if bucket_name is None else bucket_name\n",
    "        )\n",
    "        self.bucket_table_alias = (\n",
    "            os.environ[\"SCIFLOW_BUCKET_TABLE_ALIAS\"]\n",
    "            if bucket_table_alias is None\n",
    "            else bucket_table_alias\n",
    "        )\n",
    "        self.experiments_key_prefix = (\n",
    "            f\"{project}/experiments\"\n",
    "            if experiments_key_prefix is None\n",
    "            else experiments_key_prefix\n",
    "        )\n",
    "        table_path = self.experiments_key_prefix.replace(\"/\", \".\")\n",
    "        self.table_context = f\"{self.bucket_table_alias}.{table_path}\"\n",
    "        self.remote_path = (\n",
    "            f\"{self.bucket_name}/{self.experiments_key_prefix}/{self.experiment_name}\"\n",
    "        )\n",
    "        self.lake_table = f\"{self.table_context}.{self.experiment_name}\"\n",
    "\n",
    "    @lru_cache(maxsize=MAX_CACHE_SIZE)\n",
    "    def _find(\n",
    "        self,\n",
    "        experiment_name=None,\n",
    "        experiment_ids=None,\n",
    "        experiment_id: str = None,\n",
    "        order_by: str = None,\n",
    "        limit: int = None,\n",
    "    ) -> LakeExperiment:\n",
    "        if experiment_name is None:\n",
    "            experiment_name = self.experiment_name\n",
    "        table_name = f\"{self.table_context}.{experiment_name}.runs\"\n",
    "        # TODO Dremio Specific code in utils.py\n",
    "        data = query(self.connection, f\"ALTER TABLE {table_name} REFRESH METADATA\")\n",
    "\n",
    "        query_stmt = f\"select * from {table_name}\"\n",
    "        if experiment_ids:\n",
    "            \", \".join([str(i) for i in experiment_ids])\n",
    "            query_stmt += (\n",
    "                f\" where dir0 IN {tuple('{}'.format(x) for x in experiment_ids)}\"\n",
    "            )\n",
    "        if experiment_id:\n",
    "            query_stmt += f\" where dir0 = '{experiment_id}'\"\n",
    "        if order_by:\n",
    "            query_stmt += f\" order by {order_by} desc\"\n",
    "        if limit:\n",
    "            query_stmt += f\" limit {limit}\"\n",
    "        data = query(self.connection, query_stmt)\n",
    "        experiments = [\n",
    "            LakeExperiment(\n",
    "                self.bucket_name,\n",
    "                self.experiments_key_prefix,\n",
    "                experiment_name,\n",
    "                ex_id,\n",
    "                data.iloc[i, :].to_dict()[\"start_time\"],\n",
    "                data.iloc[i, :].to_dict(),\n",
    "            )\n",
    "            for i, ex_id in enumerate(data.dir0.tolist())\n",
    "        ]\n",
    "        return experiments\n",
    "\n",
    "    @lru_cache(maxsize=MAX_CACHE_SIZE)\n",
    "    def find_by_id(self, experiment_id):\n",
    "        experiments = self._find(experiment_id=experiment_id)\n",
    "        return None if len(experiments) == 0 else experiments[0]\n",
    "\n",
    "    @lru_cache(maxsize=MAX_CACHE_SIZE)\n",
    "    def find_by_ids(self, experiment_ids: Tuple[str]):\n",
    "        if len(experiment_ids) == 1:\n",
    "            raise ValueError(\"Use find_by_id for a single experiment\")\n",
    "        return self._find(experiment_ids=experiment_ids)\n",
    "\n",
    "    @lru_cache(maxsize=MAX_CACHE_SIZE)\n",
    "    def find_latest(self, n=5):\n",
    "        return self._find(order_by=\"start_time\", limit=n)\n",
    "\n",
    "    @lru_cache(maxsize=MAX_CACHE_SIZE)\n",
    "    def find_all(self):\n",
    "        return self._find()\n",
    "\n",
    "    @lru_cache(maxsize=MAX_CACHE_SIZE)\n",
    "    def find_by_name(self, experiment_name):\n",
    "        result = None\n",
    "        try:\n",
    "            result = self._find(experiment_name=experiment_name)\n",
    "        except PermissionError:\n",
    "            print(f\"File not found or access not granted; check path information\")\n",
    "        return result\n",
    "\n",
    "    def insert_docs(self, db, prop_name):\n",
    "        experiments = self.find_all()\n",
    "        for ex in experiments:\n",
    "            document = json.loads(ex._data[prop_name])\n",
    "            document[\"experiment_id\"] = ex.experiment_id\n",
    "            db.insert(document)\n",
    "\n",
    "    def find_by_key(self, prop_name, key, value):\n",
    "        db = TinyDB(storage=MemoryStorage)\n",
    "        self.insert_docs(db, prop_name)\n",
    "        Experiment = Query()\n",
    "        docs = list(db.search(Experiment[key] == value))\n",
    "        if len(docs) == 0:\n",
    "            return None\n",
    "        if len(docs) == 1:\n",
    "            return self.find_by_id(docs[0][\"experiment_id\"])\n",
    "        return self.find_by_ids(tuple(d[\"experiment_id\"] for d in docs))\n",
    "\n",
    "    def find_by_config_key(self, key, value):\n",
    "        return self.find_by_key(\"config\", key, value)\n",
    "\n",
    "    def cache_clear(self):\n",
    "        \"\"\"Clear all caches of all find functions.\n",
    "        Useful when you want to see the updates to your database.\"\"\"\n",
    "        self._find.cache_clear()\n",
    "        self.find_all.cache_clear()\n",
    "        self.find_by_id.cache_clear()\n",
    "        self.find_by_ids.cache_clear()\n",
    "        self.find_by_name.cache_clear()\n",
    "        self.find_latest.cache_clear()\n",
    "\n",
    "    def __repr__(self):\n",
    "        return (\n",
    "            f\"Project: {self.project}\\n\"\n",
    "            f\"Experiment: {self.experiment_name}\\n\"\n",
    "            f\"Remote Path: {self.remote_path}\\n\"\n",
    "            f\"Lake Table: {self.lake_table}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = LakeExpLoader(project=project, experiment_name=experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Project: sciflow\n",
       "Experiment: lake_experiment_loader\n",
       "Remote Path: pprsandboxpdlras3/sciflow/experiments/lake_experiment_loader\n",
       "Lake Table: ra_s3.sciflow.experiments.lake_experiment_loader"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (\n",
    "    loader.remote_path\n",
    "    == f\"{os.environ['SCIFLOW_BUCKET']}/{project}/experiments/{experiment_name}\"\n",
    ")\n",
    "assert (\n",
    "    loader.lake_table\n",
    "    == f\"{os.environ['SCIFLOW_BUCKET_TABLE_ALIAS']}.{project}.experiments.{experiment_name}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_loader = LakeExpLoader(\n",
    "    project, f\"generated_experiment_name_{np.random.randint(10**5)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Project: sciflow\n",
       "Experiment: generated_experiment_name_47624\n",
       "Remote Path: pprsandboxpdlras3/sciflow/experiments/generated_experiment_name_47624\n",
       "Lake Table: ra_s3.sciflow.experiments.generated_experiment_name_47624"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Experiments\n",
    "from sacred import Experiment\n",
    "from sacred.run import Run\n",
    "from sciflow.experiment.lake_observer import AWSLakeObserver\n",
    "\n",
    "ex = Experiment(experiment_name, interactive=True)\n",
    "\n",
    "obs = AWSLakeObserver(\n",
    "    project, experiment_name=experiment_name, bucket_name=os.environ[\"SCIFLOW_BUCKET\"]\n",
    ")\n",
    "\n",
    "ex.observers.append(obs)\n",
    "\n",
    "\n",
    "@ex.config\n",
    "def my_config():\n",
    "    recipient = \"test\"\n",
    "    message = f\"Hello {recipient}!\"\n",
    "    f\"{message}\"\n",
    "\n",
    "\n",
    "@ex.main\n",
    "def my_main(message, _run: Run):\n",
    "    _run.add_artifact(\"test/requirements-generated.txt\")\n",
    "    _run.add_artifact(\"test/dataframe_artifact.csv\")\n",
    "    _run.log_scalar(\"another one\", 9.12, 0)\n",
    "    print(message)\n",
    "\n",
    "\n",
    "sample_id_1 = str(uuid.uuid4()).replace(\"-\", \"_\")\n",
    "sample_id_2 = str(uuid.uuid4()).replace(\"-\", \"_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - lake_experiment_loader - Running command 'my_main'\n",
      "INFO - lake_experiment_loader - Started\n",
      "INFO - lake_experiment_loader - Completed after 0:00:01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello test!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - lake_experiment_loader - Running command 'my_main'\n",
      "INFO - lake_experiment_loader - Started\n",
      "INFO - lake_experiment_loader - Completed after 0:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello test!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<sacred.run.Run at 0x7f830d89f8b0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex.run(meta_info={\"run_id\": sample_id_1})\n",
    "ex.run(meta_info={\"run_id\": sample_id_2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sagemaker-user/.local/lib/python3.9/site-packages/pandas/io/sql.py:761: UserWarning: pandas only support SQLAlchemy connectable(engine/connection) ordatabase string URI or sqlite3 DBAPI2 connectionother DBAPI2 objects are not tested, please consider using SQLAlchemy\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    missing_loader.find_all()\n",
    "    # TODO clean up error messaging\n",
    "except DatabaseError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sagemaker-user/.local/lib/python3.9/site-packages/pandas/io/sql.py:761: UserWarning: pandas only support SQLAlchemy connectable(engine/connection) ordatabase string URI or sqlite3 DBAPI2 connectionother DBAPI2 objects are not tested, please consider using SQLAlchemy\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "assert loader.find_by_id(1) is None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sagemaker-user/.local/lib/python3.9/site-packages/pandas/io/sql.py:761: UserWarning: pandas only support SQLAlchemy connectable(engine/connection) ordatabase string URI or sqlite3 DBAPI2 connectionother DBAPI2 objects are not tested, please consider using SQLAlchemy\n",
      "  warnings.warn(\n",
      "/home/sagemaker-user/.local/lib/python3.9/site-packages/pandas/io/sql.py:761: UserWarning: pandas only support SQLAlchemy connectable(engine/connection) ordatabase string URI or sqlite3 DBAPI2 connectionother DBAPI2 objects are not tested, please consider using SQLAlchemy\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "ex1 = loader.find_by_id(sample_id_1)\n",
    "assert len(ex1.metrics) == 1\n",
    "assert ex1.metrics[\"values\"].iloc[0] == 9.12\n",
    "assert type(ex1.metrics) == pd.DataFrame\n",
    "assert len(ex1.artifacts.values()) == 2\n",
    "assert all([type(art) == CSVArtifact for art in ex1.artifacts.values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sagemaker-user/.local/lib/python3.9/site-packages/pandas/io/sql.py:761: UserWarning: pandas only support SQLAlchemy connectable(engine/connection) ordatabase string URI or sqlite3 DBAPI2 connectionother DBAPI2 objects are not tested, please consider using SQLAlchemy\n",
      "  warnings.warn(\n",
      "/home/sagemaker-user/.local/lib/python3.9/site-packages/pandas/io/sql.py:761: UserWarning: pandas only support SQLAlchemy connectable(engine/connection) ordatabase string URI or sqlite3 DBAPI2 connectionother DBAPI2 objects are not tested, please consider using SQLAlchemy\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    ex_ids = (sample_id_1,)\n",
    "    exs = loader.find_by_ids(ex_ids)\n",
    "except ValueError:\n",
    "    pass\n",
    "ex_ids = (sample_id_1, sample_id_2)\n",
    "assert len(loader.find_by_ids(ex_ids)) == 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sagemaker-user/.local/lib/python3.9/site-packages/pandas/io/sql.py:761: UserWarning: pandas only support SQLAlchemy connectable(engine/connection) ordatabase string URI or sqlite3 DBAPI2 connectionother DBAPI2 objects are not tested, please consider using SQLAlchemy\n",
      "  warnings.warn(\n",
      "/home/sagemaker-user/.local/lib/python3.9/site-packages/pandas/io/sql.py:761: UserWarning: pandas only support SQLAlchemy connectable(engine/connection) ordatabase string URI or sqlite3 DBAPI2 connectionother DBAPI2 objects are not tested, please consider using SQLAlchemy\n",
      "  warnings.warn(\n",
      "/home/sagemaker-user/.local/lib/python3.9/site-packages/pandas/io/sql.py:761: UserWarning: pandas only support SQLAlchemy connectable(engine/connection) ordatabase string URI or sqlite3 DBAPI2 connectionother DBAPI2 objects are not tested, please consider using SQLAlchemy\n",
      "  warnings.warn(\n",
      "/home/sagemaker-user/.local/lib/python3.9/site-packages/pandas/io/sql.py:761: UserWarning: pandas only support SQLAlchemy connectable(engine/connection) ordatabase string URI or sqlite3 DBAPI2 connectionother DBAPI2 objects are not tested, please consider using SQLAlchemy\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "assert [ex.experiment_id for ex in loader.find_latest()] == [sample_id_2, sample_id_1]\n",
    "assert [ex.experiment_id for ex in loader.find_latest(n=1)] == [sample_id_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sagemaker-user/.local/lib/python3.9/site-packages/pandas/io/sql.py:761: UserWarning: pandas only support SQLAlchemy connectable(engine/connection) ordatabase string URI or sqlite3 DBAPI2 connectionother DBAPI2 objects are not tested, please consider using SQLAlchemy\n",
      "  warnings.warn(\n",
      "/home/sagemaker-user/.local/lib/python3.9/site-packages/pandas/io/sql.py:761: UserWarning: pandas only support SQLAlchemy connectable(engine/connection) ordatabase string URI or sqlite3 DBAPI2 connectionother DBAPI2 objects are not tested, please consider using SQLAlchemy\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "assert len(loader.find_all()) == 2\n",
    "assert sorted([ex.experiment_id for ex in loader.find_all()]) == sorted(\n",
    "    [sample_id_2, sample_id_1]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table not found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sagemaker-user/.local/lib/python3.9/site-packages/pandas/io/sql.py:761: UserWarning: pandas only support SQLAlchemy connectable(engine/connection) ordatabase string URI or sqlite3 DBAPI2 connectionother DBAPI2 objects are not tested, please consider using SQLAlchemy\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    assert len(loader.find_by_name(\"laketest\")) is None\n",
    "except DatabaseError:\n",
    "    print(\"Table not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sagemaker-user/.local/lib/python3.9/site-packages/pandas/io/sql.py:761: UserWarning: pandas only support SQLAlchemy connectable(engine/connection) ordatabase string URI or sqlite3 DBAPI2 connectionother DBAPI2 objects are not tested, please consider using SQLAlchemy\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "assert len(loader.find_by_name(experiment_name)) == 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sagemaker-user/.local/lib/python3.9/site-packages/pandas/io/sql.py:761: UserWarning: pandas only support SQLAlchemy connectable(engine/connection) ordatabase string URI or sqlite3 DBAPI2 connectionother DBAPI2 objects are not tested, please consider using SQLAlchemy\n",
      "  warnings.warn(\n",
      "/home/sagemaker-user/.local/lib/python3.9/site-packages/pandas/io/sql.py:761: UserWarning: pandas only support SQLAlchemy connectable(engine/connection) ordatabase string URI or sqlite3 DBAPI2 connectionother DBAPI2 objects are not tested, please consider using SQLAlchemy\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "assert len(loader.find_by_config_key(\"recipient\", \"test\")) == 2\n",
    "assert loader.find_by_config_key(\"recipient\", \"hello\") is None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(loader.find_by_key(\"experiment\", \"name\", \"lake_experiment_loader\")) == 2\n",
    "assert loader.find_by_key(\"experiment\", \"mainfile\", \"extest.py\") is None"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "sciflow (sciflow/3)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:eu-west-1:368653567616:image-version/sciflow/3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
