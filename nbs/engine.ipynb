{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp experiment.engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment Query Engine\n",
    "\n",
    "> This class extends the `incense` base_key to allow you to load `sacred` experiments from a data lake store such as S3. It is assumed that there exists a ODBC SQL driver for this lake source.\n",
    "\n",
    "> NOTE: initially this class supports S3 & turbodbc only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "import datetime\n",
    "import json\n",
    "import os\n",
    "import tempfile\n",
    "import uuid\n",
    "from functools import lru_cache\n",
    "from typing import Tuple\n",
    "\n",
    "import boto3\n",
    "import numpy as np\n",
    "from pandas.io.sql import DatabaseError\n",
    "from tinydb import Query, TinyDB\n",
    "from tinydb.storages import MemoryStorage\n",
    "\n",
    "from sciflow.experiment.lake_experiment import (\n",
    "    CSVArtifact,\n",
    "    ImageArtifact,\n",
    "    LakeExperiment,\n",
    ")\n",
    "from sciflow.experiment.tracking import FlowTracker, StepTracker\n",
    "from sciflow.utils import odbc_connect, prepare_env, query\n",
    "\n",
    "MAX_CACHE_SIZE = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare_env()\n",
    "_bucket_name = os.environ[\"SCIFLOW_BUCKET\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "today = datetime.datetime.utcnow().strftime(\"%Y%m%d\")\n",
    "_base_key = f\"sciflow-engine-testing-{today}\"\n",
    "_run_id = f\"engine_{str(uuid.uuid4())[-6:]}\"\n",
    "_s3_res = boto3.resource(\"s3\")\n",
    "_s3_client = boto3.client(\"s3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib auto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment_data = create_experiment_test_data(_s3_res, _s3_client, _bucket_name, _base_key, _run_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "def create_test_flow_run(run_id, run_name=None):\n",
    "    flow_tracker = FlowTracker(\n",
    "        _bucket_name, _base_key, run_id, [\"engine-test-1\"], run_name=run_name\n",
    "    )\n",
    "    flow_tracker.start()\n",
    "    tracker = StepTracker(_bucket_name, _base_key, run_id, \"engine-test-1\")\n",
    "    with tracker.capture_out() as tracker._output_file:\n",
    "        tracker.log_metric(\"recall\", 0.87, 0)\n",
    "        df = pd.DataFrame({\"a\": [1, 2, 3], \"b\": [\"a\", \"b\", \"c\"]})\n",
    "        with tempfile.TemporaryDirectory() as temp_dir:\n",
    "            csv_path = f\"{temp_dir}/testfile.csv\"\n",
    "            df.to_csv(csv_path)\n",
    "            tracker.add_artifact(csv_path)\n",
    "            fig = df.a.plot.hist().figure\n",
    "            png_path = f\"{temp_dir}/testfile.png\"\n",
    "            fig.savefig(png_path)\n",
    "            tracker.add_artifact(png_path)\n",
    "        tracker.completed()\n",
    "    flow_tracker.completed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_run_id_2 = f\"engine_{str(uuid.uuid4())[-6:]}\"\n",
    "\n",
    "create_test_flow_run(_run_id)\n",
    "create_test_flow_run(_run_id_2, \"engine-test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class ExperimentEngine:\n",
    "    def __init__(\n",
    "        self,\n",
    "        base_key,\n",
    "        experiments_key=None,\n",
    "        connection=None,\n",
    "        bucket_name=None,\n",
    "        bucket_table_alias=None,\n",
    "    ):\n",
    "        self.base_key = '\"' + base_key + '\"'\n",
    "        self.connection = odbc_connect() if connection is None else connection\n",
    "        self.bucket_name = (\n",
    "            os.environ[\"SCIFLOW_BUCKET\"] if bucket_name is None else bucket_name\n",
    "        )\n",
    "        self.bucket_table_alias = (\n",
    "            os.environ[\"SCIFLOW_BUCKET_TABLE_ALIAS\"]\n",
    "            if bucket_table_alias is None\n",
    "            else bucket_table_alias\n",
    "        )\n",
    "        self.experiments_key = (\n",
    "            f\"{base_key}/experiments\" if experiments_key is None else experiments_key\n",
    "        )\n",
    "        table_path = f\"{self.base_key}.experiments\"\n",
    "        self.table_context = f\"{self.bucket_table_alias}.{table_path}\"\n",
    "        self.remote_path = f\"{self.bucket_name}/{self.experiments_key}\"\n",
    "        self.lake_table = f\"{self.table_context}\"\n",
    "\n",
    "    @lru_cache(maxsize=MAX_CACHE_SIZE)\n",
    "    def _find(\n",
    "        self,\n",
    "        experiment_ids=None,\n",
    "        experiment_id: str = None,\n",
    "        experiment_name: str = None,\n",
    "        order_by: str = None,\n",
    "        limit: int = None,\n",
    "    ) -> LakeExperiment:\n",
    "        table_name = f\"{self.table_context}.runs\"\n",
    "        # TODO Dremio Specific code in utils.py\n",
    "        data = query(self.connection, f\"ALTER TABLE {table_name} REFRESH METADATA\")\n",
    "\n",
    "        query_stmt = f\"select * from {table_name}\"\n",
    "        if experiment_ids:\n",
    "            \", \".join([str(i) for i in experiment_ids])\n",
    "            query_stmt += (\n",
    "                f\" where dir0 IN {tuple('{}'.format(x) for x in experiment_ids)}\"\n",
    "            )\n",
    "        if experiment_id:\n",
    "            query_stmt += f\" where dir0 = '{str(experiment_id)}'\"\n",
    "        elif experiment_name:\n",
    "            query_stmt += f\" where experiment_name = '{experiment_name}'\"\n",
    "        if order_by:\n",
    "            query_stmt += f\" order by {order_by} desc\"\n",
    "        if limit:\n",
    "            query_stmt += f\" limit {limit}\"\n",
    "        data = query(self.connection, query_stmt)\n",
    "        experiments = [\n",
    "            LakeExperiment(\n",
    "                self.bucket_name,\n",
    "                self.experiments_key,\n",
    "                ex_id,\n",
    "                data.iloc[i, :].to_dict()[\"start_time\"],\n",
    "                data.iloc[i, :].to_dict(),\n",
    "                experiment_name,\n",
    "            )\n",
    "            for i, ex_id in enumerate(data.dir0.tolist())\n",
    "        ]  # bucket_name, base_key, experiment_id, start_time, data, name\n",
    "        return experiments\n",
    "\n",
    "    @lru_cache(maxsize=MAX_CACHE_SIZE)\n",
    "    def find_by_id(self, experiment_id):\n",
    "        experiments = self._find(experiment_id=str(experiment_id))\n",
    "        return None if len(experiments) == 0 else experiments[0]\n",
    "\n",
    "    @lru_cache(maxsize=MAX_CACHE_SIZE)\n",
    "    def find_by_ids(self, experiment_ids: Tuple[str]):\n",
    "        if len(experiment_ids) == 1:\n",
    "            raise ValueError(\"Use find_by_id for a single experiment\")\n",
    "        return self._find(experiment_ids=experiment_ids)\n",
    "\n",
    "    @lru_cache(maxsize=MAX_CACHE_SIZE)\n",
    "    def find_latest(self, n=5):\n",
    "        return self._find(order_by=\"start_time\", limit=n)\n",
    "\n",
    "    @lru_cache(maxsize=MAX_CACHE_SIZE)\n",
    "    def find_all(self):\n",
    "        return self._find()\n",
    "\n",
    "    @lru_cache(maxsize=MAX_CACHE_SIZE)\n",
    "    def find_by_name(self, name):\n",
    "        result = None\n",
    "        try:\n",
    "            result = self._find(experiment_name=name)\n",
    "        except PermissionError:\n",
    "            print(f\"File not found or access not granted; check path information\")\n",
    "        return result\n",
    "\n",
    "    def insert_docs(self, db, prop_name):\n",
    "        experiments = self.find_all()\n",
    "        for ex in experiments:\n",
    "            document = json.loads(ex._data[prop_name])\n",
    "            document[\"experiment_id\"] = ex.experiment_id\n",
    "            db.insert(document)\n",
    "\n",
    "    def find_by_key(self, prop_name, key, value):\n",
    "        db = TinyDB(storage=MemoryStorage)\n",
    "        self.insert_docs(db, prop_name)\n",
    "        Experiment = Query()\n",
    "        docs = list(db.search(Experiment[key] == value))\n",
    "        if len(docs) == 0:\n",
    "            return None\n",
    "        if len(docs) == 1:\n",
    "            return self.find_by_id(docs[0][\"experiment_id\"])\n",
    "        return self.find_by_ids(tuple(d[\"experiment_id\"] for d in docs))\n",
    "\n",
    "    def find_by_config_key(self, key, value):\n",
    "        return self.find_by_key(\"config\", key, value)\n",
    "\n",
    "    def cache_clear(self):\n",
    "        \"\"\"Clear all caches of all find functions.\n",
    "        Useful when you want to see the updates to your database.\"\"\"\n",
    "        self._find.cache_clear()\n",
    "        self.find_all.cache_clear()\n",
    "        self.find_by_id.cache_clear()\n",
    "        self.find_by_ids.cache_clear()\n",
    "        self.find_by_name.cache_clear()\n",
    "        self.find_latest.cache_clear()\n",
    "\n",
    "    def __repr__(self):\n",
    "        return (\n",
    "            f\"Base Key: {self.base_key}\\n\"\n",
    "            f\"Remote Path: {self.remote_path}\\n\"\n",
    "            f\"Lake Table: {self.lake_table}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = ExperimentEngine(base_key=_base_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert engine.remote_path == f\"{os.environ['SCIFLOW_BUCKET']}/{_base_key}/experiments\"\n",
    "assert (\n",
    "    engine.lake_table\n",
    "    == f\"{os.environ['SCIFLOW_BUCKET_TABLE_ALIAS']}.\\\"{_base_key}\\\".experiments\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(engine._find()) > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_loader = ExperimentEngine(\n",
    "    _base_key, f\"generated_experiment_name_{np.random.randint(10**5)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    missing_loader.find_all()\n",
    "    # TODO clean up error messaging\n",
    "except DatabaseError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert engine.find_by_id(\"123\") is None\n",
    "assert engine.find_by_id(123) is None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert engine.find_by_id(_run_id).experiment_id == _run_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex1 = engine.find_by_id(_run_id)\n",
    "assert len(ex1.metrics) == 1\n",
    "assert ex1.metrics[\"recall\"].iloc[0] == 0.87\n",
    "assert type(ex1.metrics) == dict\n",
    "artifact_types = [type(t) for t in ex1.artifacts.values()]\n",
    "assert CSVArtifact in artifact_types\n",
    "assert ImageArtifact in artifact_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    ex_ids = (_run_id,)\n",
    "    exs = engine.find_by_ids(ex_ids)\n",
    "except ValueError:\n",
    "    pass\n",
    "ex_ids = (_run_id, _run_id_2)\n",
    "assert len(engine.find_by_ids(ex_ids)) == 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert [ex.experiment_id for ex in engine.find_latest()][:2] == [_run_id_2, _run_id]\n",
    "assert [ex.experiment_id for ex in engine.find_latest(n=1)] == [_run_id_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(engine.find_all()) >= 2\n",
    "tracked_ids = set([_run_id_2, _run_id])\n",
    "tracked_intersect = list(\n",
    "    tracked_ids.intersection(set([ex.experiment_id for ex in engine.find_all()]))\n",
    ")\n",
    "assert len(tracked_intersect) == 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    assert len(engine.find_by_name(\"laketest\")) == 0\n",
    "except DatabaseError:\n",
    "    print(\"Table not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(engine.find_by_name(\"engine-test\")) >= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assert len(engine.find_by_config_key(\"test_set_size\", \"25\")) == 2\n",
    "# assert engine.find_by_config_key(\"test_set_size\", \"hello\") is None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assert engine.find_by_key(\"experiment\", \"name\", \"engine-test\").experiment_id == _run_id_2\n",
    "# assert engine.find_by_key(\"experiment\", \"name\", \"blabla\") is None"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sciflow (sciflow/3)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:eu-west-1:368653567616:image-version/sciflow/3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
