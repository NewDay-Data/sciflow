{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp experiment.lake_experiment_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sacred Data Lake Experiment Loader\n",
    "\n",
    "> This class extends the `incense` project to allow you to load `sacred` experiments from a data lake store such as S3. It is assumed that there exists a ODBC SQL driver for this lake source.\n",
    "\n",
    "> NOTE: initially this class supports S3 & turbodbc only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "import json\n",
    "from functools import lru_cache\n",
    "from typing import Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from incense.artifact import CSVArtifact\n",
    "from incense.experiment import Experiment\n",
    "from nbdev import Config\n",
    "from sciflow.experiment.lake_experiment import LakeExperiment\n",
    "from sciflow.utils import odbc_connect, query_odbc\n",
    "from tinydb import Query, TinyDB\n",
    "from tinydb.storages import MemoryStorage\n",
    "from turbodbc.exceptions import DatabaseError\n",
    "\n",
    "MAX_CACHE_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class LakeExpLoader:\n",
    "    def __init__(\n",
    "        self,\n",
    "        experiment_name,\n",
    "        experiments_key_prefix=None,\n",
    "        connection=None,\n",
    "        bucket_name=None,\n",
    "        bucket_table_alias=None,\n",
    "    ):\n",
    "        config = Config()\n",
    "        lib_name = config.lib_name\n",
    "        self.experiment_name = experiment_name\n",
    "        self.connection = odbc_connect() if connection is None else connection\n",
    "        self.bucket_name = config.bucket if bucket_name is None else bucket_name\n",
    "        self.bucket_table_alias = (\n",
    "            config.bucket_table_alias\n",
    "            if bucket_table_alias is None\n",
    "            else bucket_table_alias\n",
    "        )\n",
    "        self.experiments_key_prefix = (\n",
    "            f\"{lib_name}/experiments\"\n",
    "            if experiments_key_prefix is None\n",
    "            else experiments_key_prefix\n",
    "        )\n",
    "        table_path = self.experiments_key_prefix.replace(\"/\", \".\")\n",
    "        self.table_context = f\"{self.bucket_table_alias}.{table_path}\"\n",
    "        self.remote_path = (\n",
    "            f\"{self.bucket_name}/{self.experiments_key_prefix}/{self.experiment_name}\"\n",
    "        )\n",
    "        self.lake_table = f\"{self.table_context}.{self.experiment_name}\"\n",
    "\n",
    "    @lru_cache(maxsize=MAX_CACHE_SIZE)\n",
    "    def _find(\n",
    "        self,\n",
    "        experiment_name=None,\n",
    "        experiment_ids=None,\n",
    "        experiment_id: int = None,\n",
    "        order_by: str = None,\n",
    "        limit: int = None,\n",
    "    ) -> Experiment:\n",
    "        if experiment_name is None:\n",
    "            experiment_name = self.experiment_name\n",
    "        query = f\"select * from {self.table_context}.{experiment_name}.runs\"\n",
    "        if experiment_ids:\n",
    "            ids = \", \".join([str(i) for i in experiment_ids])\n",
    "            query += f\" where dir0 IN ({ids})\"\n",
    "        if experiment_id:\n",
    "            query += f\" where dir0 = {experiment_id}\"\n",
    "        if order_by:\n",
    "            query += f\" order by {order_by} desc\"\n",
    "        if limit:\n",
    "            query += f\" limit {limit}\"\n",
    "        data = query_odbc(self.connection, query)\n",
    "        experiments = [\n",
    "            LakeExperiment(\n",
    "                self.bucket_name,\n",
    "                self.experiments_key_prefix,\n",
    "                experiment_name,\n",
    "                ex_id,\n",
    "                data.iloc[i, :].to_dict(),\n",
    "            )\n",
    "            for i, ex_id in enumerate(data.dir0.tolist())\n",
    "        ]\n",
    "        return experiments\n",
    "\n",
    "    @lru_cache(maxsize=MAX_CACHE_SIZE)\n",
    "    def find_by_id(self, experiment_id):\n",
    "        experiments = self._find(experiment_id=experiment_id)\n",
    "        return None if len(experiments) == 0 else experiments[0]\n",
    "\n",
    "    @lru_cache(maxsize=MAX_CACHE_SIZE)\n",
    "    def find_by_ids(self, experiment_ids: Tuple[int]):\n",
    "        if len(experiment_ids) == 1:\n",
    "            raise ValueError(\"Use find_by_id for a single experiment\")\n",
    "        return self._find(experiment_ids=experiment_ids)\n",
    "\n",
    "    @lru_cache(maxsize=MAX_CACHE_SIZE)\n",
    "    def find_latest(self, n=5):\n",
    "        return self._find(order_by=\"dir0\", limit=n)\n",
    "\n",
    "    @lru_cache(maxsize=MAX_CACHE_SIZE)\n",
    "    def find_all(self):\n",
    "        return self._find()\n",
    "\n",
    "    @lru_cache(maxsize=MAX_CACHE_SIZE)\n",
    "    def find_by_name(self, experiment_name):\n",
    "        result = None\n",
    "        try:\n",
    "            result = self._find(experiment_name=experiment_name)\n",
    "        except PermissionError:\n",
    "            print(f\"File not found or access not granted; check path information\")\n",
    "        return result\n",
    "\n",
    "    def insert_docs(self, db, prop_name):\n",
    "        experiments = self.find_all()\n",
    "        for ex in experiments:\n",
    "            document = json.loads(ex._data[prop_name])\n",
    "            document[\"experiment_id\"] = ex.experiment_id\n",
    "            db.insert(document)\n",
    "\n",
    "    def find_by_key(self, prop_name, key, value):\n",
    "        db = TinyDB(storage=MemoryStorage)\n",
    "        self.insert_docs(db, prop_name)\n",
    "        Experiment = Query()\n",
    "        docs = list(db.search(Experiment[key] == value))\n",
    "        if len(docs) == 0:\n",
    "            return None\n",
    "        if len(docs) == 1:\n",
    "            return self.find_by_id(docs[0][\"experiment_id\"])\n",
    "        return self.find_by_ids(tuple(d[\"experiment_id\"] for d in docs))\n",
    "\n",
    "    def find_by_config_key(self, key, value):\n",
    "        return self.find_by_key(\"config\", key, value)\n",
    "\n",
    "    def cache_clear(self):\n",
    "        \"\"\"Clear all caches of all find functions.\n",
    "        Useful when you want to see the updates to your database.\"\"\"\n",
    "        self._find.cache_clear()\n",
    "        self.find_all.cache_clear()\n",
    "        self.find_by_id.cache_clear()\n",
    "        self.find_by_ids.cache_clear()\n",
    "        self.find_by_name.cache_clear()\n",
    "        self.find_latest.cache_clear()\n",
    "\n",
    "    def __repr__(self):\n",
    "        return (\n",
    "            f\"Experiment: {self.experiment_name}\\n\"\n",
    "            f\"Remote Path: {self.remote_path}\\n\"\n",
    "            f\"Lake Table: {self.lake_table}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name = \"s3bawspprwe1chatbotunpub01\"  # default = bucket\n",
    "bucket_table_alias = \"chatbot_unpublish_s3\"\n",
    "experiments_key_prefix = \"discovery/experiments/test\"  # default = lib_name/experiments\n",
    "experiment_name = \"lake_observer\"  # default should be flow name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ODBC_DRIVER /opt/dremio-odbc/lib64/libdrillodbc_sb64.so\n",
      "ODBC_USER E02079\n",
      "ODBC_PWD HYgzAG9ASxy1BBUa24jS4VVOL/wrJcyDAIKUdSPv4aD+7jFzvNUyoziBWeqKjg==\n",
      "ODBC_PORT 31010\n",
      "ODBC_HOST dremio-master-0.dremio-cluster-pod.default.svc.cluster.local\n",
      "SSL_CERTS /etc/ssl/certs/ca-certificates.crt\n"
     ]
    }
   ],
   "source": [
    "loader = LakeExpLoader(\n",
    "    experiment_name=experiment_name,\n",
    "    experiments_key_prefix=experiments_key_prefix,\n",
    "    bucket_name=bucket_name,\n",
    "    bucket_table_alias=bucket_table_alias,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Experiment: lake_observer\n",
       "Remote Path: s3bawspprwe1chatbotunpub01/discovery/experiments/test/lake_observer\n",
       "Lake Table: chatbot_unpublish_s3.discovery.experiments.test.lake_observer"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (\n",
    "    loader.remote_path\n",
    "    == \"s3bawspprwe1chatbotunpub01/discovery/experiments/test/lake_observer\"\n",
    ")\n",
    "assert (\n",
    "    loader.lake_table == \"chatbot_unpublish_s3.discovery.experiments.test.lake_observer\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_loader = LakeExpLoader(f\"generated_experiment_name_{np.random.randint(10**5)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Experiment: generated_experiment_name_69049\n",
       "Remote Path: pprsandboxpdlras3/sciflow/experiments/generated_experiment_name_69049\n",
       "Lake Table: ra_s3.sciflow.experiments.generated_experiment_name_69049"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    missing_loader.find_all()\n",
    "    # TODO clean up error messaging\n",
    "except DatabaseError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex1 = loader.find_by_id(1)\n",
    "assert len(ex1.metrics) == 2\n",
    "assert type(ex1.metrics) == pd.DataFrame\n",
    "assert len(ex1.artifacts.values()) == 2\n",
    "assert all([type(art) == CSVArtifact for art in ex1.artifacts.values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    ex_ids = (1,)\n",
    "    exs = loader.find_by_ids(ex_ids)\n",
    "except ValueError:\n",
    "    pass\n",
    "ex_ids = (1, 3)\n",
    "assert len(loader.find_by_ids(ex_ids)) == 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert [int(ex.experiment_id) for ex in loader.find_latest()] == [5, 4, 3, 2, 1]\n",
    "assert [int(ex.experiment_id) for ex in loader.find_latest(n=2)] == [5, 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(loader.find_all()) == 5\n",
    "assert sorted([int(ex.experiment_id) for ex in loader.find_all()]) == [1, 2, 3, 4, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 9.54 µs\n"
     ]
    }
   ],
   "source": [
    "%time assert len(loader.find_all()) == 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader.cache_clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 336 ms, sys: 4 ms, total: 340 ms\n",
      "Wall time: 1.82 s\n"
     ]
    }
   ],
   "source": [
    "%time assert len(loader.find_all()) == 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table not found\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    assert len(loader.find_by_name(\"laketest\")) is None\n",
    "except DatabaseError:\n",
    "    print(\"Table not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(loader.find_by_name(\"lake_observer\")) == 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(loader.find_by_config_key(\"recipient\", \"test\")) == 5\n",
    "assert loader.find_by_config_key(\"recipient\", \"hello\") is None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(loader.find_by_key(\"experiment\", \"name\", \"test-lake-obs\")) == 5\n",
    "assert loader.find_by_key(\"experiment\", \"mainfile\", \"extest.py\") is None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:sciflow]",
   "language": "python",
   "name": "conda-env-sciflow-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
