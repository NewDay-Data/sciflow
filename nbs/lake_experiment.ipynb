{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp experiment.lake_experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import json\n",
    "\n",
    "# export\n",
    "import os\n",
    "import pickle\n",
    "import tempfile\n",
    "import uuid\n",
    "import warnings\n",
    "from copy import copy\n",
    "from pathlib import Path\n",
    "from typing import Dict, Set\n",
    "\n",
    "import boto3\n",
    "import pandas as pd\n",
    "from IPython import display\n",
    "from IPython.display import HTML, IFrame\n",
    "from pyrsistent import freeze, thaw\n",
    "\n",
    "from sciflow.s3_utils import S3File, load_json, put_data, s3_join\n",
    "from sciflow.utils import prepare_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "file_to_mime_type_map = {\n",
    "    \".txt\": \"text/csv\",\n",
    "    \".csv\": \"text/csv\",\n",
    "    \".png\": \"image/png\",\n",
    "    \".jpg\": \"image/jpeg\",\n",
    "    \".mp4\": \"video/mp4\",\n",
    "    \".pickle\": \"application/octet-stream\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "class Artifact:\n",
    "    \"\"\"Displays or saves an artifact.\"\"\"\n",
    "\n",
    "    can_render: Set[str] = set()\n",
    "\n",
    "    def __init__(self, name: str, file, content_type: str = None):\n",
    "        self.name = name\n",
    "        self.file = file\n",
    "        self.content_type = content_type\n",
    "        self.extension = (\n",
    "            \"\" if self.content_type is None else self.content_type.split(\"/\")[-1]\n",
    "        )\n",
    "        self._content = None\n",
    "        self._rendered = None\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"{self.__class__.__name__}(name={self.name})\"\n",
    "\n",
    "    def render(self):\n",
    "        \"\"\"Render the artifact according to its content-type.\"\"\"\n",
    "        if self._rendered is None:\n",
    "            self._rendered = self._render()\n",
    "        return self._rendered\n",
    "\n",
    "    def _render(self):\n",
    "        \"\"\"Return the object that represents the rendered artifact.\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def show(self):\n",
    "        warnings.warn(\n",
    "            \"`show` is deprecated in favor of `render` and will removed in a future release.\",\n",
    "            DeprecationWarning,\n",
    "            stacklevel=2,\n",
    "        )\n",
    "        return self.render()\n",
    "\n",
    "    def save(self, to_dir: str = \"\") -> None:\n",
    "        \"\"\"\n",
    "        Save artifact to disk.\n",
    "\n",
    "        Args:\n",
    "            to_dir: Directory in which to save the artifact. Defaults to the current working directory.\n",
    "\n",
    "        \"\"\"\n",
    "        if to_dir:\n",
    "            os.makedirs(str(to_dir), exist_ok=True)\n",
    "        with open(os.path.join(str(to_dir), self._make_filename()), \"wb\") as file:\n",
    "            file.write(self.content)\n",
    "\n",
    "    def as_content_type(self, content_type: str) -> \"Artifact\":\n",
    "        \"\"\"Interpret artifact as being of content-type.\"\"\"\n",
    "        try:\n",
    "            artifact_type = content_type_to_artifact_cls[content_type]\n",
    "        except KeyError:\n",
    "            raise ValueError(\n",
    "                f\"Incense does not have a class that maps to content-type {content_type}\"\n",
    "            )\n",
    "        else:\n",
    "            return self.as_type(artifact_type)\n",
    "\n",
    "    def as_type(self, artifact_type) -> \"Artifact\":\n",
    "        self.file.seek(0)\n",
    "        return artifact_type(self.name, self.file)\n",
    "\n",
    "    @property\n",
    "    def content(self):\n",
    "        \"\"\"Access the raw bytes of the artifact.\"\"\"\n",
    "        if self._content is None:\n",
    "            self._content = self.file.read()\n",
    "        return self._content\n",
    "\n",
    "    def _make_filename(self):\n",
    "        # TODO does this work on gridfs file?\n",
    "        exp_id, artifact_name = self.file.name.split(\"/\")[-2:]\n",
    "        return f\"{exp_id}_{artifact_name}\" + (\n",
    "            \"\" if artifact_name.endswith(self.extension) else f\".{self.extension}\"\n",
    "        )\n",
    "\n",
    "\n",
    "class ImageArtifact(Artifact):\n",
    "    \"\"\"Displays or saves an image artifact.\"\"\"\n",
    "\n",
    "    can_render = {\"image/png\", \"image/jpeg\"}\n",
    "\n",
    "    def _render(self):\n",
    "        return display.Image(data=self.content)\n",
    "\n",
    "\n",
    "class MP4Artifact(Artifact):\n",
    "    \"\"\"Displays or saves a MP4 artifact\"\"\"\n",
    "\n",
    "    can_render = {\"video/mp4\"}\n",
    "\n",
    "    def _render(self):\n",
    "        self.save()\n",
    "        return HTML(\n",
    "            f\"\"\"\n",
    "        <video width=\"640\" height=\"480\" controls autoplay>\n",
    "          <source src=\"{self._make_filename()}\" type=\"video/mp4\">\n",
    "        </video>\n",
    "        \"\"\"\n",
    "        )\n",
    "\n",
    "\n",
    "class CSVArtifact(Artifact):\n",
    "    \"\"\"Displays and saves a CSV artifact\"\"\"\n",
    "\n",
    "    can_render = {\"text/csv\"}\n",
    "\n",
    "    def _render(self):\n",
    "        return pd.read_csv(self.file)\n",
    "\n",
    "\n",
    "class PickleArtifact(Artifact):\n",
    "    \"\"\"Displays and saves a Pickle artifact\"\"\"\n",
    "\n",
    "    can_render: Set[str] = set()\n",
    "\n",
    "    def __init__(self, name: str, file, content_type: str = None):\n",
    "        super().__init__(name, file, content_type)\n",
    "        self.extension = \"pickle\"\n",
    "\n",
    "    def _render(self):\n",
    "        return pickle.load(self.file)\n",
    "\n",
    "\n",
    "class PDFArtifact(Artifact):\n",
    "    \"\"\"Displays and saves a PDF artifacts.\"\"\"\n",
    "\n",
    "    can_render = {\"application/pdf\"}\n",
    "\n",
    "    # TODO probably needs jupyter extension to be able to display pdf.\n",
    "    def _render(self):\n",
    "        return IFrame(self._make_filename(), width=600, height=300)\n",
    "\n",
    "\n",
    "content_type_to_artifact_cls = {}\n",
    "for cls in copy(locals()).values():\n",
    "    if isinstance(cls, type) and issubclass(cls, Artifact):\n",
    "        for content_type in cls.can_render:\n",
    "            content_type_to_artifact_cls[content_type] = cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "class LakeExperiment:\n",
    "    def __init__(\n",
    "        self, bucket_name, base_key, experiment_id, start_time, data, name=None\n",
    "    ):\n",
    "        self.bucket_name = bucket_name\n",
    "        self.base_key = base_key\n",
    "        self.experiment_id = experiment_id\n",
    "        self.metrics_key = s3_join(self.base_key, \"metrics\", experiment_id)\n",
    "        self.artifacts_key = s3_join(self.base_key, \"artifacts\", experiment_id)\n",
    "        self.start_time = start_time\n",
    "        self._data = freeze(data)\n",
    "        # TODO is a name needed?\n",
    "        # self.name = self.experiment_id if name is None else name\n",
    "        self.name = name\n",
    "        self.s3_res = boto3.resource(\"s3\")\n",
    "        self.bucket = self.s3_res.Bucket(self.bucket_name)\n",
    "        self.artifacts = self._load_artifacts()\n",
    "        self.metrics = self._load_metrics()\n",
    "\n",
    "    def __repr__(self):\n",
    "        if self.name is None:\n",
    "            text = f\"{self.__class__.__name__}(id={self.experiment_id}, start_time={datetime.datetime.fromtimestamp(round(self.start_time))})\"\n",
    "        else:\n",
    "            text = f\"{self.__class__.__name__}(id={self.experiment_id}, name={self.name} start_time={datetime.datetime.fromtimestamp(round(self.start_time))})\"\n",
    "        return text\n",
    "\n",
    "    def __getattr__(self, item):\n",
    "        \"\"\"Try to relay attribute access to easy dict, to allow dotted access.\"\"\"\n",
    "        return getattr(self._data, item)\n",
    "\n",
    "    def to_dict(self) -> dict:\n",
    "        return thaw(self._data)\n",
    "\n",
    "    def _load_artifacts(self) -> Dict[str, Artifact]:\n",
    "        artifacts = {}\n",
    "\n",
    "        artifact_keys = [\n",
    "            obj.key for obj in self.bucket.objects.filter(Prefix=self.artifacts_key)\n",
    "        ]\n",
    "\n",
    "        for artifact_key in artifact_keys:\n",
    "            s3_object = self.s3_res.Object(\n",
    "                bucket_name=self.bucket_name, key=artifact_key\n",
    "            )\n",
    "            artifact_file = S3File(s3_object)\n",
    "            name = os.path.basename(artifact_key)\n",
    "\n",
    "            try:\n",
    "                content_type = file_to_mime_type_map[os.path.splitext(name)[-1]]\n",
    "                artifact_type = content_type_to_artifact_cls[content_type]\n",
    "                artifacts[name] = artifact_type(\n",
    "                    name, artifact_file, content_type=content_type\n",
    "                )\n",
    "            except KeyError:\n",
    "                artifact_type = Artifact\n",
    "                artifacts[name] = artifact_type(name, artifact_file)\n",
    "\n",
    "        return artifacts\n",
    "\n",
    "    def _load_metrics(self) -> Dict[str, pd.Series]:\n",
    "        metrics = {}\n",
    "        metric_keys = self.bucket.objects.filter(Prefix=self.metrics_key)\n",
    "        for metric_key in metric_keys:\n",
    "            metrics_entry = load_json(self.s3_res, self.bucket_name, metric_key.key)\n",
    "            for metric_name in metrics_entry.keys():\n",
    "                metrics[metric_name] = pd.Series(\n",
    "                    data=metrics_entry[metric_name][\"values\"],\n",
    "                    index=pd.Index(metrics_entry[metric_name][\"steps\"], name=\"step\"),\n",
    "                    name=metric_name,\n",
    "                )\n",
    "        return metrics\n",
    "\n",
    "    def metrics_as_df(self):\n",
    "        return pd.concat(self.metrics.values(), axis=1)\n",
    "\n",
    "    def delete(self, confirmed: bool = False):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def _delete(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def _delete_metrics(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def _delete_artifacts(self):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare_env()\n",
    "_bucket_name = os.environ[\"SCIFLOW_BUCKET\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "today = datetime.datetime.utcnow().strftime(\"%Y%m%d\")\n",
    "_base_key = f\"sciflow-experiment-testing-{today}\"\n",
    "_run_id = f\"experiment_{str(uuid.uuid4())[-6:]}\"\n",
    "_run_key = s3_join(_base_key, _run_id)\n",
    "_metrics_key = f\"{_base_key}/experiments/metrics\"\n",
    "_artifacts_key = f\"{_base_key}/experiments/artifacts\"\n",
    "_runs_key = f\"{_base_key}/experiments/runs\"\n",
    "_s3_res = boto3.resource(\"s3\")\n",
    "_s3_client = boto3.client(\"s3\")\n",
    "_bucket = _s3_res.Bucket(_bucket_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib auto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set-up - Create Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "def create_experiment_test_data(\n",
    "    s3_res, s3_client, bucket_name, base_key, experiment_id\n",
    "):\n",
    "    metrics_key = f\"{base_key}/experiments/metrics\"\n",
    "    artifacts_key = f\"{base_key}/experiments/artifacts\"\n",
    "    runs_key = f\"{base_key}/experiments/runs\"\n",
    "\n",
    "    df = pd.DataFrame({\"a\": [1, 2, 3], \"b\": [\"a\", \"b\", \"c\"]})\n",
    "\n",
    "    with tempfile.TemporaryDirectory() as temp_dir:\n",
    "        csv_path = f\"{temp_dir}/testfile.csv\"\n",
    "        df.to_csv(csv_path)\n",
    "        fig = df.a.plot.hist().figure\n",
    "        pdf_path = f\"{temp_dir}/testfile.pdf\"\n",
    "        fig.savefig(pdf_path)\n",
    "        pickle_path = f\"{temp_dir}/testfile.pkl\"\n",
    "        df.to_pickle(pickle_path)\n",
    "        for artifact_path in (csv_path, pdf_path, pickle_path):\n",
    "            s3_client.upload_file(\n",
    "                artifact_path,\n",
    "                bucket_name,\n",
    "                s3_join(artifacts_key, experiment_id, Path(artifact_path).name),\n",
    "            )\n",
    "\n",
    "    metrics = {\n",
    "        \"f1\": {\n",
    "            \"steps\": [\n",
    "                0,\n",
    "                1,\n",
    "            ],\n",
    "            \"timestamps\": [\n",
    "                \"2022-06-09T19:02:33.778171\",\n",
    "                \"2022-06-09T19:02:34.014545\",\n",
    "            ],\n",
    "            \"values\": [0.37, 0.45],\n",
    "        },\n",
    "        \"precision\": {\n",
    "            \"steps\": [\n",
    "                0,\n",
    "                1,\n",
    "                2,\n",
    "            ],\n",
    "            \"timestamps\": [\n",
    "                \"2022-06-09T19:02:33.778171\",\n",
    "                \"2022-06-09T19:02:34.014545\",\n",
    "                \"2022-06-09T19:02:34.014545\",\n",
    "            ],\n",
    "            \"values\": [0.37, 0.45, 0.89],\n",
    "        },\n",
    "    }\n",
    "\n",
    "    put_data(\n",
    "        s3_res,\n",
    "        bucket_name,\n",
    "        s3_join(metrics_key, experiment_id, \"metrics.json\"),\n",
    "        json.dumps(metrics),\n",
    "    )\n",
    "\n",
    "    experiment_data = {\n",
    "        \"meta\": \"{ }\",\n",
    "        \"steps\": '[ \"experiment-test\" ]',\n",
    "        \"start_time\": 1654801353.1519804,\n",
    "        \"heartbeat\": None,\n",
    "        \"status\": \"COMPLETED\",\n",
    "        \"host\": '{\\n  \"ENV\" : { },\\n  \"cpu\" : \"777 CPU @ 9.99GHz\",\\n  \"hostname\" : \"sciflow\",\\n  \"os\" : [ \"Linux\", \"Linux-5.555-555\" ],\\n  \"python_version\" : \"3.9.0\"\\n}',\n",
    "        \"dir0\": \"sample_flow_instance_3eadd8\",\n",
    "        \"stop_time\": 1654801361.0863302,\n",
    "        \"captured_out\": \"******BEGIN step: experiment-test******\\n******END step: experiment-test******\\n\",\n",
    "        \"resources\": \"[ ]\",\n",
    "        \"artifacts\": '[ \"testfile.csv\", \"testfile.pdf\", \"testfile.pkl\" ]',\n",
    "        \"elapsed_time\": 7.93,\n",
    "        \"info\": \"{ }\",\n",
    "        \"all_hosts\": '{\\n  \"experiment-test\" : {\\n    \"ENV\" : { },\\n    \"cpu\" : \"777 CPU @ 9.99GHz\",\\n    \"hostname\" : \"sciflow\",\\n    \"os\" : [ \"Linux\", \"Linux-5.555-555\" ],\\n    \"python_version\" : \"3.9.0\"\\n  }\\n}',\n",
    "        \"experiment\": \"{ }\",\n",
    "        \"experiment_id\": \"sample_flow_instance_3eadd8\",\n",
    "    }\n",
    "\n",
    "    put_data(\n",
    "        s3_res,\n",
    "        bucket_name,\n",
    "        s3_join(runs_key, experiment_id, \"run.json\"),\n",
    "        json.dumps(experiment_data),\n",
    "    )\n",
    "\n",
    "    return experiment_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_data = create_experiment_test_data(\n",
    "    _s3_res, _s3_client, _bucket_name, _base_key, _run_id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "experiment = LakeExperiment(\n",
    "    _bucket_name, _base_key, _run_id, time.time(), experiment_data, name=\"quick-test\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment._load_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = LakeExperiment(\n",
    "    _bucket_name, _base_key, _run_id, time.time(), experiment_data\n",
    ")\n",
    "experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment._load_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sciflow (sciflow/3)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:eu-west-1:368653567616:image-version/sciflow/3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
