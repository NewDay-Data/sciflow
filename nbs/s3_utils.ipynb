{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "description: Provides utility functions for interacting with Amazon S3\n",
    "output-file: s3_utils.html\n",
    "title: S3 Utilities\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp s3_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import socket\n",
    "import uuid\n",
    "from pathlib import Path\n",
    "\n",
    "import boto3\n",
    "from botocore.client import Config\n",
    "from botocore.errorfactory import ClientError\n",
    "from botocore.exceptions import ConnectTimeoutError\n",
    "from nbdev.export import get_config\n",
    "\n",
    "from sciflow.utils import lib_path, prepare_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def is_valid_bucket(bucket_name):\n",
    "    # See https://docs.aws.amazon.com/awscloudtrail/latest/userguide/\n",
    "    # cloudtrail-s3-bucket-naming-requirements.html\n",
    "    if len(bucket_name) < 3 or len(bucket_name) > 63:\n",
    "        return False\n",
    "\n",
    "    labels = bucket_name.split(\".\")\n",
    "    # A bucket name consists of \"labels\" separated by periods\n",
    "    for label in labels:\n",
    "        if len(label) == 0 or label[0] == \"-\" or label[-1] == \"-\":\n",
    "            # Labels must be of nonzero length,\n",
    "            # and cannot begin or end with a hyphen\n",
    "            return False\n",
    "        for char in label:\n",
    "            # Labels can only contain digits, lowercase letters, or hyphens.\n",
    "            # Anything else will fail here\n",
    "            if not (char.isdigit() or char.islower() or char == \"-\"):\n",
    "                return False\n",
    "    try:\n",
    "        # If a name is a valid IP address, it cannot be a bucket name\n",
    "        socket.inet_aton(bucket_name)\n",
    "    except socket.error:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert is_valid_bucket(\"some.bucket.name\")\n",
    "assert is_valid_bucket(\"somebucketname\")\n",
    "assert not is_valid_bucket(\"path/sep\")\n",
    "assert not is_valid_bucket(\"snake_case\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def s3_join(*args):\n",
    "    return os.path.join(*args).replace(\"\\\\\", \"/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert \"windows\\path\\key\".replace(\"\\\\\", \"/\") == \"windows/path/key\"\n",
    "path_a = s3_join(\"some/path\", \"artifacts\", \"runs.json\")\n",
    "path_b = s3_join(\"some/path/\", \"artifacts\", \"runs.json\")\n",
    "path_c = s3_join(\"some/path/\", \"artifacts/\", \"runs.json\")\n",
    "expected = \"some/path/artifacts/runs.json\"\n",
    "assert path_a == path_b == path_c == expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def objects_exist_in_dir(s3_res, bucket_name, prefix):\n",
    "    bucket = s3_res.Bucket(bucket_name)\n",
    "    all_keys = [el.key for el in bucket.objects.filter(Prefix=prefix)]\n",
    "    return len(all_keys) > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sciflow/sciflow_testing_14087f979d48/s3_utils'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_bucket = \"nobuckethere\"\n",
    "invalid_bucket = \"invalid_bucket_name\"\n",
    "test_stem = f\"sciflow_testing_{str(uuid.uuid4()).split('-')[-1]}\"\n",
    "test_root = s3_join(\"sciflow\", test_stem)\n",
    "test_dir = f\"{test_root}/s3_utils\"\n",
    "lib_dir = s3_join(test_dir, \"sciflow\")\n",
    "local_dir = f\"/tmp/{test_root}/sciflow\"\n",
    "test_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config(connect_timeout=5, read_timeout=5, retries={\"max_attempts\": 0})\n",
    "s3_res = boto3.resource(\"s3\", config=config)\n",
    "s3_client = boto3.client(\"s3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def delete_dir(s3_res, bucket_name, prefix):\n",
    "    bucket = s3_res.Bucket(bucket_name)\n",
    "    bucket.objects.filter(Prefix=prefix).delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare_env()\n",
    "bucket_name = os.environ[\"SCIFLOW_BUCKET\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"s3://prosandboxpdlras3/sciflow/dataframe_artifact.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert not objects_exist_in_dir(s3_res, bucket_name, \"/non\")\n",
    "assert not objects_exist_in_dir(s3_res, bucket_name, test_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client.upload_file(\n",
    "    str(Path(\"index.ipynb\").resolve()), bucket_name, s3_join(test_dir, \"index.json\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert objects_exist_in_dir(s3_res, bucket_name, test_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def bucket_exists(s3_res, bucket_name):\n",
    "    if not is_valid_bucket(bucket_name):\n",
    "        raise ValueError(\"Bucket name does not follow AWS bucket naming rules\")\n",
    "    try:\n",
    "        s3_res.meta.client.head_bucket(Bucket=bucket_name)\n",
    "    except ClientError as er:\n",
    "        if er.response[\"Error\"][\"Code\"] == \"404\":\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 20.4 ms, sys: 5.85 ms, total: 26.3 ms\n",
      "Wall time: 77.6 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "assert bucket_exists(s3_res, bucket_name)\n",
    "try:\n",
    "    bucket_exists(s3_res, missing_bucket)\n",
    "except ConnectTimeoutError:\n",
    "    pass\n",
    "try:\n",
    "    assert bucket_exists(s3_res, invalid_bucket)\n",
    "except ValueError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def list_s3_subdirs(s3_res, bucket_name, prefix):\n",
    "    bucket = s3_res.Bucket(bucket_name)\n",
    "    all_keys = [obj.key for obj in bucket.objects.filter(Prefix=prefix)]\n",
    "    subdir_match = r\"{prefix}\\/(.*)\\/\".format(prefix=prefix)\n",
    "    subdirs = []\n",
    "    for key in all_keys:\n",
    "        match_obj = re.match(subdir_match, key)\n",
    "        if match_obj is None:\n",
    "            continue\n",
    "        else:\n",
    "            subdirs.append(match_obj.groups()[0])\n",
    "    distinct_subdirs = set(subdirs)\n",
    "    return sorted(list(distinct_subdirs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(list_s3_subdirs(s3_res, bucket_name, test_root)) > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(list_s3_subdirs(s3_res, bucket_name, \"blabla/somekey/nonsense\")) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def list_bucket(bucket_name, prefix, s3_res=None):\n",
    "    s3_res = s3_res if s3_res is not None else boto3.resource(\"s3\")\n",
    "    bucket = s3_res.Bucket(bucket_name)\n",
    "    all_keys = [obj.key for obj in bucket.objects.filter(Prefix=prefix)]\n",
    "    return all_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listed_keys = list_bucket(bucket_name, test_root, s3_res)\n",
    "assert len(listed_keys) == 1\n",
    "assert listed_keys[0].split(\"/\")[-1] == \"index.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def put_data(s3_res, bucket_name, key, binary_data):\n",
    "    s3_res.Object(bucket_name, key).put(Body=binary_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "put_data(s3_res, bucket_name, s3_join(test_root, \"put-test\"), \"value\".encode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = (\n",
    "    s3_res.Object(bucket_name, s3_join(test_root, \"put-test\"))\n",
    "    .get()[\"Body\"]\n",
    "    .read()\n",
    "    .decode(\"utf-8\")\n",
    ")\n",
    "assert \"value\" == result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def load_json(s3_res, bucket_name, key):\n",
    "    obj = s3_res.Object(bucket_name, key)\n",
    "    return json.load(obj.get()[\"Body\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_nb_json = load_json(s3_res, bucket_name, listed_keys[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert type(index_nb_json) == dict\n",
    "assert \"cells\" in index_nb_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def upload_directory(s3_client, path, bucket_name, prefix):\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        if \".ipynb_checkpoints\" not in root and \"__pycache__\" not in root:\n",
    "            # Ignore non-python source files and IPython checkpoint files\n",
    "            for file in [\n",
    "                f\n",
    "                for f in files\n",
    "                if f.split(\".\")[-1] == \"py\" and root.find(\"ipynb_checkpoints\") == -1\n",
    "            ]:\n",
    "                if root != path:\n",
    "                    sub_dir = root.replace(path, \"\").lstrip(\"/\")\n",
    "                    upload_key = f\"{prefix}/{sub_dir}/{file}\"\n",
    "                else:\n",
    "                    upload_key = f\"{prefix}/{file}\"\n",
    "                print(\n",
    "                    f\"Uploading file: {os.path.join(root, file)} to: {bucket_name}/{upload_key}\"\n",
    "                )\n",
    "                s3_client.upload_file(os.path.join(root, file), bucket_name, upload_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading file: /home/sagemaker-user/git/sciflow/sciflow/utils.py to: prosandboxpdlras3/sciflow/sciflow_testing_14087f979d48/s3_utils/sciflow/utils.py\n",
      "Uploading file: /home/sagemaker-user/git/sciflow/sciflow/export_step.py to: prosandboxpdlras3/sciflow/sciflow_testing_14087f979d48/s3_utils/sciflow/export_step.py\n",
      "Uploading file: /home/sagemaker-user/git/sciflow/sciflow/data_handler.py to: prosandboxpdlras3/sciflow/sciflow_testing_14087f979d48/s3_utils/sciflow/data_handler.py\n",
      "Uploading file: /home/sagemaker-user/git/sciflow/sciflow/_modidx.py to: prosandboxpdlras3/sciflow/sciflow_testing_14087f979d48/s3_utils/sciflow/_modidx.py\n",
      "Uploading file: /home/sagemaker-user/git/sciflow/sciflow/init.py to: prosandboxpdlras3/sciflow/sciflow_testing_14087f979d48/s3_utils/sciflow/init.py\n",
      "Uploading file: /home/sagemaker-user/git/sciflow/sciflow/params.py to: prosandboxpdlras3/sciflow/sciflow_testing_14087f979d48/s3_utils/sciflow/params.py\n",
      "Uploading file: /home/sagemaker-user/git/sciflow/sciflow/__init__.py to: prosandboxpdlras3/sciflow/sciflow_testing_14087f979d48/s3_utils/sciflow/__init__.py\n",
      "Uploading file: /home/sagemaker-user/git/sciflow/sciflow/parse_module.py to: prosandboxpdlras3/sciflow/sciflow_testing_14087f979d48/s3_utils/sciflow/parse_module.py\n",
      "Uploading file: /home/sagemaker-user/git/sciflow/sciflow/test/test_export.py to: prosandboxpdlras3/sciflow/sciflow_testing_14087f979d48/s3_utils/sciflow/test/test_export.py\n",
      "Uploading file: /home/sagemaker-user/git/sciflow/sciflow/test/test_data_handling.py to: prosandboxpdlras3/sciflow/sciflow_testing_14087f979d48/s3_utils/sciflow/test/test_data_handling.py\n",
      "Uploading file: /home/sagemaker-user/git/sciflow/sciflow/test/test_module.py to: prosandboxpdlras3/sciflow/sciflow_testing_14087f979d48/s3_utils/sciflow/test/test_module.py\n",
      "Uploading file: /home/sagemaker-user/git/sciflow/sciflow/test/test_multistep_no_params.py to: prosandboxpdlras3/sciflow/sciflow_testing_14087f979d48/s3_utils/sciflow/test/test_multistep_no_params.py\n",
      "Uploading file: /home/sagemaker-user/git/sciflow/sciflow/test/test_multistep.py to: prosandboxpdlras3/sciflow/sciflow_testing_14087f979d48/s3_utils/sciflow/test/test_multistep.py\n",
      "Uploading file: /home/sagemaker-user/git/sciflow/sciflow/test/test_export_params.py to: prosandboxpdlras3/sciflow/sciflow_testing_14087f979d48/s3_utils/sciflow/test/test_export_params.py\n",
      "Uploading file: /home/sagemaker-user/git/sciflow/sciflow/test/__init__.py to: prosandboxpdlras3/sciflow/sciflow_testing_14087f979d48/s3_utils/sciflow/test/__init__.py\n"
     ]
    }
   ],
   "source": [
    "lib_name = get_config().get(\"lib_name\")\n",
    "\n",
    "assert not (\n",
    "    objects_exist_in_dir(s3_res, bucket_name, f\"{test_dir}/{lib_name}/_modidx.py\")\n",
    ")\n",
    "assert not (\n",
    "    objects_exist_in_dir(\n",
    "        s3_res, bucket_name, f\"{test_dir}/{lib_name}/experiment/__init__.py\"\n",
    "    )\n",
    ")\n",
    "\n",
    "upload_directory(\n",
    "    s3_client,\n",
    "    str(Path(lib_path(), get_config().lib_path)),\n",
    "    bucket_name,\n",
    "    lib_dir,\n",
    ")\n",
    "\n",
    "assert objects_exist_in_dir(s3_res, bucket_name, f\"{lib_dir}/_modidx.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def download_directory(s3_client, s3_res, bucket_name, remote_key, local_dir):\n",
    "    if not Path(local_dir).exists():\n",
    "        Path(local_dir).mkdir(parents=True)\n",
    "    all_files = [\n",
    "        obj.key for obj in s3_res.Bucket(bucket_name).objects.filter(Prefix=remote_key)\n",
    "    ]\n",
    "    for file in all_files:\n",
    "        file_name = file.replace(remote_key, \"\").lstrip(\"/\")\n",
    "        local_path = Path(local_dir, file_name)\n",
    "        if not local_path.parent.exists():\n",
    "            local_path.parent.mkdir(parents=True)\n",
    "        s3_client.download_file(bucket_name, file, f\"{local_path}\")\n",
    "        assert local_path.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert not Path(f\"{local_dir}/_modidx.py\").exists()\n",
    "\n",
    "download_directory(s3_client, s3_res, bucket_name, lib_dir, local_dir)\n",
    "\n",
    "import shutil\n",
    "\n",
    "assert Path(local_dir, \"_modidx.py\").exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.rmtree(local_dir)\n",
    "delete_dir(s3_res, bucket_name, test_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "import io\n",
    "\n",
    "\n",
    "# Copied from: https://alexwlchan.net/2019/02/working-with-large-s3-objects/\n",
    "class S3File(io.RawIOBase):\n",
    "    def __init__(self, s3_object):\n",
    "        self.s3_object = s3_object\n",
    "        self.position = 0\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"<%s s3_object=%r>\" % (type(self).__name__, self.s3_object)\n",
    "\n",
    "    @property\n",
    "    def size(self):\n",
    "        return self.s3_object.content_length\n",
    "\n",
    "    def tell(self):\n",
    "        return self.position\n",
    "\n",
    "    def seek(self, offset, whence=io.SEEK_SET):\n",
    "        if whence == io.SEEK_SET:\n",
    "            self.position = offset\n",
    "        elif whence == io.SEEK_CUR:\n",
    "            self.position += offset\n",
    "        elif whence == io.SEEK_END:\n",
    "            self.position = self.size + offset\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"invalid whence ({whence}, should be {io.SEEK_SET}, io.SEEK_CUR {io.SEEK_END})\"\n",
    "            )\n",
    "\n",
    "        return self.position\n",
    "\n",
    "    def seekable(self):\n",
    "        return True\n",
    "\n",
    "    def read(self, size=-1):\n",
    "        if size == -1:\n",
    "            # Read to the end of the file\n",
    "            range_header = \"bytes=%d-\" % self.position\n",
    "            self.seek(offset=0, whence=io.SEEK_END)\n",
    "        else:\n",
    "            new_position = self.position + size\n",
    "\n",
    "            # If we're going to read beyond the end of the object, return\n",
    "            # the entire object.\n",
    "            if new_position >= self.size:\n",
    "                return self.read()\n",
    "\n",
    "            range_header = \"bytes=%d-%d\" % (self.position, new_position - 1)\n",
    "            self.seek(offset=size, whence=io.SEEK_CUR)\n",
    "\n",
    "        return self.s3_object.get(Range=range_header)[\"Body\"].read()\n",
    "\n",
    "    def readable(self):\n",
    "        return True"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:eu-west-1:819792524951:image/sagemaker-distribution-cpu-v0",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:eu-west-1:819792524951:image/sagemaker-distribution-cpu-v0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
