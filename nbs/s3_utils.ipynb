{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp s3_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# S3 Utilities\n",
    "\n",
    "> Provides utility functions for interacting with Amazon S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sciflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbotocore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01merrorfactory\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ClientError\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbotocore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ConnectTimeoutError\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msciflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m prepare_env\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sciflow'"
     ]
    }
   ],
   "source": [
    "# export\n",
    "\n",
    "import os\n",
    "import re\n",
    "import socket\n",
    "from pathlib import Path\n",
    "\n",
    "import boto3\n",
    "from botocore.client import Config\n",
    "from botocore.errorfactory import ClientError\n",
    "from botocore.exceptions import ConnectTimeoutError\n",
    "\n",
    "from sciflow.utils import prepare_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "def is_valid_bucket(bucket_name):\n",
    "    # See https://docs.aws.amazon.com/awscloudtrail/latest/userguide/\n",
    "    # cloudtrail-s3-bucket-naming-requirements.html\n",
    "    if len(bucket_name) < 3 or len(bucket_name) > 63:\n",
    "        return False\n",
    "\n",
    "    labels = bucket_name.split(\".\")\n",
    "    # A bucket name consists of \"labels\" separated by periods\n",
    "    for label in labels:\n",
    "        if len(label) == 0 or label[0] == \"-\" or label[-1] == \"-\":\n",
    "            # Labels must be of nonzero length,\n",
    "            # and cannot begin or end with a hyphen\n",
    "            return False\n",
    "        for char in label:\n",
    "            # Labels can only contain digits, lowercase letters, or hyphens.\n",
    "            # Anything else will fail here\n",
    "            if not (char.isdigit() or char.islower() or char == \"-\"):\n",
    "                return False\n",
    "    try:\n",
    "        # If a name is a valid IP address, it cannot be a bucket name\n",
    "        socket.inet_aton(bucket_name)\n",
    "    except socket.error:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert is_valid_bucket(\"some.bucket.name\")\n",
    "assert is_valid_bucket(\"somebucketname\")\n",
    "assert not is_valid_bucket(\"path/sep\")\n",
    "assert not is_valid_bucket(\"snake_case\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "def s3_join(*args):\n",
    "    return os.path.join(*args).replace(\"\\\\\", \"/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert \"windows\\path\\key\".replace(\"\\\\\", \"/\") == \"windows/path/key\"\n",
    "path_a = s3_join(\"some/path\", \"artifacts\", \"runs.json\")\n",
    "path_b = s3_join(\"some/path/\", \"artifacts\", \"runs.json\")\n",
    "path_c = s3_join(\"some/path/\", \"artifacts/\", \"runs.json\")\n",
    "expected = \"some/path/artifacts/runs.json\"\n",
    "assert path_a == path_b == path_c == expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def objects_exist_in_dir(s3_res, bucket_name, prefix):\n",
    "    bucket = s3_res.Bucket(bucket_name)\n",
    "    all_keys = [el.key for el in bucket.objects.filter(Prefix=prefix)]\n",
    "    return len(all_keys) > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_bucket = \"nobuckethere\"\n",
    "invalid_bucket = \"invalid_bucket_name\"\n",
    "experiment_dir = \"discovery/experiments/s3_test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config(connect_timeout=5, read_timeout=5, retries={\"max_attempts\": 0})\n",
    "s3_res = boto3.resource(\"s3\", config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def delete_dir(s3_res, bucket_name, prefix):\n",
    "    bucket = s3_res.Bucket(bucket_name)\n",
    "    bucket.objects.filter(Prefix=prefix).delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'SCIFLOW_BUCKET'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[0;32mIn [15]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m bucket_name \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menviron\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mSCIFLOW_BUCKET\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/kernel-env/lib/python3.9/os.py:679\u001b[0m, in \u001b[0;36m_Environ.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    676\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencodekey(key)]\n\u001b[1;32m    677\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[1;32m    678\u001b[0m     \u001b[38;5;66;03m# raise KeyError with the original key value\u001b[39;00m\n\u001b[0;32m--> 679\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    680\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecodevalue(value)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'SCIFLOW_BUCKET'"
     ]
    }
   ],
   "source": [
    "prepare_env()\n",
    "bucket_name = os.environ['SCIFLOW_BUCKET']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'bucket_name' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [14]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m objects_exist_in_dir(s3_res, \u001b[43mbucket_name\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/non\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m objects_exist_in_dir(s3_res, bucket_name, experiment_dir)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'bucket_name' is not defined"
     ]
    }
   ],
   "source": [
    "assert not objects_exist_in_dir(s3_res, bucket_name, \"/non\")\n",
    "assert objects_exist_in_dir(s3_res, bucket_name, experiment_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def bucket_exists(s3_res, bucket_name):\n",
    "    if not is_valid_bucket(bucket_name):\n",
    "        raise ValueError(\"Bucket name does not follow AWS bucket naming rules\")\n",
    "    try:\n",
    "        s3_res.meta.client.head_bucket(Bucket=bucket_name)\n",
    "    except ClientError as er:\n",
    "        if er.response[\"Error\"][\"Code\"] == \"404\":\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "assert bucket_exists(s3_res, bucket_name)\n",
    "try:\n",
    "    bucket_exists(s3_res, missing_bucket)\n",
    "except ConnectTimeoutError:\n",
    "    pass\n",
    "try:\n",
    "    assert bucket_exists(s3_res, invalid_bucket)\n",
    "except ValueError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def list_s3_subdirs(s3_res, bucket_name, prefix):\n",
    "    bucket = s3_res.Bucket(bucket_name)\n",
    "    all_keys = [obj.key for obj in bucket.objects.filter(Prefix=prefix)]\n",
    "    subdir_match = r\"{prefix}\\/(.*)\\/\".format(prefix=prefix)\n",
    "    subdirs = []\n",
    "    for key in all_keys:\n",
    "        match_obj = re.match(subdir_match, key)\n",
    "        if match_obj is None:\n",
    "            continue\n",
    "        else:\n",
    "            subdirs.append(match_obj.groups()[0])\n",
    "    distinct_subdirs = set(subdirs)\n",
    "    return sorted(list(distinct_subdirs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(list_s3_subdirs(s3_res, bucket_name, experiment_dir)) > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(list_s3_subdirs(s3_res, bucket_name, \"blabla/somekey/nonsense\")) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "def upload_directory(s3_res, path, bucket_name, prefix):\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        # Ignore non-python source files and IPython checkpoint files\n",
    "        for file in [f for f in files if f.split('.')[-1] == 'py' and root.find('ipynb_checkpoints') == -1]:\n",
    "            s3_res.upload_file(os.path.join(root, file), bucket_name, f\"{prefix}{file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "def download_directory(s3_client, bucketname, remote_key, local_dir):\n",
    "    if not Path(local_dir).exists():\n",
    "        Path(local_dir).mkdir(parents=True)\n",
    "    all_files = [obj.key for obj in boto3.resource('s3').Bucket(bucketname).objects.filter(Prefix=remote_key)]\n",
    "    for file in all_files:\n",
    "        file_name = file.split('/')[-1]\n",
    "        s3_client.download_file(bucketname, file, f'{local_dir}/{file_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "import io\n",
    "\n",
    "\n",
    "# Copied from: https://alexwlchan.net/2019/02/working-with-large-s3-objects/\n",
    "class S3File(io.RawIOBase):\n",
    "    def __init__(self, s3_object):\n",
    "        self.s3_object = s3_object\n",
    "        self.position = 0\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"<%s s3_object=%r>\" % (type(self).__name__, self.s3_object)\n",
    "\n",
    "    @property\n",
    "    def size(self):\n",
    "        return self.s3_object.content_length\n",
    "\n",
    "    def tell(self):\n",
    "        return self.position\n",
    "\n",
    "    def seek(self, offset, whence=io.SEEK_SET):\n",
    "        if whence == io.SEEK_SET:\n",
    "            self.position = offset\n",
    "        elif whence == io.SEEK_CUR:\n",
    "            self.position += offset\n",
    "        elif whence == io.SEEK_END:\n",
    "            self.position = self.size + offset\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"invalid whence ({whence}, should be {io.SEEK_SET}, io.SEEK_CUR {io.SEEK_END})\"\n",
    "            )\n",
    "\n",
    "        return self.position\n",
    "\n",
    "    def seekable(self):\n",
    "        return True\n",
    "\n",
    "    def read(self, size=-1):\n",
    "        if size == -1:\n",
    "            # Read to the end of the file\n",
    "            range_header = \"bytes=%d-\" % self.position\n",
    "            self.seek(offset=0, whence=io.SEEK_END)\n",
    "        else:\n",
    "            new_position = self.position + size\n",
    "\n",
    "            # If we're going to read beyond the end of the object, return\n",
    "            # the entire object.\n",
    "            if new_position >= self.size:\n",
    "                return self.read()\n",
    "\n",
    "            range_header = \"bytes=%d-%d\" % (self.position, new_position - 1)\n",
    "            self.seek(offset=size, whence=io.SEEK_CUR)\n",
    "\n",
    "        return self.s3_object.get(Range=range_header)[\"Body\"].read()\n",
    "\n",
    "    def readable(self):\n",
    "        return True"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "sciflow (sciflow/3)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:eu-west-1:368653567616:image-version/sciflow/3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
