{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp s3_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# S3 Utilities\n",
    "\n",
    "> Provides utility functions for interacting with Amazon S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "import os\n",
    "import re\n",
    "import socket\n",
    "import uuid\n",
    "from pathlib import Path\n",
    "\n",
    "import boto3\n",
    "from botocore.client import Config\n",
    "from botocore.errorfactory import ClientError\n",
    "from botocore.exceptions import ConnectTimeoutError\n",
    "\n",
    "from sciflow.utils import lib_path, prepare_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "def is_valid_bucket(bucket_name):\n",
    "    # See https://docs.aws.amazon.com/awscloudtrail/latest/userguide/\n",
    "    # cloudtrail-s3-bucket-naming-requirements.html\n",
    "    if len(bucket_name) < 3 or len(bucket_name) > 63:\n",
    "        return False\n",
    "\n",
    "    labels = bucket_name.split(\".\")\n",
    "    # A bucket name consists of \"labels\" separated by periods\n",
    "    for label in labels:\n",
    "        if len(label) == 0 or label[0] == \"-\" or label[-1] == \"-\":\n",
    "            # Labels must be of nonzero length,\n",
    "            # and cannot begin or end with a hyphen\n",
    "            return False\n",
    "        for char in label:\n",
    "            # Labels can only contain digits, lowercase letters, or hyphens.\n",
    "            # Anything else will fail here\n",
    "            if not (char.isdigit() or char.islower() or char == \"-\"):\n",
    "                return False\n",
    "    try:\n",
    "        # If a name is a valid IP address, it cannot be a bucket name\n",
    "        socket.inet_aton(bucket_name)\n",
    "    except socket.error:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert is_valid_bucket(\"some.bucket.name\")\n",
    "assert is_valid_bucket(\"somebucketname\")\n",
    "assert not is_valid_bucket(\"path/sep\")\n",
    "assert not is_valid_bucket(\"snake_case\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "def s3_join(*args):\n",
    "    return os.path.join(*args).replace(\"\\\\\", \"/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert \"windows\\path\\key\".replace(\"\\\\\", \"/\") == \"windows/path/key\"\n",
    "path_a = s3_join(\"some/path\", \"artifacts\", \"runs.json\")\n",
    "path_b = s3_join(\"some/path/\", \"artifacts\", \"runs.json\")\n",
    "path_c = s3_join(\"some/path/\", \"artifacts/\", \"runs.json\")\n",
    "expected = \"some/path/artifacts/runs.json\"\n",
    "assert path_a == path_b == path_c == expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def objects_exist_in_dir(s3_res, bucket_name, prefix):\n",
    "    bucket = s3_res.Bucket(bucket_name)\n",
    "    all_keys = [el.key for el in bucket.objects.filter(Prefix=prefix)]\n",
    "    return len(all_keys) > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_bucket = \"nobuckethere\"\n",
    "invalid_bucket = \"invalid_bucket_name\"\n",
    "test_root = f\"sciflow_testing_{str(uuid.uuid4()).split('-')[-1]}\"\n",
    "test_dir = f\"{test_root}/s3_utils\"\n",
    "local_dir = f\"/tmp/{test_root}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config(connect_timeout=5, read_timeout=5, retries={\"max_attempts\": 0})\n",
    "s3_res = boto3.resource(\"s3\", config=config)\n",
    "s3_client = boto3.client(\"s3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def delete_dir(s3_res, bucket_name, prefix):\n",
    "    bucket = s3_res.Bucket(bucket_name)\n",
    "    bucket.objects.filter(Prefix=prefix).delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare_env()\n",
    "bucket_name = os.environ[\"SCIFLOW_BUCKET\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert not objects_exist_in_dir(s3_res, bucket_name, \"/non\")\n",
    "assert not objects_exist_in_dir(s3_res, bucket_name, test_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client.upload_file(\n",
    "    str(Path(\"index.ipynb\").resolve()), bucket_name, s3_join(test_dir, \"index.json\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert objects_exist_in_dir(s3_res, bucket_name, test_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def bucket_exists(s3_res, bucket_name):\n",
    "    if not is_valid_bucket(bucket_name):\n",
    "        raise ValueError(\"Bucket name does not follow AWS bucket naming rules\")\n",
    "    try:\n",
    "        s3_res.meta.client.head_bucket(Bucket=bucket_name)\n",
    "    except ClientError as er:\n",
    "        if er.response[\"Error\"][\"Code\"] == \"404\":\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 17 ms, sys: 0 ns, total: 17 ms\n",
      "Wall time: 66.1 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "assert bucket_exists(s3_res, bucket_name)\n",
    "try:\n",
    "    bucket_exists(s3_res, missing_bucket)\n",
    "except ConnectTimeoutError:\n",
    "    pass\n",
    "try:\n",
    "    assert bucket_exists(s3_res, invalid_bucket)\n",
    "except ValueError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def list_s3_subdirs(s3_res, bucket_name, prefix):\n",
    "    bucket = s3_res.Bucket(bucket_name)\n",
    "    all_keys = [obj.key for obj in bucket.objects.filter(Prefix=prefix)]\n",
    "    subdir_match = r\"{prefix}\\/(.*)\\/\".format(prefix=prefix)\n",
    "    subdirs = []\n",
    "    for key in all_keys:\n",
    "        match_obj = re.match(subdir_match, key)\n",
    "        if match_obj is None:\n",
    "            continue\n",
    "        else:\n",
    "            subdirs.append(match_obj.groups()[0])\n",
    "    distinct_subdirs = set(subdirs)\n",
    "    return sorted(list(distinct_subdirs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(list_s3_subdirs(s3_res, bucket_name, test_root)) > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(list_s3_subdirs(s3_res, bucket_name, \"blabla/somekey/nonsense\")) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "def upload_directory(s3_client, path, bucket_name, prefix):\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        # Ignore non-python source files and IPython checkpoint files\n",
    "        for file in [\n",
    "            f\n",
    "            for f in files\n",
    "            if f.split(\".\")[-1] == \"py\" and root.find(\"ipynb_checkpoints\") == -1\n",
    "        ]:\n",
    "            s3_client.upload_file(\n",
    "                os.path.join(root, file), bucket_name, f\"{prefix}/{file}\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert not (objects_exist_in_dir(s3_res, bucket_name, f\"{test_dir}/_nbdev.py\"))\n",
    "\n",
    "upload_directory(s3_client, str(lib_path()), bucket_name, test_dir)\n",
    "\n",
    "assert objects_exist_in_dir(s3_res, bucket_name, f\"{test_dir}/_nbdev.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "def download_directory(s3_client, s3_res, bucket_name, remote_key, local_dir):\n",
    "    if not Path(local_dir).exists():\n",
    "        Path(local_dir).mkdir(parents=True)\n",
    "    all_files = [\n",
    "        obj.key for obj in s3_res.Bucket(bucket_name).objects.filter(Prefix=remote_key)\n",
    "    ]\n",
    "    for file in all_files:\n",
    "        file_name = file.split(\"/\")[-1]\n",
    "        s3_client.download_file(bucket_name, file, f\"{local_dir}/{file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert not Path(f\"{local_dir}/_nbdev.py\").exists()\n",
    "\n",
    "download_directory(s3_client, s3_res, bucket_name, test_dir, local_dir)\n",
    "\n",
    "import shutil\n",
    "\n",
    "py_files = [f for f in list(os.walk(local_dir))[0][2] if f.split(\".\")[-1] == \"py\"]\n",
    "assert \"_nbdev.py\" in py_files\n",
    "\n",
    "shutil.rmtree(local_dir)\n",
    "delete_dir(s3_res, bucket_name, test_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "import io\n",
    "\n",
    "\n",
    "# Copied from: https://alexwlchan.net/2019/02/working-with-large-s3-objects/\n",
    "class S3File(io.RawIOBase):\n",
    "    def __init__(self, s3_object):\n",
    "        self.s3_object = s3_object\n",
    "        self.position = 0\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"<%s s3_object=%r>\" % (type(self).__name__, self.s3_object)\n",
    "\n",
    "    @property\n",
    "    def size(self):\n",
    "        return self.s3_object.content_length\n",
    "\n",
    "    def tell(self):\n",
    "        return self.position\n",
    "\n",
    "    def seek(self, offset, whence=io.SEEK_SET):\n",
    "        if whence == io.SEEK_SET:\n",
    "            self.position = offset\n",
    "        elif whence == io.SEEK_CUR:\n",
    "            self.position += offset\n",
    "        elif whence == io.SEEK_END:\n",
    "            self.position = self.size + offset\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"invalid whence ({whence}, should be {io.SEEK_SET}, io.SEEK_CUR {io.SEEK_END})\"\n",
    "            )\n",
    "\n",
    "        return self.position\n",
    "\n",
    "    def seekable(self):\n",
    "        return True\n",
    "\n",
    "    def read(self, size=-1):\n",
    "        if size == -1:\n",
    "            # Read to the end of the file\n",
    "            range_header = \"bytes=%d-\" % self.position\n",
    "            self.seek(offset=0, whence=io.SEEK_END)\n",
    "        else:\n",
    "            new_position = self.position + size\n",
    "\n",
    "            # If we're going to read beyond the end of the object, return\n",
    "            # the entire object.\n",
    "            if new_position >= self.size:\n",
    "                return self.read()\n",
    "\n",
    "            range_header = \"bytes=%d-%d\" % (self.position, new_position - 1)\n",
    "            self.seek(offset=size, whence=io.SEEK_CUR)\n",
    "\n",
    "        return self.s3_object.get(Range=range_header)[\"Body\"].read()\n",
    "\n",
    "    def readable(self):\n",
    "        return True"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "sciflow (sciflow/3)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:eu-west-1:368653567616:image-version/sciflow/3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
