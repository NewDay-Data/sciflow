# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/to_sagemaker.ipynb (unless otherwise specified).

__all__ = ['nb_to_sagemaker_pipeline', 'is_train_step', 'is_processing_step', 'write_pipeline_to_files',
           'write_observers', 'write_track_flow', 'write_params', 'upload_directory', 'extract_step_vars',
           'format_job_arguments', 'format_hyperparams', 'format_arg', 'write_steps', 'write_track_capture',
           'get_return_var_names', 'format_args', 'generate_sagemaker_modules']

# Cell


import os
from pathlib import Path
from typing import Iterable

from nbdev.export import find_default_export, get_config, read_nb

from .data_handler import extract_param_meta
from .params import params_as_dict
from .parse_module import FuncDetails, extract_steps
from .utils import lib_path

# Cell


def nb_to_sagemaker_pipeline(
    nb_path: Path, flow_path: Path, silent=True, track_experiment=True
):
    nb = read_nb(nb_path)
    lib_name = get_config().get("lib_name")
    module_name = find_default_export(nb["cells"])
    if not module_name:
        return
    module_name = module_name
    path_sep_module_name = module_name.replace(".", "/")
    nb_name = os.path.basename(nb_path)
    exported_module = os.path.join(
        get_config().path("lib_path"), f"{path_sep_module_name}.py"
    )
    steps = extract_steps(exported_module)
    if len(steps) == 0:
        print("Skipping sagemaker conversion - not steps found")
        return
    params = params_as_dict(nb_path)
    if len(params) == 0:
        print(f"No params cell found for: {os.path.basename(nb_path)}")
    pipeline_class_name = f"{titleize(extract_module_only(module_name))}Pipeline"
    steps_param_meta = write_pipeline_to_files(
        flow_path,
        pipeline_class_name,
        lib_name,
        module_name,
        steps,
        params,
        track_experiment,
    )
    if not silent:
        print(
            f"Converted {nb_name} to {pipeline_class_name} in: {os.path.basename(flow_path)}"
        )
    generate_sagemaker_modules(
        flow_path,
        pipeline_class_name,
        lib_name,
        module_name,
        steps,
        params,
        steps_param_meta,
        track_experiment
    )

# Cell


def is_train_step(step):
    return any(step.name.startswith(prefix) for prefix in ("fit", "train"))


def is_processing_step(step):
    return not is_train_step(step)

# Cell


def write_pipeline_to_files(
    flow_path: Path,
    pipeline_class_name: str,
    lib_name: str,
    module_name: str,
    steps: Iterable[FuncDetails],
    params: dict,
    track_experiment: bool,
):
    if not os.path.exists(flow_path.parent):
        os.mkdir(flow_path.parent)
    fq_module_name = f"{lib_name}.{module_name}"
    param_meta = extract_param_meta(fq_module_name, params)
    with open(flow_path, "w") as flow_file:
        flow_file.write("#!/usr/bin/env python\n")
        flow_file.write("# coding=utf-8\n")
        flow_file.write("# SCIFLOW GENERATED FILE - EDIT COMPANION NOTEBOOK\n")

        flow_file.write("import os\n")
        flow_file.write("from datetime import datetime\n")
        flow_file.write("from pathlib import Path\n")
        flow_file.write("\n")
        flow_file.write("import boto3\n")
        flow_file.write("import sagemaker\n")
        flow_file.write("from sagemaker.session import Session\n")
        flow_file.write("from sagemaker.workflow.pipeline import Pipeline\n")

        has_train_step = any([is_train_step(s) for s in steps])
        has_processing_step = sum([is_processing_step(s) for s in steps]) != len(steps)

        if has_train_step and has_processing_step:
            flow_file.write(
                "from sagemaker.workflow.steps import ProcessingStep, TrainingStep\n"
            )
        if has_processing_step:
            flow_file.write(
                "from sagemaker.processing import ScriptProcessor, ProcessingInput, ProcessingOutput\n"
            )
            flow_file.write("from sagemaker.workflow.pipeline import Pipeline\n")
        if has_train_step:
            flow_file.write("from sagemaker.inputs import TrainingInput\n")
            flow_file.write("from sagemaker.estimator import Estimator\n")

        has_sm_param = any((p.has_sagemaker_param for p in param_meta.values()))
        if has_sm_param:
            instance_types = [p.instance_type for p in param_meta.values()]
            sm_params_import = "from sagemaker.workflow.parameters import "
            if int in instance_types:
                sm_params_import += "ParameterInteger"
                if float in instance_types or str in instance_types:
                    sm_params_import += ", "
            if float in instance_types:
                sm_params_import += "ParameterFloat"
                if str in instance_types:
                    sm_params_import += ", "
            if str in instance_types:
                sm_params_import += "ParameterString"

            flow_file.write(sm_params_import + "\n")

        flow_file.write("from sciflow.s3_utils import upload_directory\n")
        flow_file.write("\n")
        flow_file.write(
            f"from {fq_module_name} import {', '.join([s.name for s in steps])}\n"
        )
        if len(params) > 0:
            flow_file.write(
                f"from {fq_module_name} import {', '.join(params.keys())}\n"
            )

        flow_file.write(f"\n\nclass {pipeline_class_name}():\n")
        ind = "    "
        write_params(flow_file, param_meta, ind)
        flow_file.write("\n")
        flow_file.write(f"{ind}steps = {[s.name for s in steps]}\n")
        flow_file.write("\n")
        steps_param_meta = write_steps(
            module_name,
            fq_module_name,
            flow_file,
            steps,
            param_meta,
            ind,
            track_experiment,
        )
        flow_file.write("\n")

        flow_file.write(f"{ind}def get_pipeline(self) -> Pipeline:\n")
        flow_file.write(
            f"{ind}{ind}pipeline_steps = [getattr(self, step)() for step in self.steps]\n"
        )
        flow_file.write(f"{ind}{ind}pipeline = Pipeline(\n")
        flow_file.write(
            f"{ind}{ind}{ind}name=self.flow_name,\n"
        )
        flow_file.write(f"{ind}{ind}{ind}parameters=[\n")
        # Loop params
        for param_name in params.keys():
            flow_file.write(
                f"{ind}{ind}{ind}{ind}self.{param_name},\n"
            )
        flow_file.write(f"{ind}{ind}{ind}],\n")
        flow_file.write(
            f"{ind}{ind}{ind}steps = pipeline_steps,\n"
        )
        flow_file.write(
            f"{ind}{ind}{ind}sagemaker_session = self.sagemaker_session,\n"
        )
        flow_file.write(f"{ind}{ind})\n")
        flow_file.write(f"{ind}{ind}return pipeline\n")
        flow_file.write("\n")

        flow_file.write(f"{ind}def run(self):\n")
        flow_file.write(
            f"{ind}{ind}self.bucket = os.environ['SCIFLOW_BUCKET']\n"
        )
        flow_file.write(
            f"{ind}{ind}self.role = sagemaker.get_execution_role()\n"
        )
        flow_file.write(f"{ind}{ind}self.region = 'eu-west-1'\n")
        flow_file.write(
            f"{ind}{ind}self.sagemaker_session = Session(default_bucket=self.bucket)\n\n"
        )
        flow_file.write(
            f"{ind}{ind}self.flow_name = \"{extract_module_only(module_name).replace('_', '-')}\"\n"
        )
        flow_file.write(
            f"{ind}{ind}run_timestamp = datetime.today().__str__().replace(':', '-').replace('.', '-').replace(' ', '-')[:-3]\n"
        )
        flow_file.write(
            f'{ind}{ind}self.flow_run_id = f"pipeline-{{run_timestamp}}"\n'
        )
        flow_file.write(
            f'{ind}{ind}self.s3_prefix = f"{{self.flow_name}}/{{self.flow_run_id}}"\n'
        )
        flow_file.write(
            f'{ind}{ind}self.flow_s3_uri = f"s3://{{self.bucket}}/{{self.s3_prefix}}"\n'
        )
        flow_file.write(
            f'{ind}{ind}self.s3_client = boto3.client("s3")\n'
        )

        proc_steps = [s for s in steps if is_processing_step(s)]

        for proc_step in proc_steps:
            flow_file.write(
                f'{ind}{ind}self.s3_client.upload_file("test_clustering_{proc_step.name}.py", self.bucket, f"{{self.s3_prefix}}/code/test_clustering_{proc_step.name}.py")\n'
            )

        modules_dir = Path(lib_path(), get_config().lib_name)
        flow_file.write(
            f'{ind}{ind}upload_directory(self.s3_client, "{modules_dir}", self.bucket, f"{{self.s3_prefix}}/code/{get_config().lib_name}")\n'
        )
        flow_file.write("\n")
        flow_file.write(
            f"{ind}{ind}pipeline = self.get_pipeline()\n"
        )
        flow_file.write(
            f"{ind}{ind}pipeline.upsert(role_arn=self.role)\n"
        )
        flow_file.write(f"{ind}{ind}execution = pipeline.start()\n")
        flow_file.write(f"{ind}{ind}execution.wait()\n")
        flow_file.write("\n")

        flow_file.write('if __name__ == "__main__":\n')
        flow_file.write(f"{ind}{pipeline_class_name}().run()")

        return steps_param_meta

# Cell


def write_observers(lib_name, flow_file, module_name, bucket_name, project):
    pass

# Cell


def write_track_flow(flow_file, track_experiment):
    pass

# Cell


def write_params(flow_file, param_meta, ind):
    for param in param_meta.keys():
        if param_meta[param].instance_type == int:
            flow_file.write(
                f"{ind}{param} = ParameterInteger(name='{param}', default_value={param})\n"
            )
        elif param_meta[param].instance_type == float:
            flow_file.write(
                f"{ind}{param} = ParameterFloat(name='{param}', default_value={param})\n"
            )
        elif param_meta[param].instance_type == str:
            flow_file.write(
                f"{ind}{param} = ParameterString(name='{param}', default_value={param})\n"
            )

# Cell


def upload_directory(s3_res, path, bucketname, prefix):
    for root, dirs, files in os.walk(path):
        # Ignore non-python source files and IPython checkpoint files
        for file in [
            f
            for f in files
            if f.split(".")[-1] == "py" and root.find("ipynb_checkpoints") == -1
        ]:
            pass
            # print(os.path.join(root, file), bucketname, f"{prefix}{file}", file.find('ipynb_checkpoints'))
            s3_res.upload_file(os.path.join(root, file), bucketname, f"{prefix}{file}")

# Cell


def extract_step_vars(step, param_names, processing_flow_scope, train_flow_scope):
    if len(step.args) == 0:
        result = {}
    else:
        args = [x.strip() for x in step.args.split(",")]
        step_input = [a for a in args if a in param_names]
        step_proc_vars = [a for a in args if a in processing_flow_scope]
        step_train_vars = [a for a in args if a in train_flow_scope]
        unscoped_vars = set(args).difference(
            set(step_input + step_proc_vars + step_train_vars)
        )
        if len(unscoped_vars) > 0:
            raise ValueError(
                f'Step: {step.name} depends on variable(s), "{unscoped_vars}", which are not in the flow scope'
            )
        result = {
            "step_input": step_input,
            "step_proc_vars": step_proc_vars,
            "step_train_vars": step_train_vars,
        }
    return result

# Cell


def format_job_arguments(param_meta):
    job_arg_values = [
        f"str(self.{p}.__int__())"
        if param_meta[p].instance_type == int
        else f"str(self.{p}.__float__())"
        if param_meta[p].instance_type == float
        else f"self.{p}.__str__()"
        if param_meta[p].instance_type == str
        else f"str(self.{p})"
        for p in param_meta.keys()
    ]
    stitched_args = list(zip([f"--{p}" for p in param_meta.keys()], job_arg_values))
    flattened = [item for sublist in stitched_args for item in sublist]
    return flattened

# Cell


def format_hyperparams(param_meta):
    job_arg_values = [
        f"str(self.{p}.__int__())"
        if param_meta[p].instance_type == int
        else f"str(self.{p}.__float__())"
        if param_meta[p].instance_type == float
        else f"self.{p}.__str__()"
        if param_meta[p].instance_type == str
        else f"str(self.{p})"
        for p in param_meta.keys()
    ]
    return dict(zip(param_meta.keys(), job_arg_values))

# Cell


def format_arg(arg, param_meta):
    if arg in param_meta and not param_meta[arg].has_metaflow_param:
        result = arg
    else:
        result = "self." + arg
    return result


def write_steps(
    module_name,
    fq_module_name,
    flow_file,
    steps,
    param_meta,
    ind,
    track_experiment,
):
    steps_param_meta = {}
    param_names = list(param_meta.keys())
    proc_flow_scope = []
    train_flow_scope = []
    outputs = {}

    module_local_name = extract_module_only(module_name)

    for i, step in enumerate(steps):
        return_vars = get_return_var_names(step)

        step_vars = extract_step_vars(
            step, param_names, proc_flow_scope, train_flow_scope
        )

        flow_file.write(f"{ind}def {step.name}(self):\n")
        if step.docstring:
            flow_file.write(f"{ind_multiline(step.docstring, 2)}\n")
        # Processing step
        if is_processing_step(step):
            write_script_processor(flow_file, ind)

            flow_file.write("\n")
            flow_file.write(
                f"{ind}{ind}{step.name}_step = ProcessingStep(\n"
            )
            flow_file.write(
                f'{ind}{ind}{ind}name = "{step.name}",\n'
            )
            flow_file.write(
                f"{ind}{ind}{ind}processor = script_processor,\n"
            )
            flow_file.write(
                f'{ind}{ind}{ind}code = f"{{self.flow_s3_uri}}/code/{module_local_name}_{step.name}.py",\n'
            )
            if len(step_vars) > 0:
                step_param_meta = {k: param_meta[k] for k in step_vars["step_input"]}
                steps_param_meta[step.name] = step_param_meta
                if len(step_param_meta) > 0:
                    # Job Args
                    job_args = format_job_arguments(step_param_meta)
                    flow_file.write(
                        f"{ind}{ind}{ind}job_arguments=[\n"
                    )
                    job_arg_pairs = zip(job_args[::2], job_args[1::2])
                    for job_arg_pair in job_arg_pairs:
                        flow_file.write(
                            f'{ind}{ind}{ind}{ind}"{job_arg_pair[0]}", {job_arg_pair[1]},\n'
                        )
                    flow_file.write(
                        f"{ind}{ind}{ind}],\n"
                    )

                # ProcInputs
                if (
                    len(step_vars["step_proc_vars"]) > 0
                    or len(step_vars["step_train_vars"]) > 0
                ):
                    flow_file.write(
                        f"{ind}{ind}{ind}inputs = [\n"
                    )
                    flow_file.write(
                        "\n".join(
                            [
                                f'{ind}{ind}{ind}{ind}ProcessingInput(source=self.{outputs[cv]}, destination="/opt/ml/processing/{cv}"),\n'
                                for cv in step_vars["step_train_vars"]
                                + step_vars["step_proc_vars"]
                            ]
                        )
                    )
                    flow_file.write(
                        f"{ind}{ind}{ind}],\n"
                    )

                # ProcOutputs
                proc_outs = {
                    (
                        v,
                        f'{step.name}_step.properties.ProcessingOutputConfig.Outputs["{v}"].S3Output.S3Uri',
                    )
                    for v in return_vars
                }
                outputs.update(proc_outs)
                if len(proc_outs) > 0:
                    flow_file.write(
                        f"{ind}{ind}{ind}outputs = [\n"
                    )
                    flow_file.write(
                        "\n".join(
                            [
                                f'{ind}{ind}{ind}{ind}ProcessingOutput(output_name="{v}", source="/opt/ml/processing/{v}"),'
                                for v in [x[0] for x in proc_outs]
                            ]
                        )
                        + "\n"
                    )
                    flow_file.write(f"{ind}{ind}{ind}]\n")

            flow_file.write(f"{ind}{ind})\n")
            proc_flow_scope.extend(return_vars)
        elif is_train_step(step):
            if len(step_vars) > 0:
                train_outs = {
                    (v, f"{step.name}_step.properties.ModelArtifacts.S3ModelArtifacts")
                    for v in return_vars
                }
                outputs.update(train_outs)
                train_flow_scope.extend(return_vars)

                flow_file.write(f"{ind}{ind}metrics_regex = None\n")
                flow_file.write(f"{ind}{ind}if 'metric_names' in self.__dict__:\n")
                flow_file.write(
                    f'{ind}{ind}{ind}metrics = self.metric_names.split(",")\n',
                )
                flow_file.write(
                    f'{ind}{ind}{ind}metrics_regex = [{{"Name": m, "Regex": f"{{m}}=(.*?);"}} for m in metrics]\n'
                )
                flow_file.write(f"\n")
                flow_file.write(
                    f"{ind}{ind}estimator = Estimator(\n"
                )
                flow_file.write(
                    f'{ind}{ind}{ind}image_uri = "141502667606.dkr.ecr.eu-west-1.amazonaws.com/sagemaker-scikit-learn:0.23-1-cpu-py3",\n'
                )
                flow_file.write(
                    f'{ind}{ind}{ind}entry_point="{module_local_name}_{step.name}.py",\n'
                )
                # Repeated code - refactor
                step_param_meta = {k: param_meta[k] for k in step_vars["step_input"]}
                steps_param_meta[step.name] = step_param_meta
                if len(step_vars['step_input']) > 0:
                    hyper_params = format_hyperparams(step_param_meta)
                    flow_file.write(
                        f"{ind}{ind}{ind}hyperparameters={{\n"
                    )
                    for key, val in hyper_params.items():
                        flow_file.write(
                            f'{ind}{ind}{ind}{ind}"{key}", {val},\n'
                        )
                    flow_file.write(
                        f"{ind}{ind}{ind}}},\n"
                    )
                flow_file.write(
                    f'{ind}{ind}{ind}instance_type="ml.m5.xlarge",\n'
                )
                flow_file.write(
                    f"{ind}{ind}{ind}instance_count=1,\n"
                )
                flow_file.write(
                    f"{ind}{ind}{ind}output_path=self.flow_s3_uri,\n"
                )
                flow_file.write(
                    f'{ind}{ind}{ind}base_job_name="{step.name}",\n'
                )
                flow_file.write(
                    f'{ind}{ind}{ind}code_location = f"{{self.flow_s3_uri}}/code",\n'
                )
                flow_file.write(
                    f"{ind}{ind}{ind}sagemaker_session = self.sagemaker_session,\n"
                )
                flow_file.write(
                    f"{ind}{ind}{ind}role = self.role,\n"
                )
                flow_file.write(
                    f"{ind}{ind}{ind}metric_definitions=metrics_regex,\n"
                )
                flow_file.write(
                    f"{ind}{ind}{ind}enable_sagemaker_metrics=True,\n"
                )
                flow_file.write(
                    f"{ind}{ind}{ind}environment={{'AWS_DEFAULT_REGION': self.region,\n"
                )
                flow_file.write(
                    f"{ind}{ind}{ind}{ind}{ind}'SCIFLOW_BUCKET': self.bucket}}\n"
                )
                flow_file.write(f"{ind}{ind})\n")
                flow_file.write("\n")
                flow_file.write("\n")
                flow_file.write(
                    f"{ind}{ind}{step.name}_step = TrainingStep(\n"
                )
                flow_file.write(
                    f'{ind}{ind}{ind}name="{step.name}",\n'
                )
                flow_file.write(
                    f"{ind}{ind}{ind}estimator=estimator,\n"
                )
                flow_file.write(
                    f"{ind}{ind}{ind}inputs={{\n"
                )
                flow_file.write(
                    f'{ind}{ind}{ind}{ind}"documents": TrainingInput(\n'
                )
                flow_file.write(
                    f"{ind}{ind}{ind}{ind}{ind}s3_data=self.preprocess_step.properties.ProcessingOutputConfig.Outputs[\n"
                )
                flow_file.write(
                    f'{ind}{ind}{ind}{ind}{ind}{ind}"documents"\n'
                )
                flow_file.write(
                    f"{ind}{ind}{ind}{ind}{ind}].S3Output.S3Uri,\n"
                )
                flow_file.write(
                    f'{ind}{ind}{ind}{ind}{ind}content_type="text/csv",\n'
                )
                flow_file.write(
                    f"{ind}{ind}{ind}{ind})\n"
                )
                flow_file.write(f"{ind}{ind}{ind}}}\n")
                flow_file.write(f"{ind}{ind})\n")

        flow_file.write(
            f"{ind}{ind}self.{step.name}_step = {step.name}_step\n"
        )
        flow_file.write(f"{ind}{ind}return {step.name}_step\n")
        flow_file.write("\n")

    return steps_param_meta

# Cell


def write_track_capture(flow_file):
    flow_file.write(
        f"""
        for key in results.keys():
            if key in self.__dict__:
                self.__dict__[key] = self.__dict__[key] + results[key]
            else:
                self.__dict__[key] = results[key]

"""
    )

# Cell


def get_return_var_names(step):
    results_index = step.code.find("results =")
    if results_index == -1:
        return []
    return [
        l.split(":")[1].strip(", \}")
        for l in step.code[results_index:].split("\n")
        if l.strip().find(":") > -1
    ]

# Cell


def format_args(params):
    result = []
    for key, val in params.items():
        # TODO simplify
        if val.instance_type == int:
            result.append(f"int({key})")
        elif val.instance_type ==float:
            result.append(f"float({key})")
        else:
            result.append(key)
    return ', '.join(result)

# Cell


def generate_sagemaker_modules(flow_path, pipeline_class_name, lib_name, module_name, steps, params, steps_param_meta, track_experiment):
    # Read in module file as lines.
    ind = '    '
    module_path = Path(lib_path(), lib_name, f"{module_name.replace('.', '/')}.py")

    # Pass these in instead of replacting..
    fq_module_name = f"{lib_name}.{module_name}"
    param_meta = extract_param_meta(fq_module_name, params)
    # TODO needs to be step specific
    # pass these in

    with open(module_path) as module_file:
        module_lines = module_file.readlines()
    for step in steps:
        sm_module_path = (Path(str(flow_path).replace(".py", f"_{step.name}.py")))
        with open(sm_module_path, "w") as sm_module_file:
            sm_module_file.write("".join(module_lines))
            sm_module_file.write("\n\n# SCIFLOW GENERATED FROM THIS POINT\n")
            write_boilerplate_funcs(sm_module_file, ind)
            sm_module_file.write("\n")
            sm_module_file.write("\n")

            sm_module_file.write(f"def main(lib_name, remote_key, {', '.join(params.keys())}):\n")
            sm_module_file.write(f"{ind}add_lib_to_pythonpath(lib_name, remote_key)\n")
            sm_module_file.write(f"\n")
            sm_module_file.write(f"{ind}has_additional_dependencies = Path('requirements.txt').exists()\n")
            sm_module_file.write(f"{ind}if has_additional_dependencies:\n")
            sm_module_file.write(f"{ind}{ind}logger.info('Installing additional dependencies from requirements.txt')\n")
            sm_module_file.write(f"{ind}{ind}subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-r\", \"requirements.txt\"])\n")
            sm_module_file.write(f"{ind}{ind}logger.debug(\"Installed additional dependencies\")\n")
            args = ['lib_name', 'remote_key']
            if step.name in steps_param_meta:
                sm_module_file.write(f"{ind}results = {step.name}({format_args(steps_param_meta[step.name])})\n")
                args.extend(list(steps_param_meta[step.name].keys()))
            else:
                sm_module_file.write(f"{ind}results = {step.name}()\n")
            sm_module_file.write(f"{ind}add_lib_to_pythonpath(lib_name, remote_key)\n")
            sm_module_file.write(f"{ind}save_results(\"/opt/ml/processing\", results)\n")
            sm_module_file.write(f"\n")
            sm_module_file.write(f"def parse_args():\n")
            sm_module_file.write(f"{ind}parser = argparse.ArgumentParser(description=__doc__)\n")
            sm_module_file.write(f"{ind}parser.formatter_class = argparse.RawDescriptionHelpFormatter\n")

            for arg in args:
                sm_module_file.write(f"{ind}parser.add_argument(f\"--{arg}\", required=True)\n")
            sm_module_file.write("\n")
            sm_module_file.write(f"if __name__ == \"__main__\":\n")
            sm_module_file.write(f"{ind}args = parse_args()\n")
            sm_module_file.write(f"{ind}main(**vars(args))\n")
            # write to flow path/step