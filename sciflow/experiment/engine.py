# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/engine.ipynb (unless otherwise specified).

__all__ = ['MAX_CACHE_SIZE', 'ExperimentEngine']

# Cell

import datetime
import json
import os
import tempfile
import uuid
from typing import Tuple

import boto3
import numpy as np
from pandas.io.sql import DatabaseError
from tinydb import Query, TinyDB
from tinydb.storages import MemoryStorage

from .lake_experiment import (
    CSVArtifact,
    ImageArtifact,
    LakeExperiment,
)
from .tracking import FlowTracker, StepTracker
from ..utils import odbc_connect, prepare_env, query

MAX_CACHE_SIZE = 32

# Cell
class ExperimentEngine:
    def __init__(
        self,
        base_key,
        experiments_key=None,
        connection=None,
        bucket_name=None,
        bucket_table_alias=None,
    ):
        self.base_key = '"' + base_key + '"'
        self.connection = odbc_connect() if connection is None else connection
        self.bucket_name = (
            os.environ["SCIFLOW_BUCKET"] if bucket_name is None else bucket_name
        )
        self.bucket_table_alias = (
            os.environ["SCIFLOW_BUCKET_TABLE_ALIAS"]
            if bucket_table_alias is None
            else bucket_table_alias
        )
        self.experiments_key = (
            f"{base_key}/experiments" if experiments_key is None else experiments_key
        )
        table_path = f"{self.base_key}.experiments"
        self.table_context = f"{self.bucket_table_alias}.{table_path}"
        self.remote_path = f"{self.bucket_name}/{self.experiments_key}"
        self.lake_table = f"{self.table_context}"

    #    @lru_cache(maxsize=MAX_CACHE_SIZE)
    def _find(
        self,
        experiment_ids=None,
        experiment_id: str = None,
        experiment_name: str = None,
        order_by: str = None,
        limit: int = None,
    ) -> LakeExperiment:
        table_name = f"{self.table_context}.runs"
        # TODO Dremio Specific code in utils.py
        data = query(self.connection, f"ALTER TABLE {table_name} REFRESH METADATA")

        query_stmt = f"select * from {table_name}"
        if experiment_ids:
            ", ".join([str(i) for i in experiment_ids])
            query_stmt += (
                f" where dir0 IN {tuple('{}'.format(x) for x in experiment_ids)}"
            )
        if experiment_id:
            query_stmt += f" where dir0 = '{str(experiment_id)}'"
        elif experiment_name:
            query_stmt += f" where experiment_name = '{experiment_name}'"
        if order_by:
            query_stmt += f" order by {order_by} desc"
        if limit:
            query_stmt += f" limit {limit}"
        data = query(self.connection, query_stmt)
        experiments = [
            LakeExperiment(
                self.bucket_name,
                self.experiments_key,
                ex_id,
                data.iloc[i, :].to_dict()["start_time"],
                data.iloc[i, :].to_dict(),
                experiment_name,
            )
            for i, ex_id in enumerate(data.dir0.tolist())
        ]  # bucket_name, base_key, experiment_id, start_time, data, name
        return experiments

    #    @lru_cache(maxsize=MAX_CACHE_SIZE)
    def find_by_id(self, experiment_id):
        experiments = self._find(experiment_id=str(experiment_id))
        return None if len(experiments) == 0 else experiments[0]

    #    @lru_cache(maxsize=MAX_CACHE_SIZE)
    def find_by_ids(self, experiment_ids: Tuple[str]):
        if len(experiment_ids) == 1:
            raise ValueError("Use find_by_id for a single experiment")
        return self._find(experiment_ids=experiment_ids)

    #    @lru_cache(maxsize=MAX_CACHE_SIZE)
    def find_latest(self, n=5):
        return self._find(order_by="start_time", limit=n)

    #    @lru_cache(maxsize=MAX_CACHE_SIZE)
    def find_all(self):
        return self._find()

    #    @lru_cache(maxsize=MAX_CACHE_SIZE)
    def find_by_name(self, name):
        result = None
        try:
            result = self._find(experiment_name=name)
        except PermissionError:
            print(f"File not found or access not granted; check path information")
        return result

    def insert_docs(self, db, prop_name):
        experiments = self.find_all()
        for ex in experiments:
            document = json.loads(ex._data[prop_name])
            document["experiment_id"] = ex.experiment_id
            db.insert(document)

    def find_by_key(self, prop_name, key, value):
        db = TinyDB(storage=MemoryStorage)
        self.insert_docs(db, prop_name)
        Experiment = Query()
        docs = list(db.search(Experiment[key] == value))
        if len(docs) == 0:
            return None
        if len(docs) == 1:
            return self.find_by_id(docs[0]["experiment_id"])
        return self.find_by_ids(tuple(d["experiment_id"] for d in docs))

    def find_by_config_key(self, key, value):
        return self.find_by_key("config", key, value)

    def cache_clear(self):
        """Clear all caches of all find functions.
        Useful when you want to see the updates to your database."""
        self._find.cache_clear()
        self.find_all.cache_clear()
        self.find_by_id.cache_clear()
        self.find_by_ids.cache_clear()
        self.find_by_name.cache_clear()
        self.find_latest.cache_clear()

    def __repr__(self):
        return (
            f"Base Key: {self.base_key}\n"
            f"Remote Path: {self.remote_path}\n"
            f"Lake Table: {self.lake_table}"
        )