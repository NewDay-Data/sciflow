# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/converters/to_sagemaker.ipynb.

# %% auto 0
__all__ = ['logger', 'is_train_step', 'is_processing_step', 'format_job_arguments', 'format_hyperparams', 'format_arg',
           'format_args', 'write_params', 'write_sm_params', 'set_sm_params', 'nb_to_sagemaker_pipeline',
           'write_pipeline_to_files', 'write_script_processor', 'extract_step_vars', 'write_steps',
           'generate_sagemaker_modules', 'write_preamble', 'generate_flows', 'sciflow_sagemaker']

# %% ../../nbs/converters/to_sagemaker.ipynb 4
# | export


import logging
import os
import shutil
from importlib import reload
from pathlib import Path, PosixPath
from typing import Iterable

from execnb.nbio import read_nb
from fastcore.script import Param, bool_arg, call_parse
from nbdev.config import get_config
from nbdev.doclinks import nbglob
from scilint.utils import configure_logging

from ..packaging import determine_dependencies
from ..params import ParamMeta, extract_param_meta, params_as_dict
from sciflow.parse_module import (
    FuncDetails,
    extract_module_only,
    extract_return_var_names,
    extract_steps,
)
from sciflow.utils import (
    find_default_export,
    get_flow_path,
    indent_multiline,
    lib_path,
    prepare_env,
    titleize,
)

reload(logging)
logger = logging.getLogger(__name__)

# %% ../../nbs/converters/to_sagemaker.ipynb 13
# | export


def is_train_step(step):
    return any(step.name.startswith(prefix) for prefix in ("fit", "train"))


def is_processing_step(step):
    return not is_train_step(step)

# %% ../../nbs/converters/to_sagemaker.ipynb 18
# | export


def format_job_arguments(param_meta):
    job_arg_values = [f"self.{p}.to_string()" for p in param_meta.keys()]
    stitched_args = list(zip([f"--{p}" for p in param_meta.keys()], job_arg_values))
    flattened = [item for sublist in stitched_args for item in sublist]
    return flattened

# %% ../../nbs/converters/to_sagemaker.ipynb 21
# | export


def format_hyperparams(param_meta):
    job_arg_values = [f"self.{p}.to_string()" for p in param_meta.keys()]
    return dict(zip(param_meta.keys(), job_arg_values))

# %% ../../nbs/converters/to_sagemaker.ipynb 23
# | export


def format_arg(arg, param_meta):
    if arg in param_meta and not param_meta[arg].has_metaflow_param:
        result = arg
    else:
        result = "self." + arg
    return result

# %% ../../nbs/converters/to_sagemaker.ipynb 24
# | export


def format_args(params):
    result = []
    for key, val in params.items():
        # TODO simplify
        if val.instance_type == int:
            result.append(f"int({key})")
        elif val.instance_type == float:
            result.append(f"float({key})")
        else:
            result.append(key)
    return ", ".join(result)

# %% ../../nbs/converters/to_sagemaker.ipynb 28
# | export


def write_params(flow_file, param_meta, ind):
    for param in param_meta.keys():
        if param_meta[param].instance_type == int:
            flow_file.write(
                f"{ind}{param} = ParameterInteger(name='{param}', default_value={param})\n"
            )
        elif param_meta[param].instance_type == float:
            flow_file.write(
                f"{ind}{param} = ParameterFloat(name='{param}', default_value={param})\n"
            )
        elif param_meta[param].instance_type == str:
            flow_file.write(
                f"{ind}{param} = ParameterString(name='{param}', default_value={param})\n"
            )
        elif param_meta[param].instance_type == PosixPath:
            flow_file.write(
                f"{ind}{param} = ParameterString(name='{param}', default_value=str({param}))\n"
            )
        else:
            raise ValueError(
                f"Unsupported parameter type for sagemaker pipeline: {param_meta[param].instance_type}"
            )

# %% ../../nbs/converters/to_sagemaker.ipynb 30
# | export


def write_sm_params(flow_file, sm_params, ind):
    flow_file.write(f"\n{ind}# Sagemaker Specific Parameters\n")
    for key, val in sm_params.items():
        if type(val) == int:
            flow_file.write(
                f"{ind}{key} = ParameterInteger(name='{key}', default_value={val})\n"
            )
        elif type(val) == float:
            flow_file.write(
                f"{ind}{key} = ParameterFloat(name='{key}', default_value={val})\n"
            )
        elif type(val) == str:
            flow_file.write(
                f"{ind}{key} = ParameterString(name='{key}', default_value=\"{val}\")\n"
            )
        else:
            raise ValueError(
                f"Unsupported parameter type for sagemaker pipeline: {type(val)}"
            )
    flow_file.write("\n")

# %% ../../nbs/converters/to_sagemaker.ipynb 32
# | export


def set_sm_params(params, has_processing_step, has_train_step):
    # Precedence (higher given precedence)
    # 1. Environment var
    # 2. Papermill
    sm_params = {}
    if has_processing_step:
        if "proc_image_uri" not in params:
            if "SM_PROC_IMAGE_URI" not in os.environ:
                raise ValueError(
                    (
                        "Sagemaker Processing Step(s) are included but no processing image URI has "
                        "been set in either notebook parameters (papermill) or "
                        "via SM_PROC_IMAGE_URI environment variable. \nSet one of these to resolve this issue."
                    )
                )
            else:
                sm_params["proc_image_uri"] = os.environ["SM_PROC_IMAGE_URI"]
        if "proc_instance_type" not in params:
            sm_params["proc_instance_type"] = (
                os.environ["SM_PROC_INSTANCE_TYPE"]
                if "SM_PROC_INSTANCE_TYPE" in os.environ
                else "ml.m5.large"
            )
    if has_train_step:
        if "train_image_uri" not in params:
            if "SM_TRAIN_IMAGE_URI" not in os.environ:
                raise ValueError(
                    (
                        "Sagemaker Trainging Step(s) are included but no training image URI has "
                        "been set in either notebook parameters (papermill) or "
                        "via SM_TRAIN_IMAGE_URI environment variable. \nSet one of these to resolve this issue."
                    )
                )
            else:
                sm_params["train_image_uri"] = os.environ["SM_TRAIN_IMAGE_URI"]
        if "train_instance_type" not in params:
            sm_params["train_instance_type"] = (
                os.environ["SM_TRAIN_INSTANCE_TYPE"]
                if "SM_PROC_INSTANCE_TYPE" in os.environ
                else "ml.m5.large"
            )
    return sm_params

# %% ../../nbs/converters/to_sagemaker.ipynb 35
# | export


def nb_to_sagemaker_pipeline(
    nb_path: Path,
    flow_path: Path,
    silent: bool = True,
):
    nb = read_nb(nb_path)
    lib_name = get_config().get("lib_name")
    module_name = find_default_export(nb["cells"])
    if not module_name:
        logger.debug(f"Ignoring conversion for nb with no default export: {nb_path}")
        return
    module_name = module_name
    path_sep_module_name = module_name.replace(".", "/")
    nb_name = os.path.basename(nb_path)
    exported_module = os.path.join(
        get_config().path("lib_path"), f"{path_sep_module_name}.py"
    )
    steps = extract_steps(exported_module)
    if len(steps) == 0:
        logger.debug(f"Ignoring conversion for nb with no named steps: {nb_path}")
        return
    params = params_as_dict(nb_path)
    if len(params) == 0:
        logger.info(f"No params cell found for: {os.path.basename(nb_path)}")
    pipeline_class_name = f"{titleize(extract_module_only(module_name))}Pipeline"

    fq_module_name = f"{lib_name}.{module_name}"
    param_meta = extract_param_meta(fq_module_name, params)

    try:
        steps_param_meta, steps_vars = write_pipeline_to_files(
            flow_path,
            pipeline_class_name,
            lib_name,
            module_name,
            steps,
            params,
            param_meta,
            fq_module_name,
        )
    except ValueError as ve:
        print(f"Sagemaker conversion failed for {nb_name}, Reason: {ve}")
        return
    if not silent:
        print(
            f"Converted {nb_name} to {pipeline_class_name} in: {os.path.basename(flow_path)}"
        )
    generate_sagemaker_modules(
        flow_path,
        pipeline_class_name,
        lib_name,
        module_name,
        steps,
        params,
        param_meta,
        steps_param_meta,
        steps_vars,
    )

# %% ../../nbs/converters/to_sagemaker.ipynb 37
# | export


def write_pipeline_to_files(
    flow_path: Path,
    pipeline_class_name: str,
    lib_name: str,
    module_name: str,
    steps: Iterable[FuncDetails],
    params: dict,
    param_meta: Iterable[ParamMeta],
    fq_module_name: str,
):
    if not os.path.exists(flow_path.parent):
        os.mkdir(flow_path.parent)

    has_train_step = any([is_train_step(s) for s in steps])
    has_processing_step = any([is_processing_step(s) for s in steps])

    sm_params = set_sm_params(params, has_processing_step, has_train_step)

    config = get_config()

    with open(flow_path, "w") as flow_file:
        flow_file.write("#!/usr/bin/env python\n")
        flow_file.write("# coding=utf-8\n")
        flow_file.write("# SCIFLOW GENERATED FILE - EDIT COMPANION NOTEBOOK\n")

        flow_file.write("import os\n")
        flow_file.write("import sys\n")
        flow_file.write("from datetime import datetime\n")
        flow_file.write("from pathlib import Path\n")
        flow_file.write("\n")
        flow_file.write("import boto3\n")
        flow_file.write("import sagemaker\n")
        flow_file.write("from sagemaker.session import Session\n")
        flow_file.write("from sagemaker.workflow.pipeline import Pipeline\n")

        if has_train_step and has_processing_step:
            flow_file.write(
                "from sagemaker.workflow.steps import ProcessingStep, TrainingStep\n"
            )
        elif has_train_step:
            flow_file.write("from sagemaker.workflow.steps import TrainingStep\n")
        elif has_processing_step:
            flow_file.write("from sagemaker.workflow.steps import ProcessingStep\n")
        if has_processing_step:
            flow_file.write(
                "from sagemaker.processing import ScriptProcessor, ProcessingInput, ProcessingOutput\n"
            )
            flow_file.write("from sagemaker.workflow.pipeline import Pipeline\n")
        if has_train_step:
            flow_file.write("from sagemaker.inputs import TrainingInput\n")
            flow_file.write("from sagemaker.estimator import Estimator\n")

        has_sm_param = any((p.has_sagemaker_param for p in param_meta.values()))
        if has_sm_param or len(sm_params) > 0:
            instance_types = [p.instance_type for p in param_meta.values()] + [
                type(v) for v in sm_params.values()
            ]
            sm_params_import = "from sagemaker.workflow.parameters import "
            if int in instance_types:
                sm_params_import += "ParameterInteger"
                if float in instance_types or str in instance_types:
                    sm_params_import += ", "
            if float in instance_types:
                sm_params_import += "ParameterFloat"
                if str in instance_types:
                    sm_params_import += ", "
            if str in instance_types:
                sm_params_import += "ParameterString"

            flow_file.write(sm_params_import + "\n")

        flow_file.write("from sciflow.s3_utils import upload_directory\n")
        flow_file.write("\n")
        flow_file.write(
            f"from {fq_module_name} import {', '.join([s.name for s in steps])}\n"
        )
        if len(params) > 0:
            flow_file.write(
                f"from {fq_module_name} import {', '.join(params.keys())}\n"
            )

        flow_file.write(f"\n\nclass {pipeline_class_name}():\n")
        ind = "    "
        write_params(flow_file, param_meta, ind)

        if len(sm_params) > 0:
            write_sm_params(flow_file, sm_params, ind)

        params.update(sm_params)

        flow_file.write(f"{ind}args = []\n")
        flow_file.write(
            f"{ind}param_types = {dict(zip(param_meta.keys(), [v.instance_type.__name__ for v in param_meta.values()]))}\n"
        )

        flow_file.write("\n")
        flow_file.write(f"{ind}steps = {[s.name for s in steps]}\n")
        flow_file.write("\n")
        steps_param_meta, steps_vars = write_steps(
            flow_path,
            lib_name,
            module_name,
            fq_module_name,
            flow_file,
            steps,
            param_meta,
            ind,
            params.get("proc_image_uri", None),
            params.get("proc_instance_type", None),
            params.get("train_image_uri", None),
            params.get("train_instance_type", None),
        )
        flow_file.write("\n")

        flow_file.write(f"{ind}def set_run_params(self):\n")
        flow_file.write(f"{ind}{ind}for arg_key, arg_val in self.args.items():\n")
        flow_file.write(f"{ind}{ind}{ind}for param in self.param_types.keys():\n")
        flow_file.write(f"{ind}{ind}{ind}{ind}if arg_key.strip('-') == param:\n")
        flow_file.write(
            f"{ind}{ind}{ind}{ind}{ind}if self.param_types[param] == 'int':\n"
        )
        flow_file.write(
            f"{ind}{ind}{ind}{ind}{ind}{ind}setattr(self, param, ParameterInteger(name=param, default_value=int(arg_val)))\n"
        )
        flow_file.write(
            f"{ind}{ind}{ind}{ind}{ind}elif self.param_types[param] == 'float':\n"
        )
        flow_file.write(
            f"{ind}{ind}{ind}{ind}{ind}{ind}setattr(self, param, ParameterFloat(name=param, default_value=float(arg_val)))\n"
        )
        flow_file.write(
            f"{ind}{ind}{ind}{ind}{ind}elif self.param_types[param] == 'str':\n"
        )
        flow_file.write(
            f"{ind}{ind}{ind}{ind}{ind}{ind}setattr(self, param, ParameterString(name=param, default_value=arg_val))\n"
        )
        flow_file.write("\n")

        flow_file.write(f"{ind}def get_pipeline(self) -> Pipeline:\n")
        flow_file.write(
            f"{ind}{ind}pipeline_steps = [getattr(self, step)() for step in self.steps]\n"
        )
        flow_file.write(f"{ind}{ind}if len(self.args) > 0:\n")
        flow_file.write(f"{ind}{ind}{ind}self.set_run_params()\n")
        flow_file.write("\n")

        flow_file.write(f"{ind}{ind}pipeline = Pipeline(\n")
        flow_file.write(f"{ind}{ind}{ind}name=self.flow_base_key.replace('_', '-'),\n")
        flow_file.write(f"{ind}{ind}{ind}parameters=[\n")
        for param_name in params.keys():
            flow_file.write(f"{ind}{ind}{ind}{ind}self.{param_name},\n")
        flow_file.write(f"{ind}{ind}{ind}],\n")
        flow_file.write(f"{ind}{ind}{ind}steps = pipeline_steps,\n")
        flow_file.write(f"{ind}{ind}{ind}sagemaker_session = self.sagemaker_session,\n")
        flow_file.write(f"{ind}{ind})\n")
        flow_file.write(f"{ind}{ind}return pipeline\n")
        flow_file.write("\n")

        flow_file.write(f"{ind}def __init__(self):\n")
        flow_file.write(f"{ind}{ind}self.bucket = os.environ['SCIFLOW_BUCKET']\n")
        flow_file.write(f"{ind}{ind}self.role = sagemaker.get_execution_role()\n")
        flow_file.write(f"{ind}{ind}self.region = 'eu-west-1'\n")
        flow_file.write(
            f"{ind}{ind}self.sagemaker_session = Session(default_bucket=self.bucket)\n\n"
        )
        flow_file.write(
            f'{ind}{ind}self.flow_base_key = "{extract_module_only(module_name)}"\n'
        )
        flow_file.write(
            f"{ind}{ind}run_timestamp = datetime.today().__str__().replace(':', '-').replace('.', '-').replace(' ', '-')[:-3]\n"
        )
        flow_file.write(f'{ind}{ind}self.flow_run_id = f"pipeline-{{run_timestamp}}"\n')
        flow_file.write(
            f'{ind}{ind}self.s3_prefix = f"{{self.flow_base_key}}/{{self.flow_run_id}}"\n'
        )
        flow_file.write(
            f'{ind}{ind}self.flow_s3_uri = f"s3://{{self.bucket}}/{{self.s3_prefix}}"\n'
        )
        flow_file.write(f'{ind}{ind}self.s3_client = boto3.client("s3")\n')

        proc_steps = [s for s in steps if is_processing_step(s)]
        lib_reqs_path = Path(lib_path(), "requirements.txt")
        if not lib_reqs_path.exists():
            determine_dependencies(generated_pip_file_name="requirements.txt")
        flow_reqs_path = Path(
            lib_path(), config.flows_path, "sagemaker", "requirements.txt"
        )
        if not flow_reqs_path.exists():
            shutil.copyfile(lib_reqs_path, flow_reqs_path)
        flow_file.write(
            f'{ind}{ind}self.s3_client.upload_file("{flow_reqs_path}", self.bucket, f"{{self.s3_prefix}}/requirements.txt")\n'
        )
        for proc_step in proc_steps:
            step_module = (
                f"_sciflow_{extract_module_only(module_name)}_{proc_step.name}.py"
            )
            flow_file.write(
                f'{ind}{ind}self.s3_client.upload_file("{Path(lib_path(), config.flows_path, "sagemaker", step_module)}", self.bucket, f"{{self.s3_prefix}}/code/{step_module}")\n'
            )
        modules_dir = Path(lib_path(), config.lib_name)
        flow_file.write(
            f'{ind}{ind}self.lib_code_key = f"{{self.s3_prefix}}/code/{config.lib_name}"\n'
        )
        flow_file.write(
            f'{ind}{ind}upload_directory(self.s3_client, "{modules_dir}", self.bucket, self.lib_code_key)\n'
        )

        flow_file.write("\n")
        flow_file.write(f"{ind}def show(self):\n")
        flow_file.write(f"{ind}{ind}pipeline = self.get_pipeline()\n")
        flow_file.write(f"{ind}{ind}pipeline.upsert(role_arn=self.role)\n")
        flow_file.write(f"{ind}{ind}description = pipeline.describe()\n")
        flow_file.write(f'{ind}{ind}print("Sciflow generated pipeline is valid")\n')
        flow_file.write(
            f"{ind}{ind}print(f\"Pipeline name: {{description['PipelineName']}}\")\n"
        )
        flow_file.write(
            f"{ind}{ind}print(f\"Pipeline ARN: {{description['PipelineArn']}}\")\n"
        )
        flow_file.write("\n")

        flow_file.write(f"{ind}def run(self):\n")
        flow_file.write(f"{ind}{ind}pipeline = self.get_pipeline()\n")
        flow_file.write(f"{ind}{ind}pipeline.upsert(role_arn=self.role)\n")
        flow_file.write(f"{ind}{ind}execution = pipeline.start()\n")
        flow_file.write(
            f'{ind}{ind}print(f"Starting Sciflow generated pipeline: {{self.flow_run_id}}")\n'
        )
        flow_file.write(f"{ind}{ind}print(execution.describe())\n")
        flow_file.write(f"{ind}{ind}execution.wait()\n")
        flow_file.write("\n")

        flow_file.write('if __name__ == "__main__":\n')
        flow_file.write(f"{ind}if len(sys.argv) == 1:\n")
        flow_file.write(f"{ind}{ind}{pipeline_class_name}().show()\n")
        flow_file.write(f"{ind}else:\n")
        flow_file.write(f"{ind}{ind}if sys.argv[1] == 'show':\n")
        flow_file.write(f"{ind}{ind}{ind}{pipeline_class_name}().show()\n")
        flow_file.write(f"{ind}{ind}if sys.argv[1] == 'run':\n")

        flow_file.write(f"{ind}{ind}{ind}flow = {pipeline_class_name}()\n")
        flow_file.write(f"{ind}{ind}{ind}if len(sys.argv[2:]) > 0:\n")
        flow_file.write(f"{ind}{ind}{ind}{ind}args = sys.argv[2:]\n")
        flow_file.write(
            f"{ind}{ind}{ind}{ind}flow.args = dict(list(zip(args, args[1:]))[::2])\n"
        )
        flow_file.write(f"{ind}{ind}{ind}flow.run()\n")

        return steps_param_meta, steps_vars

# %% ../../nbs/converters/to_sagemaker.ipynb 39
# | export


def write_script_processor(flow_file, ind, proc_image_uri, proc_instance_type):
    flow_file.write(f"{ind}{ind}script_processor = ScriptProcessor(\n")
    flow_file.write(f"{ind}{ind}{ind}command=['python3'],\n")
    flow_file.write(f'{ind}{ind}{ind}image_uri="{proc_image_uri}",\n')
    flow_file.write(f"{ind}{ind}{ind}role=self.role,\n")
    flow_file.write(f"{ind}{ind}{ind}instance_count=1,\n")
    flow_file.write(f'{ind}{ind}{ind}instance_type="{proc_instance_type}",\n')
    flow_file.write(f"{ind}{ind}{ind}sagemaker_session=self.sagemaker_session,\n")
    flow_file.write(f'{ind}{ind}{ind}env={{"AWS_DEFAULT_REGION": self.region,\n')
    flow_file.write(f'{ind}{ind}{ind}{ind}{ind}"SCIFLOW_BUCKET": self.bucket}}\n')
    flow_file.write(f"{ind}{ind})\n")

# %% ../../nbs/converters/to_sagemaker.ipynb 41
# | export


def extract_step_vars(step, param_names, processing_flow_scope, train_flow_scope):
    logger.debug(
        f"Extracting step variables: {step.name} params: {param_names} proc scope: {processing_flow_scope} train scope: {train_flow_scope}"
    )
    if len(step.args) == 0:
        result = {}
    else:
        args = [x.strip() for x in step.args.split(",")]
        step_input = [a for a in args if a in param_names]
        step_proc_vars = [a for a in args if a in processing_flow_scope]
        step_train_vars = [a for a in args if a in train_flow_scope]
        unscoped_vars = set(args).difference(
            set(step_input + step_proc_vars + step_train_vars)
        )
        if len(unscoped_vars) > 0:
            raise ValueError(
                f'Step: {step.name} depends on variable(s) not in flow scope: "{unscoped_vars}"'
            )
        result = {
            "step_input": step_input,
            "step_proc_vars": step_proc_vars,
            "step_train_vars": step_train_vars,
        }
    return result

# %% ../../nbs/converters/to_sagemaker.ipynb 45
# | export


def write_steps(
    flow_path,
    lib_name,
    module_name,
    fq_module_name,
    flow_file,
    steps,
    param_meta,
    ind,
    proc_image_uri,
    proc_instance_type,
    train_image_uri,
    train_instance_type,
):
    steps_param_meta = {}
    steps_vars = {}
    param_names = list(param_meta.keys())
    outputs = {}
    proc_flow_scope = []
    train_flow_scope = []

    module_local_name = extract_module_only(module_name)

    for i, step in enumerate(steps):
        return_vars = extract_return_var_names(step)
        logger.debug(f"Extracted return vars: {return_vars} from step: {step.name}")

        step_vars = extract_step_vars(
            step, param_names, proc_flow_scope, train_flow_scope
        )
        steps_vars[step.name] = step_vars

        flow_file.write(f"{ind}def {step.name}(self):\n")
        if step.docstring:
            flow_file.write(f"{indent_multiline(step.docstring, 2)}\n")

        if is_processing_step(step):
            step_param_meta = _write_processing_step(
                return_vars,
                step_vars,
                flow_file,
                flow_path,
                ind,
                step,
                param_meta,
                outputs,
                lib_name,
                proc_image_uri,
                proc_instance_type,
                module_local_name,
            )
            steps_param_meta.update(step_param_meta)
            proc_flow_scope.extend(return_vars)
        elif is_train_step(step):
            step_param_meta = _write_train_step(
                return_vars,
                step_vars,
                flow_file,
                flow_path,
                ind,
                step,
                param_meta,
                outputs,
                lib_name,
                train_image_uri,
                train_instance_type,
                module_local_name,
            )
            steps_param_meta.update(step_param_meta)
            train_flow_scope.extend(return_vars)

        flow_file.write(f"{ind}{ind}self.{step.name}_step = {step.name}_step\n")
        flow_file.write(f"{ind}{ind}return {step.name}_step\n")
        flow_file.write("\n")

    return steps_param_meta, steps_vars

# %% ../../nbs/converters/to_sagemaker.ipynb 47
# | export


def _write_processing_step(
    return_vars,
    step_vars,
    flow_file,
    flow_path,
    ind,
    step,
    param_meta,
    outputs,
    lib_name,
    proc_image_uri,
    proc_instance_type,
    module_local_name,
):
    steps_param_meta = {}

    write_script_processor(flow_file, ind, proc_image_uri, proc_instance_type)
    flow_file.write("\n")
    flow_file.write(f"{ind}{ind}{step.name}_step = ProcessingStep(\n")
    flow_file.write(f'{ind}{ind}{ind}name = "{step.name}",\n')
    flow_file.write(f"{ind}{ind}{ind}processor = script_processor,\n")
    flow_file.write(
        f'{ind}{ind}{ind}code = f"{{self.flow_s3_uri}}/code/_sciflow_{module_local_name}_{step.name}.py",\n'
    )
    job_arg_pairs = [
        ("--bucket_name", "self.bucket"),
        ("--flow_base_key", "self.flow_base_key"),
        ("--flow_run_id", "self.flow_run_id"),
        ("--lib_name", f'"{lib_name}"'),
        ("--remote_key", "self.lib_code_key"),
    ]
    flow_file.write(f"{ind}{ind}{ind}inputs = [\n")
    flow_file.write(
        f'{ind}{ind}{ind}{ind}ProcessingInput(source=f"{{self.flow_s3_uri}}/requirements.txt", destination="/opt/ml/processing/requirements"),\n'
    )

    if len(step_vars) > 0:
        step_param_meta = {k: param_meta[k] for k in step_vars["step_input"]}
        steps_param_meta[step.name] = step_param_meta
        if len(step_param_meta) > 0:
            # Job Args
            job_args = format_job_arguments(step_param_meta)
            job_arg_pairs.extend(zip(job_args[::2], job_args[1::2]))

        # ProcInputs
        if (
            len(step_vars["step_proc_vars"]) > 0
            or len(step_vars["step_train_vars"]) > 0
        ):
            # For now all output artefacts from training go into model directory. Not yet using the recommended /opt/ml/output/data
            # https://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms-training-algo-output.html
            proc_inputs = (
                step_vars["step_proc_vars"]
                if len(step_vars["step_train_vars"]) == 0
                else step_vars["step_proc_vars"] + ["model"]
            )
            flow_file.write(
                "\n".join(
                    [
                        f'{ind}{ind}{ind}{ind}ProcessingInput(source=self.{outputs[cv]}, destination="/opt/ml/processing/input/{cv}"),'
                        for cv in proc_inputs
                    ]
                )
            )
    flow_file.write(f"{ind}{ind}{ind}],\n")

    if len(return_vars) > 0:
        # ProcOutputs
        proc_outs = {
            (
                v,
                f'{step.name}_step.properties.ProcessingOutputConfig.Outputs["{v}"].S3Output.S3Uri',
            )
            for v in return_vars
        }
        outputs.update(proc_outs)

        if len(proc_outs) > 0:
            flow_file.write(f"{ind}{ind}{ind}outputs = [\n")
            flow_file.write(
                "\n".join(
                    [
                        f'{ind}{ind}{ind}{ind}ProcessingOutput(destination=f"{{self.flow_s3_uri}}/{v}", output_name="{v}", source="/opt/ml/processing/output/{v}"),'
                        for v in [x[0] for x in proc_outs]
                    ]
                )
                + "\n"
            )
            flow_file.write(f"{ind}{ind}{ind}],\n")

    flow_file.write(f"{ind}{ind}{ind}job_arguments=[\n")
    for job_arg_pair in job_arg_pairs:
        flow_file.write(
            f'{ind}{ind}{ind}{ind}"{job_arg_pair[0]}", {job_arg_pair[1]},\n'
        )
    flow_file.write(f"{ind}{ind}{ind}],\n")

    flow_file.write(f"{ind}{ind})\n")
    return steps_param_meta

# %% ../../nbs/converters/to_sagemaker.ipynb 49
# | export


def _write_train_step(
    return_vars,
    step_vars,
    flow_file,
    flow_path,
    ind,
    step,
    param_meta,
    outputs,
    lib_name,
    train_image_uri,
    train_instance_type,
    module_local_name,
):
    steps_param_meta = {}

    if len(step_vars) > 0:
        train_outs = {
            (v, f"{step.name}_step.properties.ModelArtifacts.S3ModelArtifacts")
            for v in return_vars
        }
        outputs.update(train_outs)

        flow_file.write(f"{ind}{ind}metrics_regex = None\n")
        flow_file.write(f"{ind}{ind}if 'metric_names' in self.__dict__:\n")
        flow_file.write(
            f'{ind}{ind}{ind}metrics = self.metric_names.split(",")\n',
        )
        flow_file.write(
            f'{ind}{ind}{ind}metrics_regex = [{{"Name": m, "Regex": f"{{m}}=(.*?);"}} for m in metrics]\n'
        )
        flow_file.write(f"\n")
        flow_file.write(f"{ind}{ind}estimator = Estimator(\n")
        flow_file.write(f'{ind}{ind}{ind}image_uri = "{train_image_uri}",\n')
        flow_file.write(
            f'{ind}{ind}{ind}entry_point = "_sciflow_{module_local_name}_{step.name}.py",\n'
        )
        flow_file.write(f'{ind}{ind}{ind}source_dir = "{flow_path.parent}",\n')
        # Repeated code - refactor
        step_param_meta = {k: param_meta[k] for k in step_vars["step_input"]}
        steps_param_meta[step.name] = step_param_meta
        hyper_params = {
            "bucket_name": "self.bucket",
            "flow_base_key": "self.flow_base_key",
            "flow_run_id": "self.flow_run_id",
            "lib_name": f'"{lib_name}"',
            "remote_key": "self.lib_code_key",
        }
        if len(step_vars["step_input"]) > 0:
            hyper_params.update(format_hyperparams(step_param_meta))
        flow_file.write(f"{ind}{ind}{ind}hyperparameters = {{\n")
        for key, val in hyper_params.items():
            flow_file.write(f'{ind}{ind}{ind}{ind}"{key}": {val},\n')
        flow_file.write(f"{ind}{ind}{ind}}},\n")
        flow_file.write(f'{ind}{ind}{ind}instance_type = "{train_instance_type}",\n')
        flow_file.write(f"{ind}{ind}{ind}instance_count = 1,\n")
        flow_file.write(f"{ind}{ind}{ind}output_path = self.flow_s3_uri,\n")
        flow_file.write(f'{ind}{ind}{ind}base_job_name = "{step.name}",\n')
        flow_file.write(
            f'{ind}{ind}{ind}code_location = f"{{self.flow_s3_uri}}/code",\n'
        )
        flow_file.write(f"{ind}{ind}{ind}sagemaker_session = self.sagemaker_session,\n")
        flow_file.write(f"{ind}{ind}{ind}role = self.role,\n")
        flow_file.write(f"{ind}{ind}{ind}metric_definitions=metrics_regex,\n")
        flow_file.write(f"{ind}{ind}{ind}enable_sagemaker_metrics=True,\n")
        flow_file.write(
            f"{ind}{ind}{ind}environment={{'AWS_DEFAULT_REGION': self.region,\n"
        )
        flow_file.write(f"{ind}{ind}{ind}{ind}{ind}'SCIFLOW_BUCKET': self.bucket}}\n")
        flow_file.write(f"{ind}{ind})\n")
        flow_file.write("\n")
        flow_file.write(f"{ind}{ind}{step.name}_step = TrainingStep(\n")
        flow_file.write(f'{ind}{ind}{ind}name="{step.name}",\n')
        flow_file.write(f"{ind}{ind}{ind}estimator=estimator,\n")
        if "step_proc_vars" in step_vars and len(step_vars["step_proc_vars"]) > 0:
            flow_file.write(f"{ind}{ind}{ind}inputs={{\n")
            for training_input in step_vars["step_proc_vars"]:
                flow_file.write(
                    f'{ind}{ind}{ind}{ind}"{training_input}": TrainingInput(\n'
                )
                # TODO store content type mapping
                # parquet should be: "application/octet-stream"
                flow_file.write(
                    f'{ind}{ind}{ind}{ind}{ind}s3_data=self.{outputs[training_input]}, content_type="text/csv"\n'
                )
                flow_file.write(f"{ind}{ind}{ind}{ind}),\n")
            flow_file.write(f"{ind}{ind}{ind}}}\n")
        flow_file.write(f"{ind}{ind})\n")
    return steps_param_meta

# %% ../../nbs/converters/to_sagemaker.ipynb 52
# | export


def generate_sagemaker_modules(
    flow_path,
    pipeline_class_name,
    lib_name,
    module_name,
    steps,
    params,
    param_meta,
    steps_param_meta,
    steps_vars,
):
    ind = "    "
    module_path = Path(lib_path(), lib_name, f"{module_name.replace('.', '/')}.py")

    with open(module_path) as module_file:
        module_lines = module_file.readlines()
    lib_refs = []
    lines = []
    for line in module_lines:
        if line.startswith("from .."):
            lib_refs.append(line)
        else:
            lines.append(line)
    lib_refs = [l.replace("..", f"{lib_name}.") for l in lib_refs]

    for step in steps:
        sm_module_path = Path(
            flow_path.parent, "_sciflow_" + flow_path.stem + f"_{step.name}.py"
        )

        main_args = [
            "lib_name",
            "remote_key",
            "bucket_name",
            "flow_base_key",
            "flow_run_id",
        ]

        with open(sm_module_path, "w") as sm_module_file:
            sm_module_file.write(
                "# SCIFLOW GENERATED SAGEMAKER MODULE - EDIT COMPANION NOTEBOOK\n"
            )
            sm_module_file.write("import boto3\n")
            sm_module_file.write("import os\n")
            sm_module_file.write("import sys\n")
            sm_module_file.write("import pandas as pd\n")
            sm_module_file.write("import pickle\n")
            sm_module_file.write("from pathlib import Path\n")
            sm_module_file.write("import argparse\n")
            sm_module_file.write("import subprocess\n")

            if is_processing_step(step):
                sm_module_file.write(
                    f'has_additional_dependencies = Path("/opt/ml/processing/requirements/requirements.txt").exists()\n'
                )
                sm_module_file.write(f"if has_additional_dependencies:\n")
                sm_module_file.write(
                    f"{ind}print('Installing additional dependencies from requirements.txt')\n"
                )
                sm_module_file.write(
                    f'{ind}subprocess.check_call([sys.executable, "-m", "pip", "install", "-r", "/opt/ml/processing/requirements/requirements.txt"])\n'
                )
                sm_module_file.write(
                    f'{ind}print("Installed additional dependencies")\n'
                )
            sm_module_file.write("\n")
            sm_module_file.write("".join(lines))
            sm_module_file.write("\n")
            write_preamble(step, sm_module_file, ind)
            sm_module_file.write("\n")
            sm_module_file.write("\n")
            if step.name in steps_param_meta and len(steps_param_meta[step.name]) > 0:
                step_args = list(steps_param_meta[step.name].keys())
                sm_module_file.write(
                    f"def main({', '.join(main_args)}, {', '.join(step_args)}):\n"
                )
            else:
                sm_module_file.write(f"def main({', '.join(main_args)}):\n")
            if (
                is_processing_step(step)
                and "step_train_vars" in steps_vars[step.name]
                and len(steps_vars[step.name]["step_train_vars"]) > 0
            ):
                sm_module_file.write(f"\n")
                sm_module_file.write(f"{ind}import tarfile\n")
                sm_module_file.write(
                    f'{ind}model_path = f"/opt/ml/processing/input/model/model.tar.gz"\n'
                )
                sm_module_file.write(f"{ind}with tarfile.open(model_path) as tar:\n")
                sm_module_file.write(f'{ind}{ind}tar.extractall(path=".")\n')
                sm_module_file.write(f"\n")

            sm_module_file.write(f"{ind}add_lib_to_pythonpath(lib_name, remote_key)\n")
            sm_module_file.write("".join([f"{ind}{lr}" for lr in lib_refs]))

            if is_processing_step(step):
                if (
                    "step_train_vars" in steps_vars[step.name]
                    and len(steps_vars[step.name]["step_train_vars"]) > 0
                ):
                    for step_train_var in steps_vars[step.name]["step_train_vars"]:
                        sm_module_file.write(
                            f'{ind}{step_train_var} = load_result(".", "{step_train_var}")\n'
                        )
                if (
                    "step_proc_vars" in steps_vars[step.name]
                    and len(steps_vars[step.name]["step_proc_vars"]) > 0
                ):
                    for step_proc_var in steps_vars[step.name]["step_proc_vars"]:
                        sm_module_file.write(
                            f'{ind}{step_proc_var} = load_result("/opt/ml/processing/input/{step_proc_var}", "{step_proc_var}")\n'
                        )
            elif is_train_step(step):
                if (
                    "step_proc_vars" in steps_vars[step.name]
                    and len(steps_vars[step.name]["step_proc_vars"]) > 0
                ):
                    for step_proc_var in steps_vars[step.name]["step_proc_vars"]:
                        sm_module_file.write(
                            f'{ind}{step_proc_var} = load_result("/opt/ml/input/data/{step_proc_var}", "{step_proc_var}")\n'
                        )

            step_func_args = []
            if len(steps_vars[step.name]) > 0:
                step_vars = (
                    steps_vars[step.name]["step_proc_vars"]
                    + steps_vars[step.name]["step_train_vars"]
                )
                # load result for each step var
                step_func_args.extend(step_vars)
            if step.name in steps_param_meta and len(steps_param_meta[step.name]) > 0:
                step_params = (
                    format_args(steps_param_meta[step.name]).replace(" ", "").split(",")
                )
                main_args.extend(step_args)
                step_func_args.extend(step_params)
            if len(step_func_args) > 0:
                step_func_args = ",".join(
                    [
                        f"{a.replace('int', '').replace('float', '').strip('()')}={a}"
                        for a in step_func_args
                    ]
                )
                step_func_call_text = f"results = {step.name}({step_func_args})"
            else:
                step_func_call_text = f"results = {step.name}()"

            sm_module_file.write(f"{ind}{step_func_call_text}\n")

            if is_processing_step(step):
                sm_module_file.write(
                    f'{ind}save_results("/opt/ml/processing/output", results)\n'
                )
            elif is_train_step(step):
                sm_module_file.write(f'{ind}save_results("/opt/ml/model", results)\n')

            sm_module_file.write(f"\n")
            sm_module_file.write(f"def parse_args():\n")
            sm_module_file.write(
                f"{ind}parser = argparse.ArgumentParser(description=__doc__)\n"
            )
            sm_module_file.write(
                f"{ind}parser.formatter_class = argparse.RawDescriptionHelpFormatter\n"
            )

            for main_arg in main_args:
                sm_module_file.write(
                    f'{ind}parser.add_argument(f"--{main_arg}", required=True)\n'
                )
            sm_module_file.write(f"{ind}return parser.parse_args()")
            sm_module_file.write(f"\n")
            sm_module_file.write("\n")
            sm_module_file.write(f'if __name__ == "__main__":\n')
            sm_module_file.write(f"{ind}args = parse_args()\n")
            sm_module_file.write(f"{ind}main(**vars(args))\n")

# %% ../../nbs/converters/to_sagemaker.ipynb 54
# | export


def write_preamble(step, sm_module_file, ind):
    sm_module_file.write("\n")
    sm_module_file.write(
        f"def download_directory(bucket_name: str, remote_key: str, local_dir: str):\n"
    )
    sm_module_file.write(f"{ind}s3_client = boto3.client('s3')\n")
    sm_module_file.write(f"{ind}s3_res = boto3.resource('s3')\n")
    sm_module_file.write(f"{ind}if not Path(local_dir).exists():\n")
    sm_module_file.write(f"{ind}{ind}Path(local_dir).mkdir(parents=True)\n")
    sm_module_file.write(
        f"{ind}all_files = [obj.key for obj in s3_res.Bucket(bucket_name).objects.filter(Prefix=remote_key)]\n"
    )
    sm_module_file.write(f"{ind}for file in all_files:\n")
    sm_module_file.write(
        f"{ind}{ind}file_name = file.replace(remote_key, '').lstrip('/')\n"
    )
    sm_module_file.write(f"{ind}{ind}local_path = Path(local_dir, file_name)\n")
    sm_module_file.write(f"{ind}{ind}if not local_path.parent.exists():\n")
    sm_module_file.write(f"{ind}{ind}{ind}local_path.parent.mkdir(parents=True)\n")
    sm_module_file.write(
        f"{ind}{ind}s3_client.download_file(bucket_name, file, f'{{local_path}}')\n"
    )
    sm_module_file.write("\n")

    sm_module_file.write(
        f"def add_lib_to_pythonpath(lib_name: str, remote_key: str):\n"
    )
    sm_module_file.write(f"{ind}lib_dir = f'/tmp/{{lib_name}}'\n")
    sm_module_file.write(f"{ind}package_dir = f'{{lib_dir}}/{{lib_name}}'\n")
    sm_module_file.write(
        f"{ind}download_directory(os.environ['SCIFLOW_BUCKET'], remote_key, package_dir)\n"
    )
    sm_module_file.write(f"{ind}sys.path.append(lib_dir)\n")
    sm_module_file.write("\n")

    sm_module_file.write(f"def save_results(save_dir, results):\n")
    sm_module_file.write(f"{ind}if results is not None and len(results) > 0:\n")
    sm_module_file.write(f"{ind}{ind}for key, value in results.items():\n")
    sm_module_file.write(
        f"{ind}{ind}{ind}if isinstance(value, pd.Series) or isinstance(value, pd.DataFrame):\n"
    )
    if is_processing_step(step):
        sm_module_file.write(
            f'{ind}{ind}{ind}{ind}value.to_parquet(f"{{save_dir}}/{{key}}/{{key}}")\n'
        )
    elif is_train_step(step):
        sm_module_file.write(
            f'{ind}{ind}{ind}{ind}value.to_parquet(f"{{save_dir}}/{{key}}")\n'
        )
    sm_module_file.write(f"{ind}{ind}{ind}else:\n")
    if is_processing_step(step):
        sm_module_file.write(
            f'{ind}{ind}{ind}{ind}with open(f"{{save_dir}}/{{key}}/{{key}}", "wb") as pickle_file:\n'
        )
    elif is_train_step(step):
        sm_module_file.write(
            f'{ind}{ind}{ind}{ind}with open(f"{{save_dir}}/{{key}}", "wb") as pickle_file:\n'
        )
    sm_module_file.write(f"{ind}{ind}{ind}{ind}{ind}pickle.dump(value, pickle_file)\n")
    # TODO log artifacts & metrics
    sm_module_file.write("\n")

    sm_module_file.write(f"def load_result(load_dir, result_key):\n")
    sm_module_file.write(f"{ind}try:\n")
    sm_module_file.write(
        f'{ind}{ind}result = pd.read_parquet(f"{{load_dir}}/{{result_key}}")\n'
    )
    sm_module_file.write(f"{ind}except:\n")
    sm_module_file.write(
        f'{ind}{ind}result = pickle.load( open(f"{{load_dir}}/{{result_key}}", "rb") )\n'
    )
    sm_module_file.write(f"{ind}return result")

# %% ../../nbs/converters/to_sagemaker.ipynb 62
# | export


def generate_flows(nb_glob: str = None, clear_dir: bool = True):
    if clear_dir:
        metaflows_dir = Path(get_config().path("flows_path"), "sagemaker")
        [f.unlink() for f in metaflows_dir.iterdir() if not f.is_dir()]
    nb_paths = nbglob(nb_glob)
    for nb_path in nb_paths:
        nb_to_sagemaker_pipeline(
            nb_path, get_flow_path(nb_path, flow_provider="sagemaker"), silent=False
        )

# %% ../../nbs/converters/to_sagemaker.ipynb 65
# | export


@call_parse
def sciflow_sagemaker(
    nb_glob: str = None, clear_dir: bool = True, log_level: str = "warn"
):
    configure_logging(log_level)
    generate_flows(nb_glob=nb_glob, clear_dir=clear_dir)
