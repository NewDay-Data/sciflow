{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Sagemaker Pipeline Example - Abalone "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/kernel-env/lib/python3.9/site-packages/sklearn/experimental/enable_hist_gradient_boosting.py:16: UserWarning: Since version 1.0, it is not needed to import enable_hist_gradient_boosting anymore. HistGradientBoostingClassifier and HistGradientBoostingRegressor are now stable and can be normally imported from sklearn.ensemble.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# export\n",
    "\n",
    "import os\n",
    "from pathlib import Path, PosixPath\n",
    "import json\n",
    "import boto3\n",
    "import argparse\n",
    "import logging\n",
    "import pathlib\n",
    "import tempfile\n",
    "\n",
    "import boto3\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import subprocess\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from typing import Dict\n",
    "\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from six.moves.urllib.parse import urlparse\n",
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.inspection import permutation_importance\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "import time\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "bucket = \"s3bawspprwe1chatbotunpub01\"\n",
    "dataset_key_prefix = \"datasets/abalone\"\n",
    "data_path = f\"s3://{bucket}/{dataset_key_prefix}/abalone-dataset.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exportn_step:preprocess\n",
    "\n",
    "\n",
    "def preprocess(data_path):\n",
    "    logger = logging.getLogger()\n",
    "    logger.setLevel(logging.INFO)\n",
    "    logger.addHandler(logging.StreamHandler())\n",
    "\n",
    "    # Since we get a headerless CSV file we specify the column names here.\n",
    "    feature_columns_names = [\n",
    "        \"sex\",\n",
    "        \"length\",\n",
    "        \"diameter\",\n",
    "        \"height\",\n",
    "        \"whole_weight\",\n",
    "        \"shucked_weight\",\n",
    "        \"viscera_weight\",\n",
    "        \"shell_weight\",\n",
    "    ]\n",
    "    feature_columns_names = [c.lower() for c in feature_columns_names]\n",
    "    label_column = \"rings\"\n",
    "\n",
    "    has_additional_dependencies = Path(\"requirements.txt\").exists()\n",
    "    if has_additional_dependencies:\n",
    "        logger.info(\"Installing additional dependencies from requirements.txt\")\n",
    "        subprocess.check_call(\n",
    "            [sys.executable, \"-m\", \"pip\", \"install\", \"-r\", \"requirements.txt\"]\n",
    "        )\n",
    "        logger.debug(\"Installed additional dependencies\")\n",
    "\n",
    "    df = pd.read_csv(data_path)\n",
    "    # Convert column names ot lwoer case\n",
    "    df.columns = [c.strip().lower().replace(\" \", \"_\") for c in df.columns]\n",
    "\n",
    "    logger.debug(\"Defining transformers.\")\n",
    "    numeric_features = list(feature_columns_names)\n",
    "    numeric_features.remove(\"sex\")\n",
    "    numeric_transformer = Pipeline(\n",
    "        steps=[\n",
    "            (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "            (\"scaler\", StandardScaler()),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    categorical_features = [\"sex\"]\n",
    "    categorical_transformer = Pipeline(\n",
    "        steps=[\n",
    "            (\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=\"missing\")),\n",
    "            (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\")),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    preprocess = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"num\", numeric_transformer, numeric_features),\n",
    "            (\"cat\", categorical_transformer, categorical_features),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    logger.info(\"Applying transforms.\")\n",
    "    y = df[\"rings\"]\n",
    "    X_pre = df.drop(\"rings\", axis=1)\n",
    "    X_pre = preprocess.fit_transform(X_pre)\n",
    "    y_pre = y.to_numpy().reshape(len(y), 1)\n",
    "\n",
    "    X = np.concatenate((y_pre, X_pre), axis=1)\n",
    "\n",
    "    logger.info(\n",
    "        \"Splitting %d rows of data into train, validation, test datasets.\", len(X)\n",
    "    )\n",
    "    np.random.shuffle(X)\n",
    "    train, validation, test = np.split(X, [int(0.7 * len(X)), int(0.85 * len(X))])\n",
    "\n",
    "    results = {\"train\": train, \"validation\": validation, \"test\": test}\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Applying transforms.\n",
      "Splitting 4177 rows of data into train, validation, test datasets.\n"
     ]
    }
   ],
   "source": [
    "results = preprocess(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2923, 11)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exportn_step:train\n",
    "\n",
    "\n",
    "def train(model_dir, train, validation, hyperparameters):\n",
    "    logger = logging.getLogger()\n",
    "    logger.setLevel(logging.INFO)\n",
    "    logger.addHandler(logging.StreamHandler())\n",
    "\n",
    "    def plot_perm_importances(model_dir, model, X, y, dataset_name: str):\n",
    "        result = permutation_importance(\n",
    "            model, X, y, n_repeats=10, random_state=42, n_jobs=2\n",
    "        )\n",
    "        sorted_idx = result.importances_mean.argsort()\n",
    "\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.boxplot(\n",
    "            result.importances[sorted_idx].T, vert=False, labels=X.columns[sorted_idx]\n",
    "        )\n",
    "        ax.set_title(f\"Permutation Importances: {dataset_name}\")\n",
    "        fig.tight_layout()\n",
    "        plt.savefig(\n",
    "            os.path.join(model_dir, f\"permutation_importances_{dataset_name}.pdf\"),\n",
    "            dpi=150,\n",
    "        )\n",
    "\n",
    "    model = HistGradientBoostingRegressor(**hyperparameters)\n",
    "\n",
    "    logger.debug(f\"Using GBM with {model.max_iter} estimators\")\n",
    "\n",
    "    with (Path(train) / \"train.csv\").open() as f:\n",
    "        train_data = pd.read_csv(f)\n",
    "    y_train = train_data.iloc[:, 0].to_numpy()\n",
    "    train_data.drop(train_data.columns[0], axis=1, inplace=True)\n",
    "    X_train = train_data\n",
    "    logger.debug(f\"Training data loaded\")\n",
    "\n",
    "    st = time.time()\n",
    "    model.fit(X_train, y_train)\n",
    "    train_time = (time.time() - st) / 60\n",
    "    logger.info(f\"Model trained after: {train_time} minutes\")\n",
    "\n",
    "    with (Path(validation) / \"validation.csv\").open() as f:\n",
    "        validation_data = pd.read_csv(f)\n",
    "    y_val = validation_data.iloc[:, 0].to_numpy()\n",
    "    validation_data.drop(validation_data.columns[0], axis=1, inplace=True)\n",
    "    X_val = validation_data\n",
    "    logger.debug(f\"Validation data loaded\")\n",
    "\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_val = model.predict(X_val)\n",
    "\n",
    "    logger.debug(\"Calculating mean squared errors.\")\n",
    "    mse_train = mean_squared_error(y_train, y_pred_train)\n",
    "    std_train = np.std(y_train - y_pred_train)\n",
    "    mse_val = mean_squared_error(y_val, y_pred_val)\n",
    "    std_val = np.std(y_val - y_pred_val)\n",
    "\n",
    "    logger.debug(\"Calculated mean squared errors.\")\n",
    "    logger.info(f\"Train MSE={mse_train}; Train STD={std_train};\")\n",
    "    logger.info(f\"Validation MSE={mse_val}; Validation STD={std_val};\")\n",
    "\n",
    "    plot_perm_importances(model_dir, model, X_train, y_train, \"train\")\n",
    "    plot_perm_importances(model_dir, model, X_val, y_val, \"validation\")\n",
    "\n",
    "    joblib.dump(model, os.path.join(model_dir, \"model.joblib\"))\n",
    "    logger.info(\"Model persisted to file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'preprocess' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocess\u001b[49m(data_path)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'preprocess' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "python3 (conda-env/4)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:eu-west-1:368653567616:image-version/conda-env/4"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
