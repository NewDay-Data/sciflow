{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Sagemaker Pipeline Example - Abalone "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp abalone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/kernel-env/lib/python3.9/site-packages/sklearn/experimental/enable_hist_gradient_boosting.py:16: UserWarning: Since version 1.0, it is not needed to import enable_hist_gradient_boosting anymore. HistGradientBoostingClassifier and HistGradientBoostingRegressor are now stable and can be normally imported from sklearn.ensemble.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# export\n",
    "\n",
    "import logging\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "bucket = \"s3bawspprwe1chatbotunpub01\"\n",
    "dataset_key_prefix = \"datasets/abalone\"\n",
    "data_path = f\"s3://{bucket}/{dataset_key_prefix}/abalone-dataset.csv\"\n",
    "hyperparameters = {\"max_iter\": 50}\n",
    "model_dir = \"/tmp/replacewithgenerated\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exportn_step:preprocess\n",
    "\n",
    "\n",
    "def preprocess(data_path):\n",
    "    logger = logging.getLogger()\n",
    "    logger.setLevel(logging.INFO)\n",
    "    logger.addHandler(logging.StreamHandler())\n",
    "\n",
    "    # Since we get a headerless CSV file we specify the column names here.\n",
    "    feature_columns_names = [\n",
    "        \"sex\",\n",
    "        \"length\",\n",
    "        \"diameter\",\n",
    "        \"height\",\n",
    "        \"whole_weight\",\n",
    "        \"shucked_weight\",\n",
    "        \"viscera_weight\",\n",
    "        \"shell_weight\",\n",
    "    ]\n",
    "    feature_columns_names = [c.lower() for c in feature_columns_names]\n",
    "\n",
    "    has_additional_dependencies = Path(\"requirements.txt\").exists()\n",
    "    if has_additional_dependencies:\n",
    "        logger.info(\"Installing additional dependencies from requirements.txt\")\n",
    "        subprocess.check_call(\n",
    "            [sys.executable, \"-m\", \"pip\", \"install\", \"-r\", \"requirements.txt\"]\n",
    "        )\n",
    "        logger.debug(\"Installed additional dependencies\")\n",
    "\n",
    "    df = pd.read_csv(data_path)\n",
    "    # Convert column names ot lwoer case\n",
    "    df.columns = [c.strip().lower().replace(\" \", \"_\") for c in df.columns]\n",
    "\n",
    "    logger.debug(\"Defining transformers.\")\n",
    "    numeric_features = list(feature_columns_names)\n",
    "    numeric_features.remove(\"sex\")\n",
    "    numeric_transformer = Pipeline(\n",
    "        steps=[\n",
    "            (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "            (\"scaler\", StandardScaler()),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    categorical_features = [\"sex\"]\n",
    "    categorical_transformer = Pipeline(\n",
    "        steps=[\n",
    "            (\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=\"missing\")),\n",
    "            (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\")),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    preprocess = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"num\", numeric_transformer, numeric_features),\n",
    "            (\"cat\", categorical_transformer, categorical_features),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    logger.info(\"Applying transforms.\")\n",
    "    y = df[\"rings\"]\n",
    "    X_pre = df.drop(\"rings\", axis=1)\n",
    "    X_pre = preprocess.fit_transform(X_pre)\n",
    "    y_pre = y.to_numpy().reshape(len(y), 1)\n",
    "\n",
    "    X = np.concatenate((y_pre, X_pre), axis=1)\n",
    "\n",
    "    logger.info(\n",
    "        \"Splitting %d rows of data into train, validation, test datasets.\", len(X)\n",
    "    )\n",
    "    np.random.shuffle(X)\n",
    "    train, validation, test = np.split(X, [int(0.7 * len(X)), int(0.85 * len(X))])\n",
    "\n",
    "    results = {\"train\": train, \"validation\": validation, \"test\": test}\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "def plot_perm_importances(model_dir, model, X, y, dataset_name: str):\n",
    "    result = permutation_importance(\n",
    "        model, X, y, n_repeats=10, random_state=42, n_jobs=2\n",
    "    )\n",
    "    sorted_idx = result.importances_mean.argsort()\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.boxplot(\n",
    "        result.importances[sorted_idx].T, vert=False, labels=X.columns[sorted_idx]\n",
    "    )\n",
    "    ax.set_title(f\"Permutation Importances: {dataset_name}\")\n",
    "    fig.tight_layout()\n",
    "    plt.savefig(\n",
    "        os.path.join(model_dir, f\"permutation_importances_{dataset_name}.pdf\"),\n",
    "        dpi=150,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exportn_step:train\n",
    "\n",
    "\n",
    "def train(model_dir, train, validation, hyperparameters):\n",
    "    logger = logging.getLogger()\n",
    "    logger.setLevel(logging.INFO)\n",
    "    logger.addHandler(logging.StreamHandler())\n",
    "\n",
    "    model = HistGradientBoostingRegressor(**hyperparameters)\n",
    "\n",
    "    logger.debug(f\"Using GBM with {model.max_iter} estimators\")\n",
    "\n",
    "    with (Path(train) / \"train.csv\").open() as f:\n",
    "        train_data = pd.read_csv(f)\n",
    "    y_train = train_data.iloc[:, 0].to_numpy()\n",
    "    train_data.drop(train_data.columns[0], axis=1, inplace=True)\n",
    "    X_train = train_data\n",
    "    logger.debug(f\"Training data loaded\")\n",
    "\n",
    "    st = time.time()\n",
    "    model.fit(X_train, y_train)\n",
    "    train_time = (time.time() - st) / 60\n",
    "    logger.info(f\"Model trained after: {train_time} minutes\")\n",
    "\n",
    "    with (Path(validation) / \"validation.csv\").open() as f:\n",
    "        validation_data = pd.read_csv(f)\n",
    "    y_val = validation_data.iloc[:, 0].to_numpy()\n",
    "    validation_data.drop(validation_data.columns[0], axis=1, inplace=True)\n",
    "    X_val = validation_data\n",
    "    logger.debug(f\"Validation data loaded\")\n",
    "\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_val = model.predict(X_val)\n",
    "\n",
    "    logger.debug(\"Calculating mean squared errors.\")\n",
    "    mse_train = mean_squared_error(y_train, y_pred_train)\n",
    "    std_train = np.std(y_train - y_pred_train)\n",
    "    mse_val = mean_squared_error(y_val, y_pred_val)\n",
    "    std_val = np.std(y_val - y_pred_val)\n",
    "\n",
    "    logger.debug(\"Calculated mean squared errors.\")\n",
    "    logger.info(f\"Train MSE={mse_train}; Train STD={std_train};\")\n",
    "    logger.info(f\"Validation MSE={mse_val}; Validation STD={std_val};\")\n",
    "\n",
    "    plot_perm_importances(model_dir, model, X_train, y_train, \"train\")\n",
    "    plot_perm_importances(model_dir, model, X_val, y_val, \"validation\")\n",
    "\n",
    "    joblib.dump(model, os.path.join(model_dir, \"model.joblib\"))\n",
    "    logger.info(\"Model persisted to file.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sciflow (sciflow/3)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:eu-west-1:368653567616:image-version/sciflow/3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
