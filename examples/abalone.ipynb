{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `sciflow` more detailed ML example - Abalone Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp abalone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.addHandler(logging.StreamHandler())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "import json\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OrdinalEncoder, StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "bucket = os.environ[\"SCIFLOW_BUCKET\"]\n",
    "dataset_key_prefix = \"datasets/abalone\"\n",
    "data_path = f\"s3://{bucket}/{dataset_key_prefix}/abalone-dataset.csv\"\n",
    "hyperparameters = {\"max_iter\": 20}\n",
    "model_dir = \"/tmp/replacewithgenerated\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not Path(model_dir).exists():\n",
    "    Path(model_dir).mkdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exportn_step:preprocess\n",
    "\n",
    "\n",
    "def preprocess(data_path):\n",
    "    # Since we get a headerless CSV file we specify the column names here.\n",
    "    feature_columns_names = [\n",
    "        \"sex\",\n",
    "        \"length\",\n",
    "        \"diameter\",\n",
    "        \"height\",\n",
    "        \"whole_weight\",\n",
    "        \"shucked_weight\",\n",
    "        \"viscera_weight\",\n",
    "        \"shell_weight\",\n",
    "    ]\n",
    "    feature_columns_names = [c.lower() for c in feature_columns_names]\n",
    "\n",
    "    has_additional_dependencies = Path(\"requirements.txt\").exists()\n",
    "    if has_additional_dependencies:\n",
    "        logger.info(\"Installing additional dependencies from requirements.txt\")\n",
    "        subprocess.check_call(\n",
    "            [sys.executable, \"-m\", \"pip\", \"install\", \"-r\", \"requirements.txt\"]\n",
    "        )\n",
    "        logger.debug(\"Installed additional dependencies\")\n",
    "\n",
    "    df = pd.read_csv(data_path)\n",
    "    # Convert column names ot lwoer case\n",
    "    df.columns = [c.strip().lower().replace(\" \", \"_\") for c in df.columns]\n",
    "\n",
    "    logger.debug(\"Defining transformers.\")\n",
    "    numeric_features = list(feature_columns_names)\n",
    "    numeric_features.remove(\"sex\")\n",
    "    numeric_transformer = Pipeline(\n",
    "        steps=[\n",
    "            (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "            (\"scaler\", StandardScaler()),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    categorical_features = [\"sex\"]\n",
    "    categorical_transformer = Pipeline(\n",
    "        steps=[\n",
    "            (\n",
    "                \"ordinal\",\n",
    "                OrdinalEncoder(\n",
    "                    handle_unknown=\"use_encoded_value\", unknown_value=np.nan\n",
    "                ),\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "    preprocess = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"num\", numeric_transformer, numeric_features),\n",
    "            (\"cat\", categorical_transformer, categorical_features),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    logger.info(\"Applying transforms.\")\n",
    "    y = df[\"rings\"]\n",
    "    X_pre = df.drop(\"rings\", axis=1)\n",
    "    X_pre = preprocess.fit_transform(X_pre)\n",
    "    y_pre = y.to_numpy().reshape(len(y), 1)\n",
    "\n",
    "    X = np.concatenate((y_pre, X_pre), axis=1)\n",
    "\n",
    "    logger.info(\n",
    "        \"Splitting %d rows of data into train, validation, test datasets.\", len(X)\n",
    "    )\n",
    "    np.random.shuffle(X)\n",
    "    train, validation, test = np.split(X, [int(0.7 * len(X)), int(0.85 * len(X))])\n",
    "\n",
    "    categorical_mask = [False] * len(numeric_features) + [True] * len(\n",
    "        categorical_features\n",
    "    )\n",
    "    columns = [\"rings\"] + numeric_features + categorical_features\n",
    "    results = {\n",
    "        \"train\": pd.DataFrame(data=train, columns=columns),\n",
    "        \"validation\": pd.DataFrame(data=validation, columns=columns),\n",
    "        \"test\": pd.DataFrame(data=test, columns=columns),\n",
    "        \"categorical_mask\": categorical_mask,\n",
    "    }\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "def plot_perm_importances(model_dir, model, X, y, dataset_name: str):\n",
    "    result = permutation_importance(\n",
    "        model, X, y, n_repeats=10, random_state=42, n_jobs=2\n",
    "    )\n",
    "    sorted_idx = result.importances_mean.argsort()\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.boxplot(\n",
    "        result.importances[sorted_idx].T, vert=False, labels=X.columns[sorted_idx]\n",
    "    )\n",
    "    ax.set_title(f\"Permutation Importances: {dataset_name}\")\n",
    "    fig.tight_layout()\n",
    "    plt.savefig(\n",
    "        os.path.join(model_dir, f\"permutation_importances_{dataset_name}.pdf\"),\n",
    "        dpi=150,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exportn_step:fit\n",
    "\n",
    "\n",
    "def fit(model_dir, train, validation, categorical_mask, hyperparameters):\n",
    "    model = HistGradientBoostingRegressor(\n",
    "        **hyperparameters, categorical_features=categorical_mask\n",
    "    )\n",
    "\n",
    "    logger.debug(f\"Using GBM with {model.max_iter} estimators\")\n",
    "\n",
    "    train_data = train.copy()\n",
    "    y_train = train_data.iloc[:, 0].to_numpy()\n",
    "    train_data.drop(train_data.columns[0], axis=1, inplace=True)\n",
    "    X_train = train_data\n",
    "    logger.debug(f\"Training data loaded\")\n",
    "\n",
    "    st = time.time()\n",
    "    model.fit(X_train, y_train)\n",
    "    train_time = (time.time() - st) / 60\n",
    "    logger.info(f\"Model trained after: {train_time} minutes\")\n",
    "\n",
    "    validation_data = validation.copy()\n",
    "    y_val = validation_data.iloc[:, 0].to_numpy()\n",
    "    validation_data.drop(validation_data.columns[0], axis=1, inplace=True)\n",
    "    X_val = validation_data\n",
    "    logger.debug(f\"Validation data loaded\")\n",
    "\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_val = model.predict(X_val)\n",
    "\n",
    "    logger.debug(\"Calculating mean squared errors.\")\n",
    "    mse_train = mean_squared_error(y_train, y_pred_train)\n",
    "    std_train = np.std(y_train - y_pred_train)\n",
    "    mse_val = mean_squared_error(y_val, y_pred_val)\n",
    "    std_val = np.std(y_val - y_pred_val)\n",
    "\n",
    "    logger.debug(\"Calculated mean squared errors.\")\n",
    "    logger.info(f\"Train MSE={mse_train}; Train STD={std_train};\")\n",
    "    logger.info(f\"Validation MSE={mse_val}; Validation STD={std_val};\")\n",
    "\n",
    "    plot_perm_importances(model_dir, model, X_train, y_train, \"train\")\n",
    "    plot_perm_importances(model_dir, model, X_val, y_val, \"validation\")\n",
    "\n",
    "    joblib.dump(model, os.path.join(model_dir, \"model.joblib\"))\n",
    "    logger.info(\"Model persisted to file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exportn_step:evaluate\n",
    "\n",
    "\n",
    "def evaluation(model_dir, test):\n",
    "    has_additional_dependencies = Path(\"requirements.txt\").exists()\n",
    "    if has_additional_dependencies:\n",
    "        logger.info(\"Installing additional dependencies from requirements.txt\")\n",
    "        subprocess.check_call(\n",
    "            [sys.executable, \"-m\", \"pip\", \"install\", \"-r\", \"requirements.txt\"]\n",
    "        )\n",
    "        logger.debug(\"Installed additional dependencies\")\n",
    "\n",
    "    logger.debug(\"Loading sklearn model.\")\n",
    "    model = joblib.load(f\"{model_dir}/model.joblib\")\n",
    "\n",
    "    logger.debug(\"Reading test data.\")\n",
    "    df = test.copy()\n",
    "    y_test = df.iloc[:, 0].to_numpy()\n",
    "    df.drop(df.columns[0], axis=1, inplace=True)\n",
    "    X_test = df\n",
    "    logger.debug(\"Read in test data.\")\n",
    "\n",
    "    logger.info(\"Performing predictions against test data.\")\n",
    "    predictions = model.predict(X_test)\n",
    "\n",
    "    logger.debug(\"Calculating mean squared error.\")\n",
    "    mse = mean_squared_error(y_test, predictions)\n",
    "    std = np.std(y_test - predictions)\n",
    "    report_dict = {\n",
    "        \"regression_metrics\": {\n",
    "            \"mse\": {\"value\": mse, \"standard_deviation\": std},\n",
    "        },\n",
    "    }\n",
    "\n",
    "    Path(model_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    plot_perm_importances(model_dir, model, X_test, y_test, \"test\")\n",
    "\n",
    "    logger.info(\"Writing out evaluation report with mse: %f\", mse)\n",
    "    evaluation_path = f\"{model_dir}/evaluation.json\"\n",
    "    with open(evaluation_path, \"w\") as f:\n",
    "        f.write(json.dumps(report_dict))\n",
    "\n",
    "    logger.debug(\"Completed evaluation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slow\n",
    "\n",
    "results = preprocess(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slow\n",
    "\n",
    "fit(\n",
    "    model_dir,\n",
    "    results[\"train\"],\n",
    "    results[\"validation\"],\n",
    "    results[\"categorical_mask\"],\n",
    "    hyperparameters,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slow\n",
    "\n",
    "evaluation(model_dir, results[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sciflow (sciflow/3)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:eu-west-1:368653567616:image-version/sciflow/3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
