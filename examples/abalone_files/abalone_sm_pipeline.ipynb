{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Sagemaker Pipeline Example - Abalone "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import sagemaker\n",
    "import sagemaker.session\n",
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker.model_metrics import MetricsSource, ModelMetrics\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from sagemaker.sklearn.estimator import SKLearn\n",
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "from sagemaker.workflow.condition_step import ConditionStep, JsonGet\n",
    "from sagemaker.workflow.conditions import ConditionLessThanOrEqualTo\n",
    "from sagemaker.workflow.parameters import ParameterInteger, ParameterString\n",
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "from sagemaker.workflow.properties import PropertyFile\n",
    "from sagemaker.workflow.step_collections import RegisterModel\n",
    "from sagemaker.workflow.steps import CacheConfig, ProcessingStep, TrainingStep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from sagemaker.workflow.parameters import ParameterInteger, ParameterString\n",
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "from sagemaker.workflow.steps import ProcessingStep, TrainingStep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import PosixPath\n",
    "\n",
    "bucket = \"s3bawspprwe1chatbotunpub01\"\n",
    "dataset_key_prefix = \"datasets/abalone\"\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "repo_root = PosixPath(os.path.join(os.path.expanduser(\"~\"), \"git/sciflow\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_session(default_bucket, region):\n",
    "    region = region = \"eu-west-1\"\n",
    "    return sagemaker.session.Session(default_bucket=default_bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_config = CacheConfig(enable_caching=True, expire_after=\"1d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abalone Pipeline\n",
    "\n",
    "<img src=\"smpipeline_abalone.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pipeline(\n",
    "    region,\n",
    "    sagemaker_project_arn=None,\n",
    "    role=None,\n",
    "    default_bucket=None,\n",
    "    model_package_group_name=\"AbalonePackageGroup\",\n",
    "    pipeline_name=\"AbalonePipeline\",\n",
    "    base_job_prefix=\"Abalone\",\n",
    "):\n",
    "    \"\"\"Gets a SageMaker ML Pipeline instance working with on abalone data.\n",
    "\n",
    "    Args:\n",
    "        region: AWS region to create and run the pipeline.\n",
    "        role: IAM role to create and run steps and pipeline.\n",
    "        default_bucket: the bucket to use for storing the artifacts\n",
    "\n",
    "    Returns:\n",
    "        an instance of a pipeline\n",
    "    \"\"\"\n",
    "    sagemaker_session = get_session(default_bucket, region)\n",
    "    if role is None:\n",
    "        role = sagemaker.session.get_execution_role(sagemaker_session)\n",
    "\n",
    "    # parameters for pipeline execution\n",
    "    processing_instance_count = ParameterInteger(\n",
    "        name=\"ProcessingInstanceCount\", default_value=1\n",
    "    )\n",
    "    processing_instance_type = ParameterString(\n",
    "        name=\"ProcessingInstanceType\", default_value=\"ml.m5.xlarge\"\n",
    "    )\n",
    "    training_instance_type = ParameterString(\n",
    "        name=\"TrainingInstanceType\", default_value=\"ml.m5.xlarge\"\n",
    "    )\n",
    "    model_approval_status = ParameterString(\n",
    "        name=\"ModelApprovalStatus\", default_value=\"PendingManualApproval\"\n",
    "    )\n",
    "    input_data = ParameterString(\n",
    "        name=\"InputDataUrl\",\n",
    "        default_value=f\"s3://{bucket}/{dataset_key_prefix}/abalone-dataset.csv\",\n",
    "    )\n",
    "\n",
    "    # processing step for feature engineering\n",
    "    sklearn_processor = SKLearnProcessor(\n",
    "        framework_version=\"0.23-1\",\n",
    "        instance_type=processing_instance_type,\n",
    "        instance_count=processing_instance_count,\n",
    "        base_job_name=f\"{base_job_prefix}/sklearn-abalone-preprocess\",\n",
    "        sagemaker_session=sagemaker_session,\n",
    "        role=role,\n",
    "    )\n",
    "    step_process = ProcessingStep(\n",
    "        name=\"PreprocessAbaloneData\",\n",
    "        processor=sklearn_processor,\n",
    "        outputs=[\n",
    "            ProcessingOutput(output_name=\"train\", source=\"/opt/ml/processing/train\"),\n",
    "            ProcessingOutput(\n",
    "                output_name=\"validation\", source=\"/opt/ml/processing/validation\"\n",
    "            ),\n",
    "            ProcessingOutput(output_name=\"test\", source=\"/opt/ml/processing/test\"),\n",
    "        ],\n",
    "        code=os.path.join(\n",
    "            repo_root, \"examples\", \"abalone_files\", \"abalone_preprocess.py\"\n",
    "        ),\n",
    "        job_arguments=[\"--input-data\", input_data],\n",
    "        cache_config=cache_config,\n",
    "    )\n",
    "\n",
    "    # training step for generating model artifacts\n",
    "    model_path = (\n",
    "        f\"s3://{sagemaker_session.default_bucket()}/{base_job_prefix}/AbaloneTrain\"\n",
    "    )\n",
    "\n",
    "    metrics = [\"Train MSE\", \"Train STD\", \"Validation MSE\", \"Validation STD\"]\n",
    "    metrics_regex = [{\"Name\": m, \"Regex\": f\"{m}=(.*?);\"} for m in metrics]\n",
    "\n",
    "    sklearn_estimator = SKLearn(\n",
    "        source_dir=\"abalone_files\",\n",
    "        entry_point=\"abalone_train.py\",\n",
    "        framework_version=\"0.23-1\",\n",
    "        hyperparameters={\"max_iter\": 30, \"learning_rate\": 0.1},\n",
    "        instance_type=training_instance_type,\n",
    "        instance_count=1,\n",
    "        output_path=model_path,\n",
    "        base_job_name=f\"{base_job_prefix}/abalone-train\",\n",
    "        sagemaker_session=sagemaker_session,\n",
    "        role=role,\n",
    "        metric_definitions=metrics_regex,\n",
    "        enable_sagemaker_metrics=True,\n",
    "    )\n",
    "\n",
    "    step_train = TrainingStep(\n",
    "        name=\"TrainAbaloneModel\",\n",
    "        estimator=sklearn_estimator,\n",
    "        inputs={\n",
    "            \"train\": TrainingInput(\n",
    "                s3_data=step_process.properties.ProcessingOutputConfig.Outputs[\n",
    "                    \"train\"\n",
    "                ].S3Output.S3Uri,\n",
    "                content_type=\"text/csv\",\n",
    "            ),\n",
    "            \"validation\": TrainingInput(\n",
    "                s3_data=step_process.properties.ProcessingOutputConfig.Outputs[\n",
    "                    \"validation\"\n",
    "                ].S3Output.S3Uri,\n",
    "                content_type=\"text/csv\",\n",
    "            ),\n",
    "        },\n",
    "        cache_config=cache_config,\n",
    "    )\n",
    "\n",
    "    # processing step for evaluation\n",
    "    script_eval = SKLearnProcessor(\n",
    "        framework_version=\"0.23-1\",\n",
    "        instance_type=processing_instance_type,\n",
    "        instance_count=1,\n",
    "        base_job_name=f\"{base_job_prefix}/script-abalone-eval\",\n",
    "        sagemaker_session=sagemaker_session,\n",
    "        role=role,\n",
    "    )\n",
    "\n",
    "    evaluation_report = PropertyFile(\n",
    "        name=\"AbaloneEvaluationReport\",\n",
    "        output_name=\"evaluation\",\n",
    "        path=\"evaluation.json\",\n",
    "    )\n",
    "    step_eval = ProcessingStep(\n",
    "        name=\"EvaluateAbaloneModel\",\n",
    "        processor=script_eval,\n",
    "        inputs=[\n",
    "            ProcessingInput(\n",
    "                source=os.path.join(\n",
    "                    repo_root, \"examples\", \"abalone_files\", \"requirements.txt\"\n",
    "                ),\n",
    "                destination=\"/opt/ml/processing/requirements\",\n",
    "            ),\n",
    "            ProcessingInput(\n",
    "                source=step_train.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "                destination=\"/opt/ml/processing/model\",\n",
    "            ),\n",
    "            ProcessingInput(\n",
    "                source=step_process.properties.ProcessingOutputConfig.Outputs[\n",
    "                    \"test\"\n",
    "                ].S3Output.S3Uri,\n",
    "                destination=\"/opt/ml/processing/test\",\n",
    "            ),\n",
    "        ],\n",
    "        outputs=[\n",
    "            ProcessingOutput(\n",
    "                output_name=\"evaluation\", source=\"/opt/ml/processing/evaluation\"\n",
    "            ),\n",
    "        ],\n",
    "        code=os.path.join(repo_root, \"examples\", \"abalone_files\", \"abalone_eval.py\"),\n",
    "        property_files=[evaluation_report],\n",
    "        cache_config=cache_config,\n",
    "    )\n",
    "\n",
    "    # register model step that will be conditionally executed\n",
    "    model_metrics = ModelMetrics(\n",
    "        model_statistics=MetricsSource(\n",
    "            s3_uri=\"{}/evaluation.json\".format(\n",
    "                step_eval.arguments[\"ProcessingOutputConfig\"][\"Outputs\"][0][\"S3Output\"][\n",
    "                    \"S3Uri\"\n",
    "                ]\n",
    "            ),\n",
    "            content_type=\"application/json\",\n",
    "        )\n",
    "    )\n",
    "    step_register = RegisterModel(\n",
    "        name=\"RegisterAbaloneModel\",\n",
    "        estimator=sklearn_estimator,\n",
    "        model_data=step_train.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "        content_types=[\"text/csv\"],\n",
    "        response_types=[\"text/csv\"],\n",
    "        inference_instances=[\"ml.t2.medium\", \"ml.m5.large\"],\n",
    "        transform_instances=[\"ml.m5.large\"],\n",
    "        model_package_group_name=model_package_group_name,\n",
    "        approval_status=model_approval_status,\n",
    "        model_metrics=model_metrics,\n",
    "    )\n",
    "\n",
    "    # condition step for evaluating model quality and branching execution\n",
    "    cond_lte = ConditionLessThanOrEqualTo(\n",
    "        left=JsonGet(\n",
    "            step=step_eval,\n",
    "            property_file=evaluation_report,\n",
    "            json_path=\"regression_metrics.mse.value\",\n",
    "        ),\n",
    "        right=15.0,\n",
    "    )\n",
    "    step_cond = ConditionStep(\n",
    "        name=\"CheckMSEAbaloneEvaluation\",\n",
    "        conditions=[cond_lte],\n",
    "        if_steps=[step_register],\n",
    "        else_steps=[],\n",
    "    )\n",
    "\n",
    "    # pipeline instance\n",
    "    pipeline = Pipeline(\n",
    "        name=pipeline_name,\n",
    "        parameters=[\n",
    "            processing_instance_type,\n",
    "            processing_instance_count,\n",
    "            training_instance_type,\n",
    "            model_approval_status,\n",
    "            input_data,\n",
    "        ],\n",
    "        steps=[step_process, step_train, step_eval, step_cond],\n",
    "        sagemaker_session=sagemaker_session,\n",
    "    )\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = get_pipeline(default_bucket=bucket, region=\"eu-west-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.upsert(role_arn=role)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execution = pipeline.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time execution.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(fn)\n",
    "df.columns = [c.strip().lower().replace(\" \", \"_\") for c in df.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3 (conda-env/4)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:eu-west-1:368653567616:image-version/conda-env/4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
