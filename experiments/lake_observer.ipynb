{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp lake_observer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sacred Data Lake Observer\n",
    "\n",
    "`sacred` is an excellent library for tracking machine learning experiments. It has an observer model for experiments and there are many different types of observer, which accomodate many destinations. When you combined with some community provided tooling like incense and omniboard this as complete an experimentation management capability as Data Scientists need. \n",
    "\n",
    "An issue that prevents greater adoption of the SIO stack sacred/incense/omniboard is dependence on an external service, namely MongoDB. It is not easy for Data Scientists to deploy a MongoDB instance within a production environment. However most Data Science notebook environments now permit access to data lake storage such as S3.\n",
    "\n",
    "> This `sacred` observer adds support for a data lake observer. This observer stores all data in block storage under a root experiment directory. Each experiment component, e.g artifacts, metrics, runs is stored in it's own directory. Components like runs and metrics can be queried using a lake compatible query engine with a client ODBC driver. Files and other nested/unstructured entities can be accessed from the block storage client directly. The goal is to provide the same capability as the MongoDBObserver and hence to be compatible with key downstream libraries like: `incense` and `omniboard`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import json\n",
    "import os\n",
    "import os.path\n",
    "import pandas as pd\n",
    "\n",
    "from sacred.commandline_options import cli_option\n",
    "from sacred.dependencies import get_digest\n",
    "from sacred.observers.base import RunObserver\n",
    "from sacred.serializer import flatten\n",
    "import re\n",
    "import socket\n",
    "from text_discovery.s3_utils import (s3_join,\n",
    "                                     is_valid_bucket, \n",
    "                                     list_s3_subdirs, \n",
    "                                     objects_exist_in_dir,\n",
    "                                     delete_dir)\n",
    "import boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "DEFAULT_S3_PRIORITY = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class AWSLakeObserver(RunObserver):\n",
    "    VERSION = \"AWSLakeObserver-0.1.0\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        bucket_name,\n",
    "        experiment_dir,\n",
    "        priority=DEFAULT_S3_PRIORITY,\n",
    "        region=None,\n",
    "    ):\n",
    "        \"\"\"Constructor for a AWSLakeObserver object.\n",
    "\n",
    "        Run when the object is first created,\n",
    "        before it's used within an experiment.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        bucket_name\n",
    "            The name of the bucket you want to store results in.\n",
    "            Doesn't need to contain `s3://`, but needs to be a valid bucket name\n",
    "        experiment_dir\n",
    "            The relative path inside your bucket where you want this experiment to store results\n",
    "        priority\n",
    "            The priority to assign to this observer if\n",
    "            multiple observers are present\n",
    "        region\n",
    "            The AWS region in which you want to create and access\n",
    "            buckets. Needs to be either set here or configured in your AWS\n",
    "        \"\"\"\n",
    "        if not is_valid_bucket(bucket_name):\n",
    "            raise ValueError(\n",
    "                \"Your chosen bucket name doesn't follow AWS bucket naming rules\"\n",
    "            )\n",
    "        self.experiment_dir = experiment_dir\n",
    "        self.bucket_name = bucket_name\n",
    "        self.priority = priority\n",
    "        self.resource_dir = None\n",
    "        self.source_dir = None\n",
    "        self.runs_dir = None\n",
    "        self.metrics_dir = None\n",
    "        self.artifacts_dir = None\n",
    "        self.run_entry = None\n",
    "        self.config = None\n",
    "        self.info = None\n",
    "        self.experiment_id = None\n",
    "        self.cout = \"\"\n",
    "        self.cout_write_cursor = 0\n",
    "        self.saved_metrics = {}\n",
    "        if region is not None:\n",
    "            self.region = region\n",
    "            self.s3 = boto3.resource(\"s3\", region_name=region)\n",
    "        else:\n",
    "            session = boto3.session.Session()\n",
    "            if session.region_name is not None:\n",
    "                self.region = session.region_name\n",
    "                self.s3 = boto3.resource(\"s3\")\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                    \"You must either pass in an AWS region name, or have a \"\n",
    "                    \"region name specified in your AWS config file\"\n",
    "                )\n",
    "                \n",
    "    def put_data(self, key, binary_data):\n",
    "        self.s3.Object(self.bucket_name, key).put(Body=binary_data)\n",
    "\n",
    "    def save_json(self, table_dir, obj, filename):\n",
    "        key = s3_join(table_dir, filename)\n",
    "        self.put_data(key, json.dumps(flatten(obj), sort_keys=True, indent=2))\n",
    "        \n",
    "    def save_file(self, file_save_dir, filename, target_name=None):\n",
    "        target_name = target_name or os.path.basename(filename)\n",
    "        key = s3_join(file_save_dir, target_name)\n",
    "        self.put_data(key, open(filename, \"rb\"))\n",
    "    \n",
    "    def save_sources(self, ex_info):\n",
    "        base_dir = ex_info[\"base_dir\"]\n",
    "        source_info = []\n",
    "        for s, m in ex_info[\"sources\"]:\n",
    "            abspath = os.path.join(base_dir, s)\n",
    "            store_path, md5sum = self.find_or_save(abspath, self.source_dir)\n",
    "            source_info.append([s, os.path.relpath(store_path, self.experiment_dir)])\n",
    "        return source_info\n",
    "    \n",
    "    def find_or_save(self, filename, store_dir):\n",
    "        source_name, ext = os.path.splitext(os.path.basename(filename))\n",
    "        md5sum = get_digest(filename)\n",
    "        store_name = source_name + \"_\" + md5sum + ext\n",
    "        store_path = s3_join(store_dir, store_name)\n",
    "        if len(list_s3_subdirs(self.s3, self.bucket_name, prefix=store_path)) == 0:\n",
    "            self.save_file(self.source_dir, filename, store_path)\n",
    "        return store_path, md5sum\n",
    "    \n",
    "    def _determine_run_dir(self, _id):\n",
    "        if _id is None:\n",
    "            path_subdirs = list_s3_subdirs(self.s3, self.bucket_name, \n",
    "                                           s3_join(self.experiment_dir, \"runs\"))\n",
    "            if not path_subdirs:\n",
    "                max_run_id = 0\n",
    "            else:\n",
    "                integer_directories = [\n",
    "                    int(d) for d in path_subdirs if d.isdigit()\n",
    "                ]\n",
    "                if not integer_directories:\n",
    "                    max_run_id = 0\n",
    "                else:\n",
    "                    # If there are directories under experiment_dir that aren't\n",
    "                    # numeric run directories, ignore those\n",
    "                    max_run_id = max(integer_directories)\n",
    "\n",
    "            _id = max_run_id + 1\n",
    "\n",
    "        self.runs_dir = s3_join(self.experiment_dir, \"runs\", str(_id))\n",
    "        self.metrics_dir = s3_join(self.experiment_dir, \"metrics\", str(_id))\n",
    "        self.artifacts_dir = s3_join(self.experiment_dir, \"artifacts\", str(_id))\n",
    "        self.resource_dir = s3_join(self.experiment_dir, \"resources\", str(_id))\n",
    "        self.source_dir = s3_join(self.experiment_dir, \"sources\", str(_id))\n",
    "        \n",
    "        self.dirs = (self.runs_dir, self.metrics_dir, self.artifacts_dir,\n",
    "                    self.resource_dir, self.source_dir)\n",
    "        for dir_to_check in self.dirs:\n",
    "            if objects_exist_in_dir(self.s3, self.bucket_name, dir_to_check):\n",
    "                raise FileExistsError(\"S3 dir at {} already exists\".format(self.runs_dir))\n",
    "\n",
    "        return _id\n",
    "\n",
    "    def queued_event(\n",
    "        self, ex_info, command, host_info, queue_time, config, meta_info, _id\n",
    "    ):\n",
    "        _id = self._determine_run_dir(_id)\n",
    "\n",
    "        self.run_entry = {\n",
    "            \"experiment\": dict(ex_info),\n",
    "            \"command\": command,\n",
    "            \"host\": dict(host_info),\n",
    "            \"config\": flatten(config),\n",
    "            \"meta\": meta_info,\n",
    "            \"status\": \"QUEUED\",\n",
    "        }\n",
    "        self.config = config\n",
    "        self.info = {}\n",
    "\n",
    "        self.save_json(self.run_entry, \"run.json\")\n",
    "\n",
    "        return _id\n",
    "\n",
    "    def started_event(\n",
    "        self, ex_info, command, host_info, start_time, config, meta_info, _id\n",
    "    ):\n",
    "        _id = self._determine_run_dir(_id)\n",
    "        self.experiment_id = _id\n",
    "        \n",
    "        ex_info[\"sources\"] = self.save_sources(ex_info)\n",
    "\n",
    "        self.run_entry = {\n",
    "            \"experiment_id\": self.experiment_id,\n",
    "            \"experiment\": dict(ex_info),\n",
    "            \"format\": self.VERSION,\n",
    "            \"command\": command,\n",
    "            \"host\": dict(host_info),\n",
    "            \"start_time\": start_time.isoformat(),\n",
    "            \"config\": flatten(config),\n",
    "            \"meta\": meta_info,\n",
    "            \"status\": \"RUNNING\",\n",
    "            \"resources\": [],\n",
    "            \"artifacts\": [],\n",
    "            \"captured_out\": \"\",\n",
    "            \"info\": {},\n",
    "            \"heartbeat\": None,\n",
    "        }\n",
    "        self.config = config\n",
    "        self.info = {}\n",
    "        self.cout = \"\"\n",
    "        self.cout_write_cursor = 0\n",
    "\n",
    "        self.save_json(self.runs_dir, self.run_entry, \"run.json\")\n",
    "\n",
    "        return _id\n",
    "\n",
    "    def heartbeat_event(self, info, captured_out, beat_time, result):\n",
    "        self.info = info\n",
    "        self.run_entry[\"heartbeat\"] = beat_time.isoformat()\n",
    "        self.run_entry[\"captured_out\"] = captured_out\n",
    "        self.run_entry[\"result\"] = result\n",
    "        self.save_json(self.runs_dir, self.run_entry, \"run.json\")\n",
    "\n",
    "    def completed_event(self, stop_time, result):\n",
    "        self.run_entry[\"stop_time\"] = stop_time.isoformat()\n",
    "        self.run_entry[\"result\"] = result\n",
    "        self.run_entry[\"status\"] = \"COMPLETED\"\n",
    "\n",
    "        self.save_json(self.runs_dir, self.run_entry, \"run.json\")\n",
    "\n",
    "    def interrupted_event(self, interrupt_time, status):\n",
    "        self.run_entry[\"stop_time\"] = interrupt_time.isoformat()\n",
    "        self.run_entry[\"status\"] = status\n",
    "        self.save_json(self.runs_dir, self.run_entry, \"run.json\")\n",
    "\n",
    "    def failed_event(self, fail_time, fail_trace):\n",
    "        self.run_entry[\"stop_time\"] = fail_time.isoformat()\n",
    "        self.run_entry[\"status\"] = \"FAILED\"\n",
    "        self.run_entry[\"fail_trace\"] = fail_trace\n",
    "        self.save_json(self.runs_dir, self.run_entry, \"run.json\")\n",
    "\n",
    "    def resource_event(self, filename):\n",
    "        store_path, md5sum = self.find_or_save(filename, self.resource_dir)\n",
    "        self.run_entry[\"resources\"].append([filename, store_path])\n",
    "        self.save_json(self.runs_dir, self.run_entry, \"run.json\")\n",
    "\n",
    "    def artifact_event(self, name, filename, metadata=None, content_type=None):\n",
    "        self.save_file(self.artifacts_dir, filename, name)\n",
    "        self.run_entry[\"artifacts\"].append(name)\n",
    "        self.save_json(self.runs_dir, self.run_entry, \"run.json\")\n",
    "\n",
    "    def log_metrics(self, metrics_by_name, info):\n",
    "        \"\"\"Store new measurements into metrics.csv\"\"\"\n",
    "        metric_frames = [pd.DataFrame(v) for v in metrics_by_name.values()]\n",
    "        metrics = pd.concat(metric_frames).reset_index(drop=True)\n",
    "        metrics['experiment_id'] = self.experiment_id\n",
    "        metrics_path = f's3://{self.bucket_name}/{self.metrics_dir}/metrics.csv'\n",
    "        metrics.to_csv(metrics_path, index=False)\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        if isinstance(other, AWSLakeObserver):\n",
    "            return self.bucket == other.bucket and self.experiment_dir == other.experiment_dir\n",
    "        else:\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    observer = AWSLakeObserver(bucket_name, experiment_dir)\n",
    "except ValueError as ve:\n",
    "    assert('region' in str(ve).lower())\n",
    "observer = AWSLakeObserver(bucket_name, experiment_dir, region='eu-west-1')\n",
    "assert(observer.region == 'eu-west-1')\n",
    "# Do not check for missing bucket yet\n",
    "observer = AWSLakeObserver(missing_bucket, experiment_dir, region='eu-west-1')\n",
    "try:\n",
    "    observer = AWSLakeObserver(invalid_bucket, experiment_dir, \n",
    "                               region='eu-west-1')\n",
    "except ValueError as ve:\n",
    "    assert('naming' in str(ve).lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sacred import Experiment\n",
    "from sacred.run import Run\n",
    "\n",
    "ex = Experiment('test-lake-obs', interactive=True)\n",
    "\n",
    "test_key_prefix = 'discovery/experiments/test/'\n",
    "obs = AWSLakeObserver(bucket_name='s3bawspprwe1chatbotunpub01', \n",
    "                   experiment_dir=f'{test_key_prefix}lake_observer',\n",
    "                   region='eu-west-1')\n",
    "\n",
    "ex.observers.append(obs)\n",
    "\n",
    "@ex.config\n",
    "def my_config():\n",
    "    recipient = \"test\"\n",
    "    message = \"Hello %s!\" % recipient\n",
    "\n",
    "@ex.main\n",
    "def my_main(message, _run: Run):\n",
    "    _run.add_artifact('data/requirements.txt')\n",
    "    _run.add_artifact('data/dataframe_artifact.csv')\n",
    "    _run.log_scalar('rsme', 1.79, 0)\n",
    "    _run.log_scalar('another one', 9.12, 0)\n",
    "    print(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:test-lake-obs:Running command 'my_main'\n",
      "INFO:test-lake-obs:Started run with ID \"3\"\n",
      "INFO:test-lake-obs:Completed after 0:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello test!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<sacred.run.Run at 0x7fa9b7ed82d0>"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "s3_res = boto3.resource('s3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'ResponseMetadata': {'RequestId': 'XZ67G2706K98PZBY',\n",
       "   'HostId': 'Fut0qvcoUC0sX3zr8KwMSCbC+U888m4OCB4xmSfKg/s3WNjqVN88JiXoCQJio8obkRxAgFyUx8M=',\n",
       "   'HTTPStatusCode': 200,\n",
       "   'HTTPHeaders': {'x-amz-id-2': 'Fut0qvcoUC0sX3zr8KwMSCbC+U888m4OCB4xmSfKg/s3WNjqVN88JiXoCQJio8obkRxAgFyUx8M=',\n",
       "    'x-amz-request-id': 'XZ67G2706K98PZBY',\n",
       "    'date': 'Sun, 14 Mar 2021 10:58:34 GMT',\n",
       "    'content-type': 'application/xml',\n",
       "    'transfer-encoding': 'chunked',\n",
       "    'server': 'AmazonS3',\n",
       "    'connection': 'close'},\n",
       "   'RetryAttempts': 0},\n",
       "  'Deleted': [{'Key': 'discovery/experiments/test/lake_observer/runs/2/run.json',\n",
       "    'DeleteMarker': True,\n",
       "    'DeleteMarkerVersionId': '7pMG_ItD3yJzjSQBd3KASyOkCRoZUXsd'},\n",
       "   {'Key': 'discovery/experiments/test/lake_observer/artifacts/1/requirements.txt',\n",
       "    'DeleteMarker': True,\n",
       "    'DeleteMarkerVersionId': 'k4oizgXkkiiji1JnzoHPruXuVtfLeEP.'},\n",
       "   {'Key': 'discovery/experiments/test/lake_observer/metrics/1/metrics.csv',\n",
       "    'DeleteMarker': True,\n",
       "    'DeleteMarkerVersionId': 'OfVLGia.FqTwSBqQHJQApBku2ubiYQmd'},\n",
       "   {'Key': 'discovery/experiments/test/lake_observer/artifacts/3/dataframe_artifact.csv',\n",
       "    'DeleteMarker': True,\n",
       "    'DeleteMarkerVersionId': 'D78goL66SAUlpGguPp2pVZjzVKMcJS0w'},\n",
       "   {'Key': 'discovery/experiments/test/lake_observer/artifacts/1/dataframe_artifact.csv',\n",
       "    'DeleteMarker': True,\n",
       "    'DeleteMarkerVersionId': 'qmQ939tvJlTneaw0DIg52ge33ErdwhMv'},\n",
       "   {'Key': 'discovery/experiments/test/lake_observer/runs/3/run.json',\n",
       "    'DeleteMarker': True,\n",
       "    'DeleteMarkerVersionId': 'Iqw7OMIQnYYK6.vskG8RTZDimfZZG4Sf'},\n",
       "   {'Key': 'discovery/experiments/test/lake_observer/artifacts/3/requirements.txt',\n",
       "    'DeleteMarker': True,\n",
       "    'DeleteMarkerVersionId': 'MBiVElukMCiSxozTGAAoKjI15Zl9RX.T'},\n",
       "   {'Key': 'discovery/experiments/test/lake_observer/metrics/3/metrics.csv',\n",
       "    'DeleteMarker': True,\n",
       "    'DeleteMarkerVersionId': '7lbH9EXqb4dMu5RCrK.M5OYLNYgVAVNI'},\n",
       "   {'Key': 'discovery/experiments/test/lake_observer/artifacts/2/dataframe_artifact.csv',\n",
       "    'DeleteMarker': True,\n",
       "    'DeleteMarkerVersionId': 'SB7RHppk25tJI7oRCnJqbdGIdweAglNx'},\n",
       "   {'Key': 'discovery/experiments/test/lake_observer/metrics/2/metrics.csv',\n",
       "    'DeleteMarker': True,\n",
       "    'DeleteMarkerVersionId': '1L9kY589q6F3sRKmFnc.2MO_rKd4o_c1'},\n",
       "   {'Key': 'discovery/experiments/test/lake_observer/runs/1/run.json',\n",
       "    'DeleteMarker': True,\n",
       "    'DeleteMarkerVersionId': 'QNjtoFWMSPUZHJt56BTYu3BMLpk11xO1'},\n",
       "   {'Key': 'discovery/experiments/test/lake_observer/artifacts/2/requirements.txt',\n",
       "    'DeleteMarker': True,\n",
       "    'DeleteMarkerVersionId': '0aYH36k2yhW7Pt.k8TL0AC.eIEflkQ23'}]}]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# delete_dir doesn't seem to always work.. why not?\n",
    "bucket = s3_res.Bucket(bucket_name)\n",
    "bucket.objects.filter(Prefix=test_key_prefix).delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "@cli_option(\"-L\", \"--lake\")\n",
    "def lake_option(args, run):\n",
    "    \"\"\"Add a Data Lake observer to the experiment.\n",
    "\n",
    "    The argument value should be `s3://<bucket>/path/to/exp`.\n",
    "    \"\"\"\n",
    "    match_obj = re.match(r\"s3:\\/\\/([^\\/]*)\\/(.*)\", args)\n",
    "    if match_obj is None or len(match_obj.groups()) != 2:\n",
    "        raise ValueError(\n",
    "            \"Valid bucket specification not found. \"\n",
    "            \"Enter bucket and directory path like: \"\n",
    "            \"s3://<bucket>/path/to/exp\"\n",
    "        )\n",
    "    bucket_name, experiment_dir = match_obj.groups()\n",
    "    run.observers.append(AWSLakeObserver(bucket_name=bucket_name, experiment_dir=experiment_dir))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:jovyan-discovery]",
   "language": "python",
   "name": "conda-env-jovyan-discovery-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
